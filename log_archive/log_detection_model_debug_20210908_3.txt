GeneralizedRCNN.__init(self, cfg) { // BEGIN 

    super(GeneralizedRCNN, self).__init__()

    ==== backbone build ====
    build_backbone: <function build_backbone at 0x7f8150cad510>
    self.backbone = build_backbone(cfg)

        build_backbone(cfg) { // BEGIN

            registry.BACKBONES: {
                'TIMM-MOBILENETV2-100': <function build_timm_backbone at 0x7f8150f64730>,
                    'R-101-FPN-RETINANET': <function build_resnet_fpn_p3p7_backbone at 0x7f8150cadbf8>,
                    'R-50-FPN-RETINANET': <function build_resnet_fpn_p3p7_backbone at 0x7f8150cadbf8>
            }

            cfg.MODEL.BACKBONE.CONV_BODY: R-50-FPN-RETINANET

            cfg:
                DATALOADER:
                    SIZE_DIVISIBILITY: 32

                INPUT:
                    FIXED_SIZE: (-1, -1)
                    MAX_SIZE_TEST: 640
                    MIN_SIZE_TEST: 480
                    PIXEL_MEAN: [102.9801, 115.9465, 122.7717]
                    PIXEL_STD: [1.0, 1.0, 1.0]
                    RESIZE_MODE: keep_ratio
                    TARGET_INTERPOLATION: bilinear
                    TO_BGR255: True
                    TO_N1P1: False

                MODEL:
                    BACKBONE:
                        CONV_BODY: R-50-FPN-RETINANET
                        FREEZE_CONV_BODY_AT: 2
                        DEVICE: cuda

                    FPN:
                        USE_GN: False
                        USE_RELU: False

                    GROUP_NORM:
                        DIM_PER_GP: -1
                        EPSILON: 1e-05
                        NUM_GROUPS: 32

                    MASK_ON: False

                    META_ARCHITECTURE: GeneralizedRCNN

                    RECOGNITION: False

                    RESNETS:
                        BACKBONE_OUT_CHANNELS: 1024
                        DEFORMABLE_GROUPS: 1
                        NUM_GROUPS: 1
                        RES2_OUT_CHANNELS: 256
                        RES5_DILATION: 1
                        STAGE_WITH_DCN: (False, False, False, False)
                        STEM_FUNC: StemWithFixedBatchNorm
                        STEM_OUT_CHANNELS: 64
                        STRIDE_IN_1X1: True
                        TRANS_FUNC: BottleneckWithFixedBatchNorm
                        WIDTH_PER_GROUP: 64
                        WITH_MODULATED_DCN: False

                    RETINANET:
                        ANCHOR_SIZES: (32, 64, 128, 256, 512)
                        ANCHOR_STRIDES: (8, 16, 32, 64, 128)
                        ASPECT_RATIOS: (0.5, 1.0, 2.0)
                        BBOX_REG_BETA: 0.11
                        BBOX_REG_WEIGHT: 4.0
                        BG_IOU_THRESHOLD: 0.4
                        FG_IOU_THRESHOLD: 0.5
                        INFERENCE_TH: 0.05
                        LOSS_ALPHA: 0.25
                        LOSS_GAMMA: 2.0
                        NMS_TH: 0.4
                        NUM_CLASSES: 2
                        NUM_CONVS: 4
                        OCTAVE: 2.0
                        PRE_NMS_TOP_N: 1000
                        PRIOR_PROB: 0.01
                        SCALES_PER_OCTAVE: 3
                        STRADDLE_THRESH: -1
                        USE_C5: True

                        RETINANET_ON: True

                    RPN:
                        ANCHOR_SIZES: (32, 64, 128, 256, 512)
                        ANCHOR_STRIDE: (4, 8, 16, 32, 64)
                        ASPECT_RATIOS: (0.5, 1.0, 2.0)
                        BATCH_SIZE_PER_IMAGE: 256
                        BG_IOU_THRESHOLD: 0.4
                        FG_IOU_THRESHOLD: 0.5
                        FPN_POST_NMS_PER_BATCH: True
                        FPN_POST_NMS_TOP_N_TEST: 1000
                        FPN_POST_NMS_TOP_N_TRAIN: 2000
                        MIN_SIZE: 0
                        NMS_THRESH: 0.7
                        POSITIVE_FRACTION: 0.5
                        POST_NMS_TOP_N_TEST: 1000
                        POST_NMS_TOP_N_TRAIN: 2000
                        PRE_NMS_TOP_N_TEST: 1000
                        PRE_NMS_TOP_N_TRAIN: 2000
                        RPN_HEAD: SingleConvRPNHead
                        STRADDLE_THRESH: 0

                        USE_FPN: True

                        RPN_ONLY: True

                    TEXT_RECOGNIZER:
                        BATCH_MAX_LENGTH: 25
                        CHARACTER: ('num', 'eng_cap', 'eng_low', 'kor_2350')
                        CNN_CHANNELS: 512
                        CNN_NUM_POOLING: 4
                        OUTCONV_KS: (1, 1)
                        TRANSFORMER_DECODER_MODULE_NO: -1
                        TRANSFORMER_ENCODER_MODULE_NO: -1
                        TRANSFORMER_FSIZE: 512
                        TRANSFORMER_MODULE_NO: 6
                        TRANSFORMER_NO_RECURRENT_PATH: False
                        USE_PROJECTION: False

                    TIMM:
                        BACKBONE_OUT_CHANNELS: 256
                        OUT_INDICES: (2, 3, 4)
                        USE_PRETRAINED: True

                    TEST:
                        DETECTIONS_PER_IMG: 100
                        EXPECTED_RESULTS: []
                        EXPECTED_RESULTS_SIGMA_TOL: 4
                        IMS_PER_BATCH: 8
                        NMS_THRESH: 0.5
                    SCORE_THRESHOLD: 0.3

            build_resnet_fpn_p3p7_backbone(cfg) { // BEGIN

              Resnet.__init__ { //BEGIN

                  _STEM_MODULES: {'StemWithFixedBatchNorm': <class 'maskrcnn_benc  mark.modeling.backbone.resnet.StemWithFixedBatchNorm'>}
                  _cfg.MODEL.RESNETS.STEM_FUNC: StemWithFixedBatchNorm

                  stem_module = _STEM_MODULES[cfg.MODEL.RESNETS.STEM_FUNC]
                  stem_module: <class 'maskrcnn_benc  mark.modeling.backbone.resnet.StemWithFixedBatchNorm'>

                  _STAGE_SPECS: {

									    'R-50-C4':
											    ( StageSpec(index=1, block_count=3, return_features=False),
											      StageSpec(index=2, block_count=4, return_features=False),
														StageSpec(index=3, block_count=6, return_features=True)
													),

											'R-50-C5':
											    ( StageSpec(index=1, block_count=3, return_features=False),
											      StageSpec(index=2, block_count=4, return_features=False),
														StageSpec(index=3, block_count=6, return_features=False),
														StageSpec(index=4, block_count=3, return_features=True)
													),

											'R-101-C4':
											    ( StageSpec(index=1, block_count=3,  return_features=False),
											      StageSpec(index=2, block_count=4,  return_features=False),
													  StageSpec(index=3, block_count=23, return_features=True)
													),

											'R-101-C5':
											    ( StageSpec(index=1, block_count=3,  return_features=False),
											      StageSpec(index=2, block_count=4,  return_features=False),
														StageSpec(index=3, block_count=23, return_features=False),
														StageSpec(index=4, block_count=3,  return_features=True)
													),

											'R-50-FPN':
											    ( StageSpec(index=1, block_count=3, return_features=True),
											      StageSpec(index=2, block_count=4, return_features=True),
														StageSpec(index=3, block_count=6, return_features=True),
														StageSpec(index=4, block_count=3, return_features=True)
													),

											'R-50-FPN-RETINANET':
											    ( StageSpec(index=1, block_count=3, return_features=True),
													  StageSpec(index=2, block_count=4, return_features=True),
														StageSpec(index=3, block_count=6, return_features=True),
														StageSpec(index=4, block_count=3, return_features=True)
													),

											'R-101-FPN':
											    ( StageSpec(index=1, block_count=3,  return_features=True),
													  StageSpec(index=2, block_count=4,  return_features=True),
														StageSpec(index=3, block_count=23, return_features=True),
														StageSpec(index=4, block_count=3,  return_features=True)
													),

											'R-101-FPN-RETINANET':
											    ( StageSpec(index=1, block_count=3,  return_features=True),
													  StageSpec(index=2, block_count=4,  return_features=True),
														StageSpec(index=3, block_count=23, return_features=True),
														StageSpec(index=4, block_count=3,  return_features=True)
													),

											'R-152-FPN':
											    ( StageSpec(index=1, block_count=3, return_features=True),
													  StageSpec(index=2, block_count=8, return_features=True),
														StageSpec(index=3, block_count=36, return_features=True),
														StageSpec(index=4, block_count=3, return_features=True))
									}

                  cfg.MODEL.BACKBONE.CONV_BODY: R-50-FPN-RETINANET

									stage_specs = _STAGE_SPECS[cfg.MODEL.BACKBONE.CONV_BODY]

                  stage_specs:
									    ( StageSpec(index=1, block_count=3, return_features=True),
											  StageSpec(index=2, block_count=4, return_features=True),
												StageSpec(index=3, block_count=6, return_features=True),
												StageSpec(index=4, block_count=3, return_features=True)
											)

                  _TRANSFORMATION_MODULES:
									    {'BottleneckWithFixedBatchNorm': <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>}

                  cfg.MODEL.RESNETS.TRANS_FUNC: BottleneckWithFixedBatchNorm

									transformation_module = _TRANSFORMATION_MODULES[cfg.MODEL.RESNETS.TRANS_FUNC]
									transformation_module: <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>

                  self.stem = stem_module(cfg)
                      self.stem:
											    StemWithFixedBatchNorm(
											              (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): FrozenBatchNorm2d()
													)

                  num_groups = cfg.MODEL.RESNETS.NUM_GROUPS
									num_groups.stem: 1

                  width_per_group = cfg.MODEL.RESNETS.WIDTH_PER_GROUP
									width_per_group: 64

                  in_channels = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS
									in_channels: 64

                  stage2_bottleneck_channels = num_groups * width_per_group
                  stage2_bottleneck_channels: 64

                  stage2_out_channels = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
                  stage2_out_channels: 256

                  self.stages = []
                  self.return_features = {}

                  for stage_spec in stage_specs:

                      // ---------------------------------------
                      // iteration at stage_spec.index == 1
                      // ---------------------------------------
                      stage_spec: StageSpec(index=1, block_count=3, return_features=True)
                      stage_spec.index: 1

                      name = "layer" + str(stage_spec.index)
                      name: layer1

                      stage2_relative_factor = 2 ** (stage_spec.index - 1)
                      stage2_relative_factor: 1

                      bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
                      bottlenec_channels: 64

                      out_channels = stage2_out_channels * stage2_relative_factor
                      out_channels: 256

                      stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
                      stage_with_dcn: False

                      module = _make_stage(
                          transformation_module = <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
                          in_channels = 64,
                          bottleneck_channels = 64,
                          out_channels = 256,
                          stage_spec.block_count = 3,
                          num_groups = 1,
                          cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,

                          first_stride=int(stage_spec.index > 1) + 1: 1,

                          dcn_config={
                              'stage_with_dcn': False,
                              'with_modulated_dcn': False,
                              'deformable_groups': 1,
                          }
                      )

                      in_channels = out_channels
                      in_channels: 256

                      self.add_module( name=layer1, module=Sequential(

												      (0): BottleneckWithFixedBatchNorm(

																		 (downsample):
																				 Sequential(
																						 (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
																						 (1): FrozenBatchNorm2d()
																				 )

																		 (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
																		 (bn1): FrozenBatchNorm2d()
																		 (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
																		 (bn2): FrozenBatchNorm2d()
																		 (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
																		 (bn3): FrozenBatchNorm2d()
														)

														(1): BottleneckWithFixedBatchNorm(
																		(conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
																		(bn1): FrozenBatchNorm2d()
																		(conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
																		(bn2): FrozenBatchNorm2d()
																		(conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
																		(bn3): FrozenBatchNorm2d()
														)

														(2): BottleneckWithFixedBatchNorm(
																		(conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
																		(bn1): FrozenBatchNorm2d()
																		(conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
																		(bn2): FrozenBatchNorm2d()
																		(conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
																		(bn3): FrozenBatchNorm2d()
														)
												) // Sequential
											) // self.add_module ()

                      self.stages.append(name=layer1)
                      name: layer1

                      stage_spec.return_features: True
                      self.return_features[name] = stage_spec.return_features


                      // ---------------------------------------
                      // iteration at stage_spec.index == 2
                      // ---------------------------------------
                      stage_spec: StageSpec(index=2, block_count=4, return_features=True)

                      stage_spec.index: 2
                      name = "layer" + str(stage_spec.index)
                      name: layer2

                      stage2_relative_factor = 2 ** (stage_spec.index - 1)
                      stage2_relative_factor: 2

                      bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
                      bottleneck_channels: 128

                      out_channels = stage2_out_channels * stage2_relative_factor
                      out_channels: 512

                      stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
                      stage_wid_dcn: False

                      module = _make_stage(
                          transformation_module = <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
                          in_channels = 256,
                          bottleneck_channels = 128,
                          out_channels = 512,
                          stage_spec.block_count = 4,
                          num_groups = 1,
                          cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
                          first_stride=int(stage_spec.index > 1) + 1: 2,

                          dcn_config={
                              'stage_with_dcn': False,
                              'wit  _modulated_dcn': False,
                              'deformable_groups': 1,
                          }
                      )

                      in_channels = out_channels
                      in_channels: 512

                      self.add_module(name=layer2, module=Sequential(
                            (0): BottleneckWithFixedBatchNorm(
                              (downsample): Sequential(
                                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                (1): FrozenBatchNorm2d()
                                )
                              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                              (bn1): FrozenBatchNorm2d()
                              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                              (bn2): FrozenBatchNorm2d()
                              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn3): FrozenBatchNorm2d()
                            )

                            (1): BottleneckWithFixedBatchNorm(
                              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn1): FrozenBatchNorm2d()
                              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                              (bn2): FrozenBatchNorm2d()
                              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn3): FrozenBatchNorm2d()
                            )

                            (2): BottleneckWithFixedBatchNorm(
                                   (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                   (bn1): FrozenBatchNorm2d()
                                   (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                   (bn2): FrozenBatchNorm2d()
                                   (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                   (bn3): FrozenBatchNorm2d()
                            )
                            3): BottleneckWithFixedBatchNorm(
                                        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                        (bn1): FrozenBatchNorm2d()
                                        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                                        (bn2): FrozenBatchNorm2d()
                                        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                                        (bn3): FrozenBatchNorm2d()
                            )
                          )
                      )

                      self.stages.append(name=layer2)
                      name: layer2
                      stage_spec.return_features: True
                      self.return_features[name] = stage_spec.return_features


                      // ---------------------------------------
                      // iteration at stage_spec.index == 3
                      // ---------------------------------------
                      stage_spec: StageSpec(index=3, block_count=6, return_features=True)
                      stage_spec.index: 3
                      name = "layer" + str(stage_spec.index)
                      name: layer3

                      stage2_relative_factor = 2 ** (stage_spec.index - 1)
                      stage2_relative_factor: 4

                      bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
                      bottleneck_channels: 256

                      out_channels = stage2_out_channels * stage2_relative_factor
                      out_channels: 1024

                      stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
                      stage_wid_dcn: False

                      module = _make_stage(
                          transformation_module = <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
                          in_channels = 512,
                          bottleneck_channels = 256,
                          out_channels = 1024,
                          stage_spec.block_count = 6,
                          num_groups = 1,
                          cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
                          first_stride=int(stage_spec.index > 1) + 1: 2,
                          dcn_config={
                          'stage_with_dcn': False,
                          'wit  _modulated_dcn': False,
                          'deformable_groups': 1,
                          }
                          )

                      in_channels = out_channels
                      in_channels: 1024

                      self.add_module(name=layer3, module=Sequential(
                            (0): BottleneckWithFixedBatchNorm(
                              (downsample): Sequential(
                                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                (1): FrozenBatchNorm2d()
                                )
                              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                              (bn1): FrozenBatchNorm2d()
                              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                              (bn2): FrozenBatchNorm2d()
                              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn3): FrozenBatchNorm2d()
                              )

                            (1): BottleneckWithFixedBatchNorm(
                              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn1): FrozenBatchNorm2d()
                              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                              (bn2): FrozenBatchNorm2d()
                              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn3): FrozenBatchNorm2d()
                              )

                            (2): BottleneckWithFixedBatchNorm(
                               (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                               (bn1): FrozenBatchNorm2d()
                               (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                               (bn2): FrozenBatchNorm2d()
                               (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                               (bn3): FrozenBatchNorm2d()
                               )

                            (3): BottleneckWithFixedBatchNorm(
                               (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                               (bn1): FrozenBatchNorm2d()
                               (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                               (bn2): FrozenBatchNorm2d()
                               (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                               (bn3): FrozenBatchNorm2d()
                               )

                            (4): BottleneckWithFixedBatchNorm(
                               (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                               (bn1): FrozenBatchNorm2d()
                               (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                               (bn2): FrozenBatchNorm2d()
                               (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                               (bn3): FrozenBatchNorm2d()
                               )

                            (5): BottleneckWithFixedBatchNorm(
                               (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                               (bn1): FrozenBatchNorm2d()
                               (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                               (bn2): FrozenBatchNorm2d()
                               (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                               (bn3): FrozenBatchNorm2d()
                               )
                            )
                      )

                      self.stages.append(name=layer3)
                      name: layer3

                      stage_spec.return_features: True
                      self.return_features[name] = stage_spec.return_features


                      // ---------------------------------------
                      // iteration at stage_spec.index == 4
                      // ---------------------------------------
                      stage_spec: StageSpec(index=4, block_count=3, return_features=True)
                      stage_spec.index: 4

                      name = "layer" + str(stage_spec.index)
                      name: layer4

                      stage2_relative_factor = 2 ** (stage_spec.index - 1)
                      stage2_relative_factor: 8

                      bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
                      bottleneck_channels: 512

                      out_channels = stage2_out_channels * stage2_relative_factor
                      out_channels: 2048

                      stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
                      stage_wid_dcn: False

                      module = _make_stage(
                          transformation_module = <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
                          in_channels = 1024,
                          bottleneck_channels = 512,
                          out_channels = 2048,
                          stage_spec.block_count = 3,
                          num_groups = 1,
                          cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
                          first_stride=int(stage_spec.index > 1) + 1: 2,
                          dcn_config={
                          'stage_with_dcn': False,
                          'wit  _modulated_dcn': False,
                          'deformable_groups': 1,
                          }
                      )

                      in_channels = out_channels
                      in_channels: 2048

                      self.add_module(name=layer4, module=Sequential(
                            (0): BottleneckWithFixedBatchNorm(
                              (downsample): Sequential(
                                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                                (1): FrozenBatchNorm2d()
                                )
                              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                              (bn1): FrozenBatchNorm2d()
                              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                              (bn2): FrozenBatchNorm2d()
                              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn3): FrozenBatchNorm2d()
                              )

                            (1): BottleneckWithFixedBatchNorm(
                              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn1): FrozenBatchNorm2d()
                              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                              (bn2): FrozenBatchNorm2d()
                              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn3): FrozenBatchNorm2d()
                              )

                            (2): BottleneckWithFixedBatchNorm(
                              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn1): FrozenBatchNorm2d()
                              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                              (bn2): FrozenBatchNorm2d()
                              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                              (bn3): FrozenBatchNorm2d()
                             )
                          )
                      )

                      self.stages.append(name=layer4)
                      name: layer4

                      stage_spec.return_features: True
                      self.return_features[name] = stage_spec.return_features

                      cfg.MODEL.BACKBONE.FREEZE_CONV_BODY_AT: 2)
                      self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_CONV_BODY_AT)

                      Resnet.__freeze_backbone() { // START

                      } // Resnet.__freeze_backbone() END

              } // Resnet.__init__ END

            body = resnet.ResNet(cfg)

              body: ResNet(

                  (stem): StemWithFixedBatchNorm(
                    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
                    (bn1): FrozenBatchNorm2d()
                    )

                  (layer1): Sequential(
                    (0): BottleneckWithFixedBatchNorm(

                      (downsample): Sequential(
                        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                        (1): FrozenBatchNorm2d()
                        )

                      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (1): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (2): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )
                  )

                  (layer2): Sequential(

                    (0): BottleneckWithFixedBatchNorm(

                      (downsample): Sequential(
                       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                       (1): FrozenBatchNorm2d()
                       )

                      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (1): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (2): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (3): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )
                  )

                  (layer3): Sequential(
                    (0): BottleneckWithFixedBatchNorm(

                      (downsample): Sequential(
                        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): FrozenBatchNorm2d()
                      )

                      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (1): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (2): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (3): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (4): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (5): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )
                  )

                   (layer4): Sequential(
                    (0): BottleneckWithFixedBatchNorm(

                      (downsample): Sequential(
                        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
                        (1): FrozenBatchNorm2d()
                      )
                      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (1): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )

                    (2): BottleneckWithFixedBatchNorm(
                      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn1): FrozenBatchNorm2d()
                      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
                      (bn2): FrozenBatchNorm2d()
                      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
                      (bn3): FrozenBatchNorm2d()
                    )
                  )
              )

            cfg.MODEL.RESNETS.RES2_OUT_CHANNELS: 256
            in_channels_stage2 = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
            in_channels_stage2 = 256

            cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS:1024
            out_channels = cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS
            out_channels = 1024

            in_channels_stage2: 256
            out_channels: 1024

            cfg.MODEL.RETINANET.USE_C5: True
            in_channels_p6p7 = in_channels_stage2 * 8 if cfg.MODEL.RETINANET.USE_C5 else out_channels
            in_channels_p6p7 = 2048

            fpn = fpn_module.FPN (
                in_channels_list = [0, 512, 1024, 2048],
                out_channels = 1024,

                conv_block=conv_with_kaiming_uniform( cfg.MODEL.FPN.USE_GN =False, cfg.MODEL.FPN.USE_RELU =False ),

                top_blocks=fpn_module.LastLevelP6P7(in_channels_p6p7=2048, out_channels=1024,)

                conv_with_kaiming_uniform(use_gn=False, use_relut=False) {// BEGIN
                    return make_conv
                } // END

                LastLevelP6P7.__init__(self, in_channels=2048, out_channels=1024) { // BEGIN
                    super(LastLevelP6P7, self).__init__()
                    self.p6 = nn.Conv2d(in_channels=2048, out_channels=1024, 3, 2, 1)
                    self.p7 = nn.Conv2d(out_channels=1024, out_channels=1024, 3, 2, 1)

                    for module in [self.p6, self.p7]:
                        module=Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                        nn.init.kaiming_uniform_(module.weight=module.weight, a=1)
                        nn.init.constant_(module.bias=module.bias, 0)

                        module=Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                        nn.init.kaiming_uniform_(module.weight=module.weight, a=1)
                        nn.init.constant_(module.bias=module.bias, 0)

                        self.use_p5 : False

                } // END



                FPN.__init__ { // BEGIN

                 == constructor params ==

                 in_channels_list: [0, 512, 1024, 2048]
                 out_channels: 1024

                 conv_block: <function conv_with_kaiming_uniform.<locals>.make_conv at 0x7f8124ec5268>

                 top_blocks: LastLevelP6P7(
                     (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                     (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                 )

                 == constructor params ==

                 super(FPN, self).__init__()

                 for idx, in_channels in enumerate(in_channels_list, 1):

                   //--------------------------------------------
                   // iteration with idx:1, in_channels:0
                   //--------------------------------------------
                   if in_channels ==0, skip


                   //--------------------------------------------
                   // iteration with idx:2, in_channels:512
                   //--------------------------------------------
                   inner_block: fpn_inner2
                   layer_block: fpn_layer2

                   inner_block_module = conv_block(in_channels=512, out_channels=1024, 1)

                     conv_with_kaiming_uniform().make_conv() ====== BEGIN
                     {
                        Conv2d(in_channles=512, out_channels=1024, kernel_size=1, stride=1, padding=0, dilation=1, bias=True,)
                        nn.init.kaiming_uniform_(conv.weight, a=1)

                        if not use_gn:
                           nn.init.constant_(conv.bias, 0)
                        module = [conv,]

                        return conv
                     } // END


                   layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)

                     conv_with_kaiming_uniform().make_conv() ====== BEGIN
                     {
                        Conv2d(in_channles=1024, out_channels=1024, kernel_size=3, stride=1, padding=1, dilation=1, bias=True,)
                        nn.init.kaiming_uniform_(conv.weight, a=1)

                        if not use_gn:
                           nn.init.constant_(conv.bias, 0)

                        module = [conv,]
                        return conv
                     } // END


                   self.add_module(fpn_inner2, inner_block_module)
                   self.add_module(fpn_layer2, layer_block_module)

                   self.inner_blocks.append(fpn_inner2)
                   self.layer_blocks.append(fpn_layer2)


                   //--------------------------------------------
                   // iteration with idx:3, in_channels:1024
                   //--------------------------------------------
                   inner_block: fpn_inner3
                   layer_block: fpn_layer3

                   inner_block_module = conv_block(in_channels=1024, out_channels=1024, 1)

                     conv_with_kaiming_uniform().make_conv() ===== BEGIN
                     {

                       Conv2d(in_c  annles=1024, out_channels=1024, kernel_size=1, stride=1 padding=0, dilation=1, bias=True,)
                       nn.init.kaiming_uniform_(conv.weight, a=1)

                       if not use_gn:
                         nn.init.constant_(conv.bias, 0)

                       module = [conv,]
                       return conv
                     }  //  END

                   layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)

                       conv_with_kaiming_uniform().make_conv() ====== BEGIN
                       {
                         Conv2d(in_c  annles=1024, out_channels=1024, kernel_size=3, stride=1 padding=1, dilation=1, bias=True,)
                         nn.init.kaiming_uniform_(conv.weight, a=1)

                         if not use_gn:
                             nn.init.constant_(conv.bias, 0)

                         module = [conv,]
                         return conv

                       } // END


                   self.add_module(fpn_inner3, inner_block_module)
                   self.add_module(fpn_layer3, layer_block_module)

                   self.inner_blocks.append(fpn_inner3)
                   self.layer_blocks.append(fpn_layer3)

                   //--------------------------------------------
                   // iteration with idx:4, in_channels:2048
                   //--------------------------------------------
                   inner_block: fpn_inner4
                   layer_block: fpn_layer4

                   inner_block_module = conv_block(in_channels=2048, out_channels=1024, 1)

                     conv_with_kaiming_uniform().make_conv() ====== BEGIN
                     {
                       Conv2d(in_channles=2048, out_channels=1024, kernel_size=1, stride=1 padding=0, dilation=1, bias=True,)
                       nn.init.kaiming_uniform_(conv.weig  t, a=1)

                       if not use_gn:
                           nn.init.constant_(conv.bias, 0)

                       module = [conv,]
                       return conv
                     } // END


                   layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)

                     conv_with_kaiming_uniform().make_conv() ====== BEGIN
                     {
                       Conv2d(in_c  annles=1024, out_channels=1024, kernel_size=3, stride=1 padding=1, dilation=1, bias=True,)
                       nn.init.kaiming_uniform_(conv.weig  t, a=1)

                       if not use_gn:
                           nn.init.constant_(conv.bias, 0)

                       module = [conv,]
                       return conv
                     } // END


                   self.add_module(fpn_inner4, inner_block_module)
                   self.add_module(fpn_layer4, layer_block_module)

                   self.inner_blocks.append(fpn_inner4)
                   self.layer_blocks.append(fpn_layer4)

                   // summary
                   self.inner_blocks: ['fpn_inner2', 'fpn_inner3', 'fpn_inner4']

                      self.fpn_inner2: Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
                      self.fpn_inner3: Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                      self.fpn_inner4: Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))

                   self.layer_blocks: ['fpn_layer2', 'fpn_layer3', 'fpn_layer4']
                      self.fpn_layer2: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                      self.fpn_layer3: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                      self.fpn_layer4: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

                   self.top_blocks: LastLevelP6P7(
                      (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                      (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                   )
                } END of FPN.__init__ 

    fpn: {fpn}

    model = nn.Sequential(OrderedDict([("body", body), ("fpn", fpn)]))
    model: Sequential(
  (body): ResNet(
    (stem): StemWithFixedBatchNorm(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): FrozenBatchNorm2d()
    )
    (layer1): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer2): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (3): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer3): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (3): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (4): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (5): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer4): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
  )
  (fpn): FPN(
    (fpn_inner2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_inner3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_inner4): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_blocks): LastLevelP6P7(
      (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
)
    model.out_channels = out_channels
    model.out_channels: 1024
    return model
build_resnet_fpn_p3p7_backbone(cfg) ====== END


    registry.BACKBONES[cfg.MODEL.BACKBONE.CONV_BODY](cfg): Sequential(
  (body): ResNet(
    (stem): StemWithFixedBatchNorm(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): FrozenBatchNorm2d()
    )
    (layer1): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer2): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (3): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer3): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (3): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (4): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (5): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer4): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
  )
  (fpn): FPN(
    (fpn_inner2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_inner3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_inner4): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_blocks): LastLevelP6P7(
      (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
)
    return registry.BACKBONES[cfg.MODEL.BACKBONE.CONV_BODY](cfg)

      } // build_backbone(cfg) ====== END
build_resnet_fpn_p3p7_backbone(cfg) ====== BEGIN


=========================================== Resnet.__init__ BEGIN
    _STEM_MODULES: {'StemWithFixedBatchNorm': <class 'maskrcnn_benc  mark.modeling.backbone.resnet.StemWithFixedBatchNorm'>}
    _cfg.MODEL.RESNETS.STEM_FUNC: StemWithFixedBatchNorm
    stem_module = _STEM_MODULES[cfg.MODEL.RESNETS.STEM_FUNC]
    stem_module: <class 'maskrcnn_benc  mark.modeling.backbone.resnet.StemWithFixedBatchNorm'>
    _STAGE_SPECS: {'R-50-C4': (StageSpec(index=1, block_count=3, return_features=False), StageSpec(index=2, block_count=4, return_features=False), StageSpec(index=3, block_count=6, return_features=True)), 'R-50-C5': (StageSpec(index=1, block_count=3, return_features=False), StageSpec(index=2, block_count=4, return_features=False), StageSpec(index=3, block_count=6, return_features=False), StageSpec(index=4, block_count=3, return_features=True)), 'R-101-C4': (StageSpec(index=1, block_count=3, return_features=False), StageSpec(index=2, block_count=4, return_features=False), StageSpec(index=3, block_count=23, return_features=True)), 'R-101-C5': (StageSpec(index=1, block_count=3, return_features=False), StageSpec(index=2, block_count=4, return_features=False), StageSpec(index=3, block_count=23, return_features=False), StageSpec(index=4, block_count=3, return_features=True)), 'R-50-FPN': (StageSpec(index=1, block_count=3, return_features=True), StageSpec(index=2, block_count=4, return_features=True), StageSpec(index=3, block_count=6, return_features=True), StageSpec(index=4, block_count=3, return_features=True)), 'R-50-FPN-RETINANET': (StageSpec(index=1, block_count=3, return_features=True), StageSpec(index=2, block_count=4, return_features=True), StageSpec(index=3, block_count=6, return_features=True), StageSpec(index=4, block_count=3, return_features=True)), 'R-101-FPN': (StageSpec(index=1, block_count=3, return_features=True), StageSpec(index=2, block_count=4, return_features=True), StageSpec(index=3, block_count=23, return_features=True), StageSpec(index=4, block_count=3, return_features=True)), 'R-101-FPN-RETINANET': (StageSpec(index=1, block_count=3, return_features=True), StageSpec(index=2, block_count=4, return_features=True), StageSpec(index=3, block_count=23, return_features=True), StageSpec(index=4, block_count=3, return_features=True)), 'R-152-FPN': (StageSpec(index=1, block_count=3, return_features=True), StageSpec(index=2, block_count=8, return_features=True), StageSpec(index=3, block_count=36, return_features=True), StageSpec(index=4, block_count=3, return_features=True))}
    cfg.MODEL.BACKBONE.CONV_BODY: R-50-FPN-RETINANET
    stage_specs = _STAGE_SPECS[cfg.MODEL.BACKBONE.CONV_BODY]
    stage_specs: (StageSpec(index=1, block_count=3, return_features=True), StageSpec(index=2, block_count=4, return_features=True), StageSpec(index=3, block_count=6, return_features=True), StageSpec(index=4, block_count=3, return_features=True))
    _TRANSFORMATION_MODULES: {'BottleneckWithFixedBatchNorm': <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>}
    cfg.MODEL.RESNETS.TRANS_FUNC: BottleneckWithFixedBatchNorm
    transformation_module = _TRANSFORMATION_MODULES[cfg.MODEL.RESNETS.TRANS_FUNC]
    transformation_module: <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>
    self.stem = stem_module(cfg)
    self.stem: StemWithFixedBatchNorm(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): FrozenBatchNorm2d()
)
    num_groups = cfg.MODEL.RESNETS.NUM_GROUPS
    num_groups.stem: 1
    widt  _per_group = cfg.MODEL.RESNETS.WIDTH_PER_GROUP
    widt  _per_group: 64
    in_channels = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS
    in_channels: 64
    stage2_bottleneck_channels = num_groups * widt  _per_group
    stage2_bottleneck_channels: 64
    stage2_out_channels = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
    stage2_out_channels: 256
    self.stages = []
    self.return_features = {}
    for stage_spec in stage_specs:
        stage_spec: StageSpec(index=1, block_count=3, return_features=True)
        stage_spec.index: 1
        name = "layer" + str(stage_spec.index)
        name: layer1
        stage2_relative_factor = 2 ** (stage_spec.index - 1)
        stage2_relative_factor: 1
        bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
        bottleneck_channels: 64
        out_channels = stage2_out_channels * stage2_relative_factor
        out_channels: 256
        stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
        stage_wid_dcn: False
        module = _make_stage(
            transformation_module = <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
            in_channels = 64,
            bottleneck_channels = 64,
            out_channels = 256,
            stage_spec.block_count = 3,
            num_groups = 1,
            cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
            first_stride=int(stage_spec.index > 1) + 1: 1,
            dcn_config={
                'stage_with_dcn': False,
                'wit  _modulated_dcn': False,
                'deformable_groups': 1,
                }
            )
        in_channels = out_channels
        in_channels: 256
        self.add_module(name=layer1, module=Sequential(
  (0): BottleneckWithFixedBatchNorm(
    (downsample): Sequential(
      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): FrozenBatchNorm2d()
    )
    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (1): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (2): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
))
        self.stages.append(name=layer1)
        name: layer1
        stage_spec.return_features: True
        self.return_features[name] = stage_spec.return_features
        stage_spec: StageSpec(index=2, block_count=4, return_features=True)
        stage_spec.index: 2
        name = "layer" + str(stage_spec.index)
        name: layer2
        stage2_relative_factor = 2 ** (stage_spec.index - 1)
        stage2_relative_factor: 2
        bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
        bottleneck_channels: 128
        out_channels = stage2_out_channels * stage2_relative_factor
        out_channels: 512
        stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
        stage_wid_dcn: False
        module = _make_stage(
            transformation_module = <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
            in_channels = 256,
            bottleneck_channels = 128,
            out_channels = 512,
            stage_spec.block_count = 4,
            num_groups = 1,
            cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
            first_stride=int(stage_spec.index > 1) + 1: 2,
            dcn_config={
                'stage_with_dcn': False,
                'wit  _modulated_dcn': False,
                'deformable_groups': 1,
                }
            )
        in_channels = out_channels
        in_channels: 512
        self.add_module(name=layer2, module=Sequential(
  (0): BottleneckWithFixedBatchNorm(
    (downsample): Sequential(
      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): FrozenBatchNorm2d()
    )
    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (1): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (2): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (3): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
))
        self.stages.append(name=layer2)
        name: layer2
        stage_spec.return_features: True
        self.return_features[name] = stage_spec.return_features
        stage_spec: StageSpec(index=3, block_count=6, return_features=True)
        stage_spec.index: 3
        name = "layer" + str(stage_spec.index)
        name: layer3
        stage2_relative_factor = 2 ** (stage_spec.index - 1)
        stage2_relative_factor: 4
        bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
        bottleneck_channels: 256
        out_channels = stage2_out_channels * stage2_relative_factor
        out_channels: 1024
        stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
        stage_wid_dcn: False
        module = _make_stage(
            transformation_module = <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
            in_channels = 512,
            bottleneck_channels = 256,
            out_channels = 1024,
            stage_spec.block_count = 6,
            num_groups = 1,
            cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
            first_stride=int(stage_spec.index > 1) + 1: 2,
            dcn_config={
                'stage_with_dcn': False,
                'wit  _modulated_dcn': False,
                'deformable_groups': 1,
                }
            )
        in_channels = out_channels
        in_channels: 1024
        self.add_module(name=layer3, module=Sequential(
  (0): BottleneckWithFixedBatchNorm(
    (downsample): Sequential(
      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): FrozenBatchNorm2d()
    )
    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (1): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (2): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (3): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (4): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (5): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
))
        self.stages.append(name=layer3)
        name: layer3
        stage_spec.return_features: True
        self.return_features[name] = stage_spec.return_features
        stage_spec: StageSpec(index=4, block_count=3, return_features=True)
        stage_spec.index: 4
        name = "layer" + str(stage_spec.index)
        name: layer4
        stage2_relative_factor = 2 ** (stage_spec.index - 1)
        stage2_relative_factor: 8
        bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
        bottleneck_channels: 512
        out_channels = stage2_out_channels * stage2_relative_factor
        out_channels: 2048
        stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
        stage_wid_dcn: False
        module = _make_stage(
            transformation_module = <class 'maskrcnn_benc  mark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
            in_channels = 1024,
            bottleneck_channels = 512,
            out_channels = 2048,
            stage_spec.block_count = 3,
            num_groups = 1,
            cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
            first_stride=int(stage_spec.index > 1) + 1: 2,
            dcn_config={
                'stage_with_dcn': False,
                'wit  _modulated_dcn': False,
                'deformable_groups': 1,
                }
            )
        in_channels = out_channels
        in_channels: 2048
        self.add_module(name=layer4, module=Sequential(
  (0): BottleneckWithFixedBatchNorm(
    (downsample): Sequential(
      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): FrozenBatchNorm2d()
    )
    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (1): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
  (2): BottleneckWithFixedBatchNorm(
    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn1): FrozenBatchNorm2d()
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): FrozenBatchNorm2d()
    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn3): FrozenBatchNorm2d()
  )
))
        self.stages.append(name=layer4)
        name: layer4
        stage_spec.return_features: True
        self.return_features[name] = stage_spec.return_features
        cfg.MODEL.BACKBONE.FREEZE_CONV_BODY_AT: 2)
        self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_CONV_BODY_AT)

    =========================================== Resnet.__freeze_backbone() START
    =========================================== Resnet.__freeze_backbone() END

=========================================== Resnet.__init__ END

    body = resnet.ResNet(cfg)
    body: ResNet(
  (stem): StemWithFixedBatchNorm(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): FrozenBatchNorm2d()
  )
  (layer1): Sequential(
    (0): BottleneckWithFixedBatchNorm(
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): FrozenBatchNorm2d()
      )
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (1): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (2): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
  )
  (layer2): Sequential(
    (0): BottleneckWithFixedBatchNorm(
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): FrozenBatchNorm2d()
      )
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (1): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (2): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (3): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
  )
  (layer3): Sequential(
    (0): BottleneckWithFixedBatchNorm(
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): FrozenBatchNorm2d()
      )
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (1): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (2): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (3): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (4): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (5): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
  )
  (layer4): Sequential(
    (0): BottleneckWithFixedBatchNorm(
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): FrozenBatchNorm2d()
      )
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (1): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
    (2): BottleneckWithFixedBatchNorm(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): FrozenBatchNorm2d()
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): FrozenBatchNorm2d()
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): FrozenBatchNorm2d()
    )
  )
)

    cfg.MODEL.RESNETS.RES2_OUT_CHANNELS: 256
    in_channels_stage2 = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
    in_channels_stage2 = 256
    cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS:1024
    out_channels = cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS
    out_channels = 1024
    in_channels_stage2: 256
    out_channels: 1024
    cfg.MODEL.RETINANET.USE_C5: True
    in_channels_p6p7 = in_channels_stage2 * 8 if cfg.MODEL.RETINANET.USE_C5 else out_channels
    in_channels_p6p7 = 2048

    fpn = fpn_module.FPN(

            in_channels_list = [0, 512, 1024, 2048],

            out_channels = 1024, 

            conv_block=conv_wit  _kaiming_uniform( cfg.MODEL.FPN.USE_GN =False, cfg.MODEL.FPN.USE_RELU =False ),

            top_blocks=fpn_module.LastLevelP6P7(in_channels_p6p7=2048, out_channels=1024,)

        conv_wit  _kaiming_uniform(use_gn=False, use_relut=False) ======== BEGIN
            return make_conv
        tconv_wit  _kaiming_uniform(use_gn=False, use_relut=False) ======== END



        LastLevelP6P7.__init__(self, in_channels=2048, out_channels=1024) ====== BEGIN
            super(LastLevelP6P7, self).__init__()
            self.p6 = nn.Conv2d(in_channels=2048, out_channels=1024, 3, 2, 1)
            self.p7 = nn.Conv2d(out_channels=1024, out_channels=1024, 3, 2, 1)
            for module in [self.p6, self.p7]:
                module=Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                nn.init.kaiming_uniform_(module.weig  t=module.weig  t, a=1)
                nn.init.constant_(module.bias=module.bias, 0)
                module=Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                nn.init.kaiming_uniform_(module.weig  t=module.weig  t, a=1)
                nn.init.constant_(module.bias=module.bias, 0)
            self.use_p5 : False

        LastLevelP6P7.__init__(self, in_channels=2048, out_channels=1024) ====== END




=========================================== FPN.__init__ begin
    ======constructor params
        in_channels_list: [0, 512, 1024, 2048]
        out_channels: 1024
        conv_block: <function conv_wit  _kaiming_uniform.<locals>.make_conv at 0x7f8128a90268>
        top_blocks: LastLevelP6P7(
  (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
    ======constructor params
    super(FPN, self).__init__()

    for idx, in_channels in enumerate(in_channels_list, 1):

        ==> iteration wit   idx:1, in_channels:0
        if in_channels ==0, skip


        ==> iteration wit   idx:2, in_channels:512
        inner_block: fpn_inner2
        layer_block: fpn_layer2
        inner_block_module = conv_block(in_channels=512, out_channels=1024, 1)

            conv_wit  _kaiming_uniform().make_conv() ====== BEGIN
                Conv2d(in_c  annles=512, out_channels=1024, kernel_size=1, stride=1
                       padding=0, dilation=1, bias=True,
                nn.init.kaiming_uniform_(conv.weig  t, a=1)
                if not use_gn:
                    nn.init.constant_(conv.bias, 0)
                module = [conv,]
                return conv

            conv_wit  _kaiming_uniform().make_conv() ====== END

        layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)

            conv_wit  _kaiming_uniform().make_conv() ====== BEGIN
                Conv2d(in_c  annles=1024, out_channels=1024, kernel_size=3, stride=1
                       padding=1, dilation=1, bias=True,
                nn.init.kaiming_uniform_(conv.weig  t, a=1)
                if not use_gn:
                    nn.init.constant_(conv.bias, 0)
                module = [conv,]
                return conv

            conv_wit  _kaiming_uniform().make_conv() ====== END

        self.add_module(fpn_inner2, inner_block_module)
        self.add_module(fpn_layer2, layer_block_module)
        self.inner_blocks.append(fpn_inner2)
        self.layer_blocks.append(fpn_layer2)

        ==> iteration wit   idx:3, in_channels:1024
        inner_block: fpn_inner3
        layer_block: fpn_layer3
        inner_block_module = conv_block(in_channels=1024, out_channels=1024, 1)

            conv_wit  _kaiming_uniform().make_conv() ====== BEGIN
                Conv2d(in_c  annles=1024, out_channels=1024, kernel_size=1, stride=1
                       padding=0, dilation=1, bias=True,
                nn.init.kaiming_uniform_(conv.weig  t, a=1)
                if not use_gn:
                    nn.init.constant_(conv.bias, 0)
                module = [conv,]
                return conv

            conv_wit  _kaiming_uniform().make_conv() ====== END

        layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)

            conv_wit  _kaiming_uniform().make_conv() ====== BEGIN
                Conv2d(in_c  annles=1024, out_channels=1024, kernel_size=3, stride=1
                       padding=1, dilation=1, bias=True,
                nn.init.kaiming_uniform_(conv.weig  t, a=1)
                if not use_gn:
                    nn.init.constant_(conv.bias, 0)
                module = [conv,]
                return conv

            conv_wit  _kaiming_uniform().make_conv() ====== END

        self.add_module(fpn_inner3, inner_block_module)
        self.add_module(fpn_layer3, layer_block_module)
        self.inner_blocks.append(fpn_inner3)
        self.layer_blocks.append(fpn_layer3)

        ==> iteration wit   idx:4, in_channels:2048
        inner_block: fpn_inner4
        layer_block: fpn_layer4
        inner_block_module = conv_block(in_channels=2048, out_channels=1024, 1)

            conv_wit  _kaiming_uniform().make_conv() ====== BEGIN
                Conv2d(in_c  annles=2048, out_channels=1024, kernel_size=1, stride=1
                       padding=0, dilation=1, bias=True,
                nn.init.kaiming_uniform_(conv.weig  t, a=1)
                if not use_gn:
                    nn.init.constant_(conv.bias, 0)
                module = [conv,]
                return conv

            conv_wit  _kaiming_uniform().make_conv() ====== END

        layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)

            conv_wit  _kaiming_uniform().make_conv() ====== BEGIN
                Conv2d(in_c  annles=1024, out_channels=1024, kernel_size=3, stride=1
                       padding=1, dilation=1, bias=True,
                nn.init.kaiming_uniform_(conv.weig  t, a=1)
                if not use_gn:
                    nn.init.constant_(conv.bias, 0)
                module = [conv,]
                return conv

            conv_wit  _kaiming_uniform().make_conv() ====== END

        self.add_module(fpn_inner4, inner_block_module)
        self.add_module(fpn_layer4, layer_block_module)
        self.inner_blocks.append(fpn_inner4)
        self.layer_blocks.append(fpn_layer4)

    self.inner_blocks: ['fpn_inner2', 'fpn_inner3', 'fpn_inner4']
        self.fpn_inner2: Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        self.fpn_inner3: Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        self.fpn_inner4: Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))

    self.layer_blocks: ['fpn_layer2', 'fpn_layer3', 'fpn_layer4']
        self.fpn_layer2: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        self.fpn_layer3: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        self.fpn_layer4: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

    self.top_blocks: LastLevelP6P7(
  (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
=========================================== FPN.__init__ end



    fpn: {fpn}
    model = nn.Sequential(OrderedDict([("body", body), ("fpn", fpn)]))
    model: Sequential(
  (body): ResNet(
    (stem): StemWithFixedBatchNorm(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): FrozenBatchNorm2d()
    )
    (layer1): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer2): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (3): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer3): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (3): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (4): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (5): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
    (layer4): Sequential(
      (0): BottleneckWithFixedBatchNorm(
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): FrozenBatchNorm2d()
        )
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (1): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
      (2): BottleneckWithFixedBatchNorm(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): FrozenBatchNorm2d()
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): FrozenBatchNorm2d()
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): FrozenBatchNorm2d()
      )
    )
  )
  (fpn): FPN(
    (fpn_inner2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_inner3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (fpn_inner4): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
    (fpn_layer4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (top_blocks): LastLevelP6P7(
      (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    )
  )
)
    model.out_channels = out_channels
    model.out_channels: 1024
    return model
build_resnet_fpn_p3p7_backbone(cfg) ====== END


==== rpn build ==== 
self.backbone.out_channels: 1024
build_rpn : <function build_rpn at 0x7f8150cadb70>
self.rpn = build_rpn(cfg, self.backbone.out_channels)


=========================================== RetinaNetModule.__init__(self, cfg, in_channels): BEGIN
    super(RetinaNetModule, self).__init__()
    self.cfg = cfg.clone()
    anchor_generator = make_anchor_generator_retinanet(cfg)

        =================   make_anchor_generator_retinanet(config) BEGIN
        config params
            anc  or_sizes: (32, 64, 128, 256, 512)
            aspect_ratios: (0.5, 1.0, 2.0)
            anc  or_strides: (8, 16, 32, 64, 128)
            straddle_t  res  : -1
            octave: 2.0
            scales_per_octave: 3
        new_anc  or_sizes = []
        for size in anc  or_sizes:
            size: 32
            per_layer_anc  or_sizes = []
            for scale_per_octave in range(scales_per_octave):
                octave : 2.0
                octave : 0
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.0
                size: 32
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [32.0]
                octave : 2.0
                octave : 1
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.2599210498948732
                size: 32
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [32.0, 40.31747359663594]
                octave : 2.0
                octave : 2
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.5874010519681994
                size: 32
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [32.0, 40.31747359663594, 50.79683366298238]
            new_anc  or_sizes.append(tuple(per_layer_anc  or_sizes))
            new_anc  or_sizes: [(32.0, 40.31747359663594, 50.79683366298238)]
            size: 64
            per_layer_anc  or_sizes = []
            for scale_per_octave in range(scales_per_octave):
                octave : 2.0
                octave : 0
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.0
                size: 64
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [64.0]
                octave : 2.0
                octave : 1
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.2599210498948732
                size: 64
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [64.0, 80.63494719327188]
                octave : 2.0
                octave : 2
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.5874010519681994
                size: 64
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [64.0, 80.63494719327188, 101.59366732596476]
            new_anc  or_sizes.append(tuple(per_layer_anc  or_sizes))
            new_anc  or_sizes: [(32.0, 40.31747359663594, 50.79683366298238), (64.0, 80.63494719327188, 101.59366732596476)]
            size: 128
            per_layer_anc  or_sizes = []
            for scale_per_octave in range(scales_per_octave):
                octave : 2.0
                octave : 0
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.0
                size: 128
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [128.0]
                octave : 2.0
                octave : 1
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.2599210498948732
                size: 128
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [128.0, 161.26989438654377]
                octave : 2.0
                octave : 2
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.5874010519681994
                size: 128
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [128.0, 161.26989438654377, 203.18733465192952]
            new_anc  or_sizes.append(tuple(per_layer_anc  or_sizes))
            new_anc  or_sizes: [(32.0, 40.31747359663594, 50.79683366298238), (64.0, 80.63494719327188, 101.59366732596476), (128.0, 161.26989438654377, 203.18733465192952)]
            size: 256
            per_layer_anc  or_sizes = []
            for scale_per_octave in range(scales_per_octave):
                octave : 2.0
                octave : 0
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.0
                size: 256
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [256.0]
                octave : 2.0
                octave : 1
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.2599210498948732
                size: 256
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [256.0, 322.53978877308754]
                octave : 2.0
                octave : 2
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.5874010519681994
                size: 256
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [256.0, 322.53978877308754, 406.37466930385904]
            new_anc  or_sizes.append(tuple(per_layer_anc  or_sizes))
            new_anc  or_sizes: [(32.0, 40.31747359663594, 50.79683366298238), (64.0, 80.63494719327188, 101.59366732596476), (128.0, 161.26989438654377, 203.18733465192952), (256.0, 322.53978877308754, 406.37466930385904)]
            size: 512
            per_layer_anc  or_sizes = []
            for scale_per_octave in range(scales_per_octave):
                octave : 2.0
                octave : 0
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.0
                size: 512
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [512.0]
                octave : 2.0
                octave : 1
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.2599210498948732
                size: 512
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [512.0, 645.0795775461751]
                octave : 2.0
                octave : 2
                octave_scale = octave ** (scale_per_octave / float(scales_per_octave))
                octave_scale: 1.5874010519681994
                size: 512
                per_layer_anc  or_sizes.append(octave_scale * size)
                per_layer_anc  or_sizes: [512.0, 645.0795775461751, 812.7493386077181]
            new_anc  or_sizes.append(tuple(per_layer_anc  or_sizes))
            new_anc  or_sizes: [(32.0, 40.31747359663594, 50.79683366298238), (64.0, 80.63494719327188, 101.59366732596476), (128.0, 161.26989438654377, 203.18733465192952), (256.0, 322.53978877308754, 406.37466930385904), (512.0, 645.0795775461751, 812.7493386077181)]
        new_anc  or_sizes:
            [(32.0, 40.31747359663594, 50.79683366298238), (64.0, 80.63494719327188, 101.59366732596476), (128.0, 161.26989438654377, 203.18733465192952), (256.0, 322.53978877308754, 406.37466930385904), (512.0, 645.0795775461751, 812.7493386077181)]
        aspect_ratios:
            (0.5, 1.0, 2.0)
        anc  or_strides:
            (8, 16, 32, 64, 128)
        straddle_t  res  :
            -1
        anchor_generator = Anc  orGenerator( tuple(new_anc  or_sizes), aspect_ratios, anc  or_strides, straddle_t  res   )
        =================   Anc  orGenerator.__init__(sizes, apect_ratios, anc  or_strides, straddle_t  res  ) BEGIN
            Params
                sizes: ((32.0, 40.31747359663594, 50.79683366298238), (64.0, 80.63494719327188, 101.59366732596476), (128.0, 161.26989438654377, 203.18733465192952), (256.0, 322.53978877308754, 406.37466930385904), (512.0, 645.0795775461751, 812.7493386077181))
                aspect_ratios: (0.5, 1.0, 2.0)
                anc  or_strides: (8, 16, 32, 64, 128)
                straddle_t  res  : -1
        else: i.e, len(anc  or_strides) !=1 
            anc  or_stride = anc  or_strides[0]
            len(anc  or_strides):5, len(size): 5
        else: i.e, len(anc  or_strides) == len(sizes) 
        cell_anchors = [ generate_anchors( anc  or_stride, size if isinstance(size, (tuple, list)) else (size,), aspect_ratios ).float()
                         for anc  or_stride, size in zip(anc  or_strides, sizes)
=================   generate_anchors(stride, sizes, apect_ratios) BEGIN
    Params:
        stride: 8
        sizes: (32.0, 40.31747359663594, 50.79683366298238)
        aspect_ratios: (0.5, 1.0, 2.0)
return _generate_anchors(stride, 
     np.array(sizes, dtype=np.float) / stride,
     np.array(aspect_ratios, dtype=np.float),
=================   generate_anchors(stride, sizes, apect_ratios) END
=================   _generate_anchors(base_size, scales, apect_ratios) BEGIN
=================   _ratio_enum(anc  or, ratios) BEGIN
    Param:
        anc  or: [0. 0. 7. 7.]
        ratios: [0.5 1.  2. ]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [0. 0. 7. 7.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [11.  8.  6.]
          s: [ 6.  8. 12.]
        x_ctr: 3.5
        y_ctr: 3.5
anchors: [[-1.5  1.   8.5  6. ]
 [ 0.   0.   7.   7. ]
 [ 1.  -2.   6.   9. ]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-1.5  1.   8.5  6. ]
 [ 0.   0.   7.   7. ]
 [ 1.  -2.   6.   9. ]]
return anchors
=================   _ratio_enum(anc  or, ratios) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [-1.5  1.   8.5  6. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [-1.5  1.   8.5  6. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [44.         55.4365262  69.84564629]
          s: [24.         30.2381052  38.09762525]
        x_ctr: 3.5
        y_ctr: 3.5
anchors: [[-18.          -8.          25.          15.        ]
 [-23.7182631  -11.1190526   30.7182631   18.1190526 ]
 [-30.92282314 -15.04881262  37.92282314  22.04881262]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-18.          -8.          25.          15.        ]
 [-23.7182631  -11.1190526   30.7182631   18.1190526 ]
 [-30.92282314 -15.04881262  37.92282314  22.04881262]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [0. 0. 7. 7.]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [0. 0. 7. 7.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [32.         40.3174736  50.79683366]
          s: [32.         40.3174736  50.79683366]
        x_ctr: 3.5
        y_ctr: 3.5
anchors: [[-12.         -12.          19.          19.        ]
 [-16.1587368  -16.1587368   23.1587368   23.1587368 ]
 [-21.39841683 -21.39841683  28.39841683  28.39841683]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-12.         -12.          19.          19.        ]
 [-16.1587368  -16.1587368   23.1587368   23.1587368 ]
 [-21.39841683 -21.39841683  28.39841683  28.39841683]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [ 1. -2.  6.  9.]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 1. -2.  6.  9.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [24.         30.2381052  38.09762525]
          s: [48.         60.47621039 76.19525049]
        x_ctr: 3.5
        y_ctr: 3.5
anchors: [[ -8.         -20.          15.          27.        ]
 [-11.1190526  -26.2381052   18.1190526   33.2381052 ]
 [-15.04881262 -34.09762525  22.04881262  41.09762525]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[ -8.         -20.          15.          27.        ]
 [-11.1190526  -26.2381052   18.1190526   33.2381052 ]
 [-15.04881262 -34.09762525  22.04881262  41.09762525]]
=================   _scale_enum(anc  or, scales) END
return torc  .from_numpy(anchors)
=================   _generate_anchors(base_size, scales, apect_ratios) END
=================   generate_anchors(stride, sizes, apect_ratios) BEGIN
    Params:
        stride: 16
        sizes: (64.0, 80.63494719327188, 101.59366732596476)
        aspect_ratios: (0.5, 1.0, 2.0)
return _generate_anchors(stride, 
     np.array(sizes, dtype=np.float) / stride,
     np.array(aspect_ratios, dtype=np.float),
=================   generate_anchors(stride, sizes, apect_ratios) END
=================   _generate_anchors(base_size, scales, apect_ratios) BEGIN
=================   _ratio_enum(anc  or, ratios) BEGIN
    Param:
        anc  or: [ 0.  0. 15. 15.]
        ratios: [0.5 1.  2. ]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 0.  0. 15. 15.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [23. 16. 11.]
          s: [12. 16. 22.]
        x_ctr: 7.5
        y_ctr: 7.5
anchors: [[-3.5  2.  18.5 13. ]
 [ 0.   0.  15.  15. ]
 [ 2.5 -3.  12.5 18. ]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-3.5  2.  18.5 13. ]
 [ 0.   0.  15.  15. ]
 [ 2.5 -3.  12.5 18. ]]
return anchors
=================   _ratio_enum(anc  or, ratios) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [-3.5  2.  18.5 13. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [-3.5  2.  18.5 13. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [ 92.         115.91273659 146.04089678]
          s: [48.         60.47621039 76.19525049]
        x_ctr: 7.5
        y_ctr: 7.5
anchors: [[-38.         -16.          53.          31.        ]
 [-49.9563683  -22.2381052   64.9563683   37.2381052 ]
 [-65.02044839 -30.09762525  80.02044839  45.09762525]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-38.         -16.          53.          31.        ]
 [-49.9563683  -22.2381052   64.9563683   37.2381052 ]
 [-65.02044839 -30.09762525  80.02044839  45.09762525]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [ 0.  0. 15. 15.]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 0.  0. 15. 15.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [ 64.          80.63494719 101.59366733]
          s: [ 64.          80.63494719 101.59366733]
        x_ctr: 7.5
        y_ctr: 7.5
anchors: [[-24.         -24.          39.          39.        ]
 [-32.3174736  -32.3174736   47.3174736   47.3174736 ]
 [-42.79683366 -42.79683366  57.79683366  57.79683366]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-24.         -24.          39.          39.        ]
 [-32.3174736  -32.3174736   47.3174736   47.3174736 ]
 [-42.79683366 -42.79683366  57.79683366  57.79683366]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [ 2.5 -3.  12.5 18. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 2.5 -3.  12.5 18. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [44.         55.4365262  69.84564629]
          s: [ 88.         110.87305239 139.69129257]
        x_ctr: 7.5
        y_ctr: 7.5
anchors: [[-14.         -36.          29.          51.        ]
 [-19.7182631  -47.4365262   34.7182631   62.4365262 ]
 [-26.92282314 -61.84564629  41.92282314  76.84564629]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-14.         -36.          29.          51.        ]
 [-19.7182631  -47.4365262   34.7182631   62.4365262 ]
 [-26.92282314 -61.84564629  41.92282314  76.84564629]]
=================   _scale_enum(anc  or, scales) END
return torc  .from_numpy(anchors)
=================   _generate_anchors(base_size, scales, apect_ratios) END
=================   generate_anchors(stride, sizes, apect_ratios) BEGIN
    Params:
        stride: 32
        sizes: (128.0, 161.26989438654377, 203.18733465192952)
        aspect_ratios: (0.5, 1.0, 2.0)
return _generate_anchors(stride, 
     np.array(sizes, dtype=np.float) / stride,
     np.array(aspect_ratios, dtype=np.float),
=================   generate_anchors(stride, sizes, apect_ratios) END
=================   _generate_anchors(base_size, scales, apect_ratios) BEGIN
=================   _ratio_enum(anc  or, ratios) BEGIN
    Param:
        anc  or: [ 0.  0. 31. 31.]
        ratios: [0.5 1.  2. ]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 0.  0. 31. 31.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [45. 32. 23.]
          s: [22. 32. 46.]
        x_ctr: 15.5
        y_ctr: 15.5
anchors: [[-6.5  5.  37.5 26. ]
 [ 0.   0.  31.  31. ]
 [ 4.5 -7.  26.5 38. ]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-6.5  5.  37.5 26. ]
 [ 0.   0.  31.  31. ]
 [ 4.5 -7.  26.5 38. ]]
return anchors
=================   _ratio_enum(anc  or, ratios) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [-6.5  5.  37.5 26. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [-6.5  5.  37.5 26. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [180.         226.78578898 285.73218935]
          s: [ 88.         110.87305239 139.69129257]
        x_ctr: 15.5
        y_ctr: 15.5
anchors: [[ -74.          -28.          105.           59.        ]
 [ -97.39289449  -39.4365262   128.39289449   70.4365262 ]
 [-126.86609468  -53.84564629  157.86609468   84.84564629]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[ -74.          -28.          105.           59.        ]
 [ -97.39289449  -39.4365262   128.39289449   70.4365262 ]
 [-126.86609468  -53.84564629  157.86609468   84.84564629]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [ 0.  0. 31. 31.]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 0.  0. 31. 31.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [128.         161.26989439 203.18733465]
          s: [128.         161.26989439 203.18733465]
        x_ctr: 15.5
        y_ctr: 15.5
anchors: [[-48.         -48.          79.          79.        ]
 [-64.63494719 -64.63494719  95.63494719  95.63494719]
 [-85.59366733 -85.59366733 116.59366733 116.59366733]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-48.         -48.          79.          79.        ]
 [-64.63494719 -64.63494719  95.63494719  95.63494719]
 [-85.59366733 -85.59366733 116.59366733 116.59366733]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [ 4.5 -7.  26.5 38. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 4.5 -7.  26.5 38. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [ 92.         115.91273659 146.04089678]
          s: [184.         231.82547318 292.08179356]
        x_ctr: 15.5
        y_ctr: 15.5
anchors: [[ -30.          -76.           61.          107.        ]
 [ -41.9563683   -99.91273659   72.9563683   130.91273659]
 [ -57.02044839 -130.04089678   88.02044839  161.04089678]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[ -30.          -76.           61.          107.        ]
 [ -41.9563683   -99.91273659   72.9563683   130.91273659]
 [ -57.02044839 -130.04089678   88.02044839  161.04089678]]
=================   _scale_enum(anc  or, scales) END
return torc  .from_numpy(anchors)
=================   _generate_anchors(base_size, scales, apect_ratios) END
=================   generate_anchors(stride, sizes, apect_ratios) BEGIN
    Params:
        stride: 64
        sizes: (256.0, 322.53978877308754, 406.37466930385904)
        aspect_ratios: (0.5, 1.0, 2.0)
return _generate_anchors(stride, 
     np.array(sizes, dtype=np.float) / stride,
     np.array(aspect_ratios, dtype=np.float),
=================   generate_anchors(stride, sizes, apect_ratios) END
=================   _generate_anchors(base_size, scales, apect_ratios) BEGIN
=================   _ratio_enum(anc  or, ratios) BEGIN
    Param:
        anc  or: [ 0.  0. 63. 63.]
        ratios: [0.5 1.  2. ]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 0.  0. 63. 63.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [91. 64. 45.]
          s: [46. 64. 90.]
        x_ctr: 31.5
        y_ctr: 31.5
anchors: [[-13.5   9.   76.5  54. ]
 [  0.    0.   63.   63. ]
 [  9.5 -13.   53.5  76. ]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-13.5   9.   76.5  54. ]
 [  0.    0.   63.   63. ]
 [  9.5 -13.   53.5  76. ]]
return anchors
=================   _ratio_enum(anc  or, ratios) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [-13.5   9.   76.5  54. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [-13.5   9.   76.5  54. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [364.         458.61126216 577.81398292]
          s: [184.         231.82547318 292.08179356]
        x_ctr: 31.5
        y_ctr: 31.5
anchors: [[-150.          -60.          213.          123.        ]
 [-197.30563108  -83.91273659  260.30563108  146.91273659]
 [-256.90699146 -114.04089678  319.90699146  177.04089678]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-150.          -60.          213.          123.        ]
 [-197.30563108  -83.91273659  260.30563108  146.91273659]
 [-256.90699146 -114.04089678  319.90699146  177.04089678]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [ 0.  0. 63. 63.]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 0.  0. 63. 63.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [256.         322.53978877 406.3746693 ]
          s: [256.         322.53978877 406.3746693 ]
        x_ctr: 31.5
        y_ctr: 31.5
anchors: [[ -96.          -96.          159.          159.        ]
 [-129.26989439 -129.26989439  192.26989439  192.26989439]
 [-171.18733465 -171.18733465  234.18733465  234.18733465]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[ -96.          -96.          159.          159.        ]
 [-129.26989439 -129.26989439  192.26989439  192.26989439]
 [-171.18733465 -171.18733465  234.18733465  234.18733465]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [  9.5 -13.   53.5  76. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [  9.5 -13.   53.5  76. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [180.         226.78578898 285.73218935]
          s: [360.         453.57157796 571.46437871]
        x_ctr: 31.5
        y_ctr: 31.5
anchors: [[ -58.         -148.          121.          211.        ]
 [ -81.39289449 -194.78578898  144.39289449  257.78578898]
 [-110.86609468 -253.73218935  173.86609468  316.73218935]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[ -58.         -148.          121.          211.        ]
 [ -81.39289449 -194.78578898  144.39289449  257.78578898]
 [-110.86609468 -253.73218935  173.86609468  316.73218935]]
=================   _scale_enum(anc  or, scales) END
return torc  .from_numpy(anchors)
=================   _generate_anchors(base_size, scales, apect_ratios) END
=================   generate_anchors(stride, sizes, apect_ratios) BEGIN
    Params:
        stride: 128
        sizes: (512.0, 645.0795775461751, 812.7493386077181)
        aspect_ratios: (0.5, 1.0, 2.0)
return _generate_anchors(stride, 
     np.array(sizes, dtype=np.float) / stride,
     np.array(aspect_ratios, dtype=np.float),
=================   generate_anchors(stride, sizes, apect_ratios) END
=================   _generate_anchors(base_size, scales, apect_ratios) BEGIN
=================   _ratio_enum(anc  or, ratios) BEGIN
    Param:
        anc  or: [  0.   0. 127. 127.]
        ratios: [0.5 1.  2. ]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [  0.   0. 127. 127.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [181. 128.  91.]
          s: [ 90. 128. 182.]
        x_ctr: 63.5
        y_ctr: 63.5
anchors: [[-26.5  19.  153.5 108. ]
 [  0.    0.  127.  127. ]
 [ 18.5 -27.  108.5 154. ]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-26.5  19.  153.5 108. ]
 [  0.    0.  127.  127. ]
 [ 18.5 -27.  108.5 154. ]]
return anchors
=================   _ratio_enum(anc  or, ratios) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [-26.5  19.  153.5 108. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [-26.5  19.  153.5 108. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [ 724.          912.18284012 1149.27836162]
          s: [360.         453.57157796 571.46437871]
        x_ctr: 63.5
        y_ctr: 63.5
anchors: [[-298.         -116.          425.          243.        ]
 [-392.09142006 -162.78578898  519.09142006  289.78578898]
 [-510.63918081 -221.73218935  637.63918081  348.73218935]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-298.         -116.          425.          243.        ]
 [-392.09142006 -162.78578898  519.09142006  289.78578898]
 [-510.63918081 -221.73218935  637.63918081  348.73218935]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [  0.   0. 127. 127.]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [  0.   0. 127. 127.]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [512.         645.07957755 812.74933861]
          s: [512.         645.07957755 812.74933861]
        x_ctr: 63.5
        y_ctr: 63.5
anchors: [[-192.         -192.          319.          319.        ]
 [-258.53978877 -258.53978877  385.53978877  385.53978877]
 [-342.3746693  -342.3746693   469.3746693   469.3746693 ]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-192.         -192.          319.          319.        ]
 [-258.53978877 -258.53978877  385.53978877  385.53978877]
 [-342.3746693  -342.3746693   469.3746693   469.3746693 ]]
=================   _scale_enum(anc  or, scales) END
=================   _scale_enum(anc  or, scales) BEGIN
    Param:
        anc  or: [ 18.5 -27.  108.5 154. ]
        scales: [4.         5.0396842  6.34960421]
=================   _w  ctrs(anchors) BEGIN
    Param:
        anc  or: [ 18.5 -27.  108.5 154. ]
return w,   , x_ctr, y_ctr
=================   _w  ctrs(anchors) END
=================   _mkanchors(ws,   s, x_ctr, y_ctr) BEGIN
    Param:
        ws: [364.         458.61126216 577.81398292]
          s: [ 728.          917.22252432 1155.62796583]
        x_ctr: 63.5
        y_ctr: 63.5
anchors: [[-118.         -300.          245.          427.        ]
 [-165.30563108 -394.61126216  292.30563108  521.61126216]
 [-224.90699146 -513.81398292  351.90699146  640.81398292]]
return anchors
=================   _mkanchors(ws,   s, x_ctr, y_ctr) END
anchors: [[-118.         -300.          245.          427.        ]
 [-165.30563108 -394.61126216  292.30563108  521.61126216]
 [-224.90699146 -513.81398292  351.90699146  640.81398292]]
=================   _scale_enum(anc  or, scales) END
return torc  .from_numpy(anchors)
=================   _generate_anchors(base_size, scales, apect_ratios) END
cell_anchors: [tensor([[-18.0000,  -8.0000,  25.0000,  15.0000],
        [-23.7183, -11.1191,  30.7183,  18.1191],
        [-30.9228, -15.0488,  37.9228,  22.0488],
        [-12.0000, -12.0000,  19.0000,  19.0000],
        [-16.1587, -16.1587,  23.1587,  23.1587],
        [-21.3984, -21.3984,  28.3984,  28.3984],
        [ -8.0000, -20.0000,  15.0000,  27.0000],
        [-11.1191, -26.2381,  18.1191,  33.2381],
        [-15.0488, -34.0976,  22.0488,  41.0976]]), tensor([[-38.0000, -16.0000,  53.0000,  31.0000],
        [-49.9564, -22.2381,  64.9564,  37.2381],
        [-65.0204, -30.0976,  80.0204,  45.0976],
        [-24.0000, -24.0000,  39.0000,  39.0000],
        [-32.3175, -32.3175,  47.3175,  47.3175],
        [-42.7968, -42.7968,  57.7968,  57.7968],
        [-14.0000, -36.0000,  29.0000,  51.0000],
        [-19.7183, -47.4365,  34.7183,  62.4365],
        [-26.9228, -61.8456,  41.9228,  76.8456]]), tensor([[ -74.0000,  -28.0000,  105.0000,   59.0000],
        [ -97.3929,  -39.4365,  128.3929,   70.4365],
        [-126.8661,  -53.8456,  157.8661,   84.8456],
        [ -48.0000,  -48.0000,   79.0000,   79.0000],
        [ -64.6349,  -64.6349,   95.6349,   95.6349],
        [ -85.5937,  -85.5937,  116.5937,  116.5937],
        [ -30.0000,  -76.0000,   61.0000,  107.0000],
        [ -41.9564,  -99.9127,   72.9564,  130.9127],
        [ -57.0204, -130.0409,   88.0204,  161.0409]]), tensor([[-150.0000,  -60.0000,  213.0000,  123.0000],
        [-197.3056,  -83.9127,  260.3056,  146.9127],
        [-256.9070, -114.0409,  319.9070,  177.0409],
        [ -96.0000,  -96.0000,  159.0000,  159.0000],
        [-129.2699, -129.2699,  192.2699,  192.2699],
        [-171.1873, -171.1873,  234.1873,  234.1873],
        [ -58.0000, -148.0000,  121.0000,  211.0000],
        [ -81.3929, -194.7858,  144.3929,  257.7858],
        [-110.8661, -253.7322,  173.8661,  316.7322]]), tensor([[-298.0000, -116.0000,  425.0000,  243.0000],
        [-392.0914, -162.7858,  519.0914,  289.7858],
        [-510.6392, -221.7322,  637.6392,  348.7322],
        [-192.0000, -192.0000,  319.0000,  319.0000],
        [-258.5398, -258.5398,  385.5398,  385.5398],
        [-342.3747, -342.3747,  469.3747,  469.3747],
        [-118.0000, -300.0000,  245.0000,  427.0000],
        [-165.3056, -394.6113,  292.3056,  521.6113],
        [-224.9070, -513.8140,  351.9070,  640.8140]])]
    self.strides: (8, 16, 32, 64, 128)
    self.cel_anchors: BufferList()
    self.straddle_t  res  : -1
=================   Anc  orGenerator.__init__(sizes, apect_ratios, anc  or_strides, straddle_t  res  ) END
        return anchor_generator
        =================   make_anchor_generator_retinanet(config) END

  ead = RetinaNetHead(cfg, in_channels=1024)


=========================================== RetinaNetHead._init__(cfg, in_channels): BEGIN


=========================================== RetinaNetHead._init__(cfg, in_channels): END
box_coder = BoxCoder(weig  ts=(10., 10., 5., 5.))
box_selector_test = make_retinanet_postprocessor(cfg, box_coder)
========== RPNPostProcessing.__init__() BEGIN
========== RPNPostProcessing.__init__() END
self.anchor_generator = anchor_generator
self.  ead =   ead
self.box_selector_test = box_selector_test


=========================================== RetinaNetModule.__init__(self, cfg, in_channels): END
self.rpn: RetinaNetModule(
  (anchor_generator): Anc  orGenerator(
    (cell_anchors): BufferList()
  )
  (  ead): RetinaNetHead(
    (cls_tower): Sequential(
      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
    )
    (bbox_tower): Sequential(
      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
    )
    (cls_logits): Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bbox_pred): Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (box_selector_test): RetinaNetPostProcessor()
)
} // GeneralizedRCNN.__init(self, cfg)  END

INFO:maskrcnn_benc  mark.utils.c  eckpoint:Loading c  eckpoint from ./model/detection/model_det_v2_200924_002_180k.pt  
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn1.bias                  loaded from backbone.body.layer1.0.bn1.bias                  of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn1.running_mean          loaded from backbone.body.layer1.0.bn1.running_mean          of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn1.running_var           loaded from backbone.body.layer1.0.bn1.running_var           of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn1.weig  t                loaded from backbone.body.layer1.0.bn1.weig  t                of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn2.bias                  loaded from backbone.body.layer1.0.bn2.bias                  of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn2.running_mean          loaded from backbone.body.layer1.0.bn2.running_mean          of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn2.running_var           loaded from backbone.body.layer1.0.bn2.running_var           of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn2.weig  t                loaded from backbone.body.layer1.0.bn2.weig  t                of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn3.bias                  loaded from backbone.body.layer1.0.bn3.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn3.running_mean          loaded from backbone.body.layer1.0.bn3.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn3.running_var           loaded from backbone.body.layer1.0.bn3.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.bn3.weig  t                loaded from backbone.body.layer1.0.bn3.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.conv1.weig  t              loaded from backbone.body.layer1.0.conv1.weig  t              of s  ape (64, 64, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.conv2.weig  t              loaded from backbone.body.layer1.0.conv2.weig  t              of s  ape (64, 64, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.conv3.weig  t              loaded from backbone.body.layer1.0.conv3.weig  t              of s  ape (256, 64, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.downsample.0.weig  t       loaded from backbone.body.layer1.0.downsample.0.weig  t       of s  ape (256, 64, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.downsample.1.bias         loaded from backbone.body.layer1.0.downsample.1.bias         of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.downsample.1.running_mean loaded from backbone.body.layer1.0.downsample.1.running_mean of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.downsample.1.running_var  loaded from backbone.body.layer1.0.downsample.1.running_var  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.0.downsample.1.weig  t       loaded from backbone.body.layer1.0.downsample.1.weig  t       of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn1.bias                  loaded from backbone.body.layer1.1.bn1.bias                  of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn1.running_mean          loaded from backbone.body.layer1.1.bn1.running_mean          of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn1.running_var           loaded from backbone.body.layer1.1.bn1.running_var           of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn1.weig  t                loaded from backbone.body.layer1.1.bn1.weig  t                of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn2.bias                  loaded from backbone.body.layer1.1.bn2.bias                  of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn2.running_mean          loaded from backbone.body.layer1.1.bn2.running_mean          of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn2.running_var           loaded from backbone.body.layer1.1.bn2.running_var           of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn2.weig  t                loaded from backbone.body.layer1.1.bn2.weig  t                of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn3.bias                  loaded from backbone.body.layer1.1.bn3.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn3.running_mean          loaded from backbone.body.layer1.1.bn3.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn3.running_var           loaded from backbone.body.layer1.1.bn3.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.bn3.weig  t                loaded from backbone.body.layer1.1.bn3.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.conv1.weig  t              loaded from backbone.body.layer1.1.conv1.weig  t              of s  ape (64, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.conv2.weig  t              loaded from backbone.body.layer1.1.conv2.weig  t              of s  ape (64, 64, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.1.conv3.weig  t              loaded from backbone.body.layer1.1.conv3.weig  t              of s  ape (256, 64, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn1.bias                  loaded from backbone.body.layer1.2.bn1.bias                  of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn1.running_mean          loaded from backbone.body.layer1.2.bn1.running_mean          of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn1.running_var           loaded from backbone.body.layer1.2.bn1.running_var           of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn1.weig  t                loaded from backbone.body.layer1.2.bn1.weig  t                of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn2.bias                  loaded from backbone.body.layer1.2.bn2.bias                  of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn2.running_mean          loaded from backbone.body.layer1.2.bn2.running_mean          of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn2.running_var           loaded from backbone.body.layer1.2.bn2.running_var           of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn2.weig  t                loaded from backbone.body.layer1.2.bn2.weig  t                of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn3.bias                  loaded from backbone.body.layer1.2.bn3.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn3.running_mean          loaded from backbone.body.layer1.2.bn3.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn3.running_var           loaded from backbone.body.layer1.2.bn3.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.bn3.weig  t                loaded from backbone.body.layer1.2.bn3.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.conv1.weig  t              loaded from backbone.body.layer1.2.conv1.weig  t              of s  ape (64, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.conv2.weig  t              loaded from backbone.body.layer1.2.conv2.weig  t              of s  ape (64, 64, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer1.2.conv3.weig  t              loaded from backbone.body.layer1.2.conv3.weig  t              of s  ape (256, 64, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn1.bias                  loaded from backbone.body.layer2.0.bn1.bias                  of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn1.running_mean          loaded from backbone.body.layer2.0.bn1.running_mean          of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn1.running_var           loaded from backbone.body.layer2.0.bn1.running_var           of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn1.weig  t                loaded from backbone.body.layer2.0.bn1.weig  t                of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn2.bias                  loaded from backbone.body.layer2.0.bn2.bias                  of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn2.running_mean          loaded from backbone.body.layer2.0.bn2.running_mean          of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn2.running_var           loaded from backbone.body.layer2.0.bn2.running_var           of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn2.weig  t                loaded from backbone.body.layer2.0.bn2.weig  t                of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn3.bias                  loaded from backbone.body.layer2.0.bn3.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn3.running_mean          loaded from backbone.body.layer2.0.bn3.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn3.running_var           loaded from backbone.body.layer2.0.bn3.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.bn3.weig  t                loaded from backbone.body.layer2.0.bn3.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.conv1.weig  t              loaded from backbone.body.layer2.0.conv1.weig  t              of s  ape (128, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.conv2.weig  t              loaded from backbone.body.layer2.0.conv2.weig  t              of s  ape (128, 128, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.conv3.weig  t              loaded from backbone.body.layer2.0.conv3.weig  t              of s  ape (512, 128, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.downsample.0.weig  t       loaded from backbone.body.layer2.0.downsample.0.weig  t       of s  ape (512, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.downsample.1.bias         loaded from backbone.body.layer2.0.downsample.1.bias         of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.downsample.1.running_mean loaded from backbone.body.layer2.0.downsample.1.running_mean of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.downsample.1.running_var  loaded from backbone.body.layer2.0.downsample.1.running_var  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.0.downsample.1.weig  t       loaded from backbone.body.layer2.0.downsample.1.weig  t       of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn1.bias                  loaded from backbone.body.layer2.1.bn1.bias                  of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn1.running_mean          loaded from backbone.body.layer2.1.bn1.running_mean          of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn1.running_var           loaded from backbone.body.layer2.1.bn1.running_var           of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn1.weig  t                loaded from backbone.body.layer2.1.bn1.weig  t                of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn2.bias                  loaded from backbone.body.layer2.1.bn2.bias                  of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn2.running_mean          loaded from backbone.body.layer2.1.bn2.running_mean          of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn2.running_var           loaded from backbone.body.layer2.1.bn2.running_var           of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn2.weig  t                loaded from backbone.body.layer2.1.bn2.weig  t                of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn3.bias                  loaded from backbone.body.layer2.1.bn3.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn3.running_mean          loaded from backbone.body.layer2.1.bn3.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn3.running_var           loaded from backbone.body.layer2.1.bn3.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.bn3.weig  t                loaded from backbone.body.layer2.1.bn3.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.conv1.weig  t              loaded from backbone.body.layer2.1.conv1.weig  t              of s  ape (128, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.conv2.weig  t              loaded from backbone.body.layer2.1.conv2.weig  t              of s  ape (128, 128, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.1.conv3.weig  t              loaded from backbone.body.layer2.1.conv3.weig  t              of s  ape (512, 128, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn1.bias                  loaded from backbone.body.layer2.2.bn1.bias                  of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn1.running_mean          loaded from backbone.body.layer2.2.bn1.running_mean          of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn1.running_var           loaded from backbone.body.layer2.2.bn1.running_var           of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn1.weig  t                loaded from backbone.body.layer2.2.bn1.weig  t                of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn2.bias                  loaded from backbone.body.layer2.2.bn2.bias                  of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn2.running_mean          loaded from backbone.body.layer2.2.bn2.running_mean          of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn2.running_var           loaded from backbone.body.layer2.2.bn2.running_var           of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn2.weig  t                loaded from backbone.body.layer2.2.bn2.weig  t                of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn3.bias                  loaded from backbone.body.layer2.2.bn3.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn3.running_mean          loaded from backbone.body.layer2.2.bn3.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn3.running_var           loaded from backbone.body.layer2.2.bn3.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.bn3.weig  t                loaded from backbone.body.layer2.2.bn3.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.conv1.weig  t              loaded from backbone.body.layer2.2.conv1.weig  t              of s  ape (128, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.conv2.weig  t              loaded from backbone.body.layer2.2.conv2.weig  t              of s  ape (128, 128, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.2.conv3.weig  t              loaded from backbone.body.layer2.2.conv3.weig  t              of s  ape (512, 128, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn1.bias                  loaded from backbone.body.layer2.3.bn1.bias                  of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn1.running_mean          loaded from backbone.body.layer2.3.bn1.running_mean          of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn1.running_var           loaded from backbone.body.layer2.3.bn1.running_var           of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn1.weig  t                loaded from backbone.body.layer2.3.bn1.weig  t                of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn2.bias                  loaded from backbone.body.layer2.3.bn2.bias                  of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn2.running_mean          loaded from backbone.body.layer2.3.bn2.running_mean          of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn2.running_var           loaded from backbone.body.layer2.3.bn2.running_var           of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn2.weig  t                loaded from backbone.body.layer2.3.bn2.weig  t                of s  ape (128,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn3.bias                  loaded from backbone.body.layer2.3.bn3.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn3.running_mean          loaded from backbone.body.layer2.3.bn3.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn3.running_var           loaded from backbone.body.layer2.3.bn3.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.bn3.weig  t                loaded from backbone.body.layer2.3.bn3.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.conv1.weig  t              loaded from backbone.body.layer2.3.conv1.weig  t              of s  ape (128, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.conv2.weig  t              loaded from backbone.body.layer2.3.conv2.weig  t              of s  ape (128, 128, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer2.3.conv3.weig  t              loaded from backbone.body.layer2.3.conv3.weig  t              of s  ape (512, 128, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn1.bias                  loaded from backbone.body.layer3.0.bn1.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn1.running_mean          loaded from backbone.body.layer3.0.bn1.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn1.running_var           loaded from backbone.body.layer3.0.bn1.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn1.weig  t                loaded from backbone.body.layer3.0.bn1.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn2.bias                  loaded from backbone.body.layer3.0.bn2.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn2.running_mean          loaded from backbone.body.layer3.0.bn2.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn2.running_var           loaded from backbone.body.layer3.0.bn2.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn2.weig  t                loaded from backbone.body.layer3.0.bn2.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn3.bias                  loaded from backbone.body.layer3.0.bn3.bias                  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn3.running_mean          loaded from backbone.body.layer3.0.bn3.running_mean          of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn3.running_var           loaded from backbone.body.layer3.0.bn3.running_var           of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.bn3.weig  t                loaded from backbone.body.layer3.0.bn3.weig  t                of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.conv1.weig  t              loaded from backbone.body.layer3.0.conv1.weig  t              of s  ape (256, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.conv2.weig  t              loaded from backbone.body.layer3.0.conv2.weig  t              of s  ape (256, 256, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.conv3.weig  t              loaded from backbone.body.layer3.0.conv3.weig  t              of s  ape (1024, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.downsample.0.weig  t       loaded from backbone.body.layer3.0.downsample.0.weig  t       of s  ape (1024, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.downsample.1.bias         loaded from backbone.body.layer3.0.downsample.1.bias         of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.downsample.1.running_mean loaded from backbone.body.layer3.0.downsample.1.running_mean of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.downsample.1.running_var  loaded from backbone.body.layer3.0.downsample.1.running_var  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.0.downsample.1.weig  t       loaded from backbone.body.layer3.0.downsample.1.weig  t       of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn1.bias                  loaded from backbone.body.layer3.1.bn1.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn1.running_mean          loaded from backbone.body.layer3.1.bn1.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn1.running_var           loaded from backbone.body.layer3.1.bn1.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn1.weig  t                loaded from backbone.body.layer3.1.bn1.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn2.bias                  loaded from backbone.body.layer3.1.bn2.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn2.running_mean          loaded from backbone.body.layer3.1.bn2.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn2.running_var           loaded from backbone.body.layer3.1.bn2.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn2.weig  t                loaded from backbone.body.layer3.1.bn2.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn3.bias                  loaded from backbone.body.layer3.1.bn3.bias                  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn3.running_mean          loaded from backbone.body.layer3.1.bn3.running_mean          of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn3.running_var           loaded from backbone.body.layer3.1.bn3.running_var           of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.bn3.weig  t                loaded from backbone.body.layer3.1.bn3.weig  t                of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.conv1.weig  t              loaded from backbone.body.layer3.1.conv1.weig  t              of s  ape (256, 1024, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.conv2.weig  t              loaded from backbone.body.layer3.1.conv2.weig  t              of s  ape (256, 256, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.1.conv3.weig  t              loaded from backbone.body.layer3.1.conv3.weig  t              of s  ape (1024, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn1.bias                  loaded from backbone.body.layer3.2.bn1.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn1.running_mean          loaded from backbone.body.layer3.2.bn1.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn1.running_var           loaded from backbone.body.layer3.2.bn1.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn1.weig  t                loaded from backbone.body.layer3.2.bn1.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn2.bias                  loaded from backbone.body.layer3.2.bn2.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn2.running_mean          loaded from backbone.body.layer3.2.bn2.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn2.running_var           loaded from backbone.body.layer3.2.bn2.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn2.weig  t                loaded from backbone.body.layer3.2.bn2.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn3.bias                  loaded from backbone.body.layer3.2.bn3.bias                  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn3.running_mean          loaded from backbone.body.layer3.2.bn3.running_mean          of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn3.running_var           loaded from backbone.body.layer3.2.bn3.running_var           of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.bn3.weig  t                loaded from backbone.body.layer3.2.bn3.weig  t                of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.conv1.weig  t              loaded from backbone.body.layer3.2.conv1.weig  t              of s  ape (256, 1024, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.conv2.weig  t              loaded from backbone.body.layer3.2.conv2.weig  t              of s  ape (256, 256, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.2.conv3.weig  t              loaded from backbone.body.layer3.2.conv3.weig  t              of s  ape (1024, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn1.bias                  loaded from backbone.body.layer3.3.bn1.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn1.running_mean          loaded from backbone.body.layer3.3.bn1.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn1.running_var           loaded from backbone.body.layer3.3.bn1.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn1.weig  t                loaded from backbone.body.layer3.3.bn1.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn2.bias                  loaded from backbone.body.layer3.3.bn2.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn2.running_mean          loaded from backbone.body.layer3.3.bn2.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn2.running_var           loaded from backbone.body.layer3.3.bn2.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn2.weig  t                loaded from backbone.body.layer3.3.bn2.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn3.bias                  loaded from backbone.body.layer3.3.bn3.bias                  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn3.running_mean          loaded from backbone.body.layer3.3.bn3.running_mean          of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn3.running_var           loaded from backbone.body.layer3.3.bn3.running_var           of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.bn3.weig  t                loaded from backbone.body.layer3.3.bn3.weig  t                of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.conv1.weig  t              loaded from backbone.body.layer3.3.conv1.weig  t              of s  ape (256, 1024, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.conv2.weig  t              loaded from backbone.body.layer3.3.conv2.weig  t              of s  ape (256, 256, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.3.conv3.weig  t              loaded from backbone.body.layer3.3.conv3.weig  t              of s  ape (1024, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn1.bias                  loaded from backbone.body.layer3.4.bn1.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn1.running_mean          loaded from backbone.body.layer3.4.bn1.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn1.running_var           loaded from backbone.body.layer3.4.bn1.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn1.weig  t                loaded from backbone.body.layer3.4.bn1.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn2.bias                  loaded from backbone.body.layer3.4.bn2.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn2.running_mean          loaded from backbone.body.layer3.4.bn2.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn2.running_var           loaded from backbone.body.layer3.4.bn2.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn2.weig  t                loaded from backbone.body.layer3.4.bn2.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn3.bias                  loaded from backbone.body.layer3.4.bn3.bias                  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn3.running_mean          loaded from backbone.body.layer3.4.bn3.running_mean          of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn3.running_var           loaded from backbone.body.layer3.4.bn3.running_var           of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.bn3.weig  t                loaded from backbone.body.layer3.4.bn3.weig  t                of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.conv1.weig  t              loaded from backbone.body.layer3.4.conv1.weig  t              of s  ape (256, 1024, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.conv2.weig  t              loaded from backbone.body.layer3.4.conv2.weig  t              of s  ape (256, 256, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.4.conv3.weig  t              loaded from backbone.body.layer3.4.conv3.weig  t              of s  ape (1024, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn1.bias                  loaded from backbone.body.layer3.5.bn1.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn1.running_mean          loaded from backbone.body.layer3.5.bn1.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn1.running_var           loaded from backbone.body.layer3.5.bn1.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn1.weig  t                loaded from backbone.body.layer3.5.bn1.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn2.bias                  loaded from backbone.body.layer3.5.bn2.bias                  of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn2.running_mean          loaded from backbone.body.layer3.5.bn2.running_mean          of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn2.running_var           loaded from backbone.body.layer3.5.bn2.running_var           of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn2.weig  t                loaded from backbone.body.layer3.5.bn2.weig  t                of s  ape (256,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn3.bias                  loaded from backbone.body.layer3.5.bn3.bias                  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn3.running_mean          loaded from backbone.body.layer3.5.bn3.running_mean          of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn3.running_var           loaded from backbone.body.layer3.5.bn3.running_var           of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.bn3.weig  t                loaded from backbone.body.layer3.5.bn3.weig  t                of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.conv1.weig  t              loaded from backbone.body.layer3.5.conv1.weig  t              of s  ape (256, 1024, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.conv2.weig  t              loaded from backbone.body.layer3.5.conv2.weig  t              of s  ape (256, 256, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer3.5.conv3.weig  t              loaded from backbone.body.layer3.5.conv3.weig  t              of s  ape (1024, 256, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn1.bias                  loaded from backbone.body.layer4.0.bn1.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn1.running_mean          loaded from backbone.body.layer4.0.bn1.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn1.running_var           loaded from backbone.body.layer4.0.bn1.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn1.weig  t                loaded from backbone.body.layer4.0.bn1.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn2.bias                  loaded from backbone.body.layer4.0.bn2.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn2.running_mean          loaded from backbone.body.layer4.0.bn2.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn2.running_var           loaded from backbone.body.layer4.0.bn2.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn2.weig  t                loaded from backbone.body.layer4.0.bn2.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn3.bias                  loaded from backbone.body.layer4.0.bn3.bias                  of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn3.running_mean          loaded from backbone.body.layer4.0.bn3.running_mean          of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn3.running_var           loaded from backbone.body.layer4.0.bn3.running_var           of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.bn3.weig  t                loaded from backbone.body.layer4.0.bn3.weig  t                of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.conv1.weig  t              loaded from backbone.body.layer4.0.conv1.weig  t              of s  ape (512, 1024, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.conv2.weig  t              loaded from backbone.body.layer4.0.conv2.weig  t              of s  ape (512, 512, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.conv3.weig  t              loaded from backbone.body.layer4.0.conv3.weig  t              of s  ape (2048, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.downsample.0.weig  t       loaded from backbone.body.layer4.0.downsample.0.weig  t       of s  ape (2048, 1024, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.downsample.1.bias         loaded from backbone.body.layer4.0.downsample.1.bias         of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.downsample.1.running_mean loaded from backbone.body.layer4.0.downsample.1.running_mean of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.downsample.1.running_var  loaded from backbone.body.layer4.0.downsample.1.running_var  of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.0.downsample.1.weig  t       loaded from backbone.body.layer4.0.downsample.1.weig  t       of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn1.bias                  loaded from backbone.body.layer4.1.bn1.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn1.running_mean          loaded from backbone.body.layer4.1.bn1.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn1.running_var           loaded from backbone.body.layer4.1.bn1.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn1.weig  t                loaded from backbone.body.layer4.1.bn1.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn2.bias                  loaded from backbone.body.layer4.1.bn2.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn2.running_mean          loaded from backbone.body.layer4.1.bn2.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn2.running_var           loaded from backbone.body.layer4.1.bn2.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn2.weig  t                loaded from backbone.body.layer4.1.bn2.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn3.bias                  loaded from backbone.body.layer4.1.bn3.bias                  of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn3.running_mean          loaded from backbone.body.layer4.1.bn3.running_mean          of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn3.running_var           loaded from backbone.body.layer4.1.bn3.running_var           of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.bn3.weig  t                loaded from backbone.body.layer4.1.bn3.weig  t                of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.conv1.weig  t              loaded from backbone.body.layer4.1.conv1.weig  t              of s  ape (512, 2048, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.conv2.weig  t              loaded from backbone.body.layer4.1.conv2.weig  t              of s  ape (512, 512, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.1.conv3.weig  t              loaded from backbone.body.layer4.1.conv3.weig  t              of s  ape (2048, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn1.bias                  loaded from backbone.body.layer4.2.bn1.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn1.running_mean          loaded from backbone.body.layer4.2.bn1.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn1.running_var           loaded from backbone.body.layer4.2.bn1.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn1.weig  t                loaded from backbone.body.layer4.2.bn1.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn2.bias                  loaded from backbone.body.layer4.2.bn2.bias                  of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn2.running_mean          loaded from backbone.body.layer4.2.bn2.running_mean          of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn2.running_var           loaded from backbone.body.layer4.2.bn2.running_var           of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn2.weig  t                loaded from backbone.body.layer4.2.bn2.weig  t                of s  ape (512,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn3.bias                  loaded from backbone.body.layer4.2.bn3.bias                  of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn3.running_mean          loaded from backbone.body.layer4.2.bn3.running_mean          of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn3.running_var           loaded from backbone.body.layer4.2.bn3.running_var           of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.bn3.weig  t                loaded from backbone.body.layer4.2.bn3.weig  t                of s  ape (2048,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.conv1.weig  t              loaded from backbone.body.layer4.2.conv1.weig  t              of s  ape (512, 2048, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.conv2.weig  t              loaded from backbone.body.layer4.2.conv2.weig  t              of s  ape (512, 512, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.layer4.2.conv3.weig  t              loaded from backbone.body.layer4.2.conv3.weig  t              of s  ape (2048, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.stem.bn1.bias                      loaded from backbone.body.stem.bn1.bias                      of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.stem.bn1.running_mean              loaded from backbone.body.stem.bn1.running_mean              of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.stem.bn1.running_var               loaded from backbone.body.stem.bn1.running_var               of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.stem.bn1.weig  t                    loaded from backbone.body.stem.bn1.weig  t                    of s  ape (64,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.body.stem.conv1.weig  t                  loaded from backbone.body.stem.conv1.weig  t                  of s  ape (64, 3, 7, 7)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_inner2.bias                     loaded from backbone.fpn.fpn_inner2.bias                     of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_inner2.weig  t                   loaded from backbone.fpn.fpn_inner2.weig  t                   of s  ape (1024, 512, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_inner3.bias                     loaded from backbone.fpn.fpn_inner3.bias                     of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_inner3.weig  t                   loaded from backbone.fpn.fpn_inner3.weig  t                   of s  ape (1024, 1024, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_inner4.bias                     loaded from backbone.fpn.fpn_inner4.bias                     of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_inner4.weig  t                   loaded from backbone.fpn.fpn_inner4.weig  t                   of s  ape (1024, 2048, 1, 1)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_layer2.bias                     loaded from backbone.fpn.fpn_layer2.bias                     of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_layer2.weig  t                   loaded from backbone.fpn.fpn_layer2.weig  t                   of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_layer3.bias                     loaded from backbone.fpn.fpn_layer3.bias                     of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_layer3.weig  t                   loaded from backbone.fpn.fpn_layer3.weig  t                   of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_layer4.bias                     loaded from backbone.fpn.fpn_layer4.bias                     of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.fpn_layer4.weig  t                   loaded from backbone.fpn.fpn_layer4.weig  t                   of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.top_blocks.p6.bias                  loaded from backbone.fpn.top_blocks.p6.bias                  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.top_blocks.p6.weig  t                loaded from backbone.fpn.top_blocks.p6.weig  t                of s  ape (1024, 2048, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.top_blocks.p7.bias                  loaded from backbone.fpn.top_blocks.p7.bias                  of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:backbone.fpn.top_blocks.p7.weig  t                loaded from backbone.fpn.top_blocks.p7.weig  t                of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.anchor_generator.cell_anchors.0              loaded from rpn.anchor_generator.cell_anchors.0              of s  ape (9, 4)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.anchor_generator.cell_anchors.1              loaded from rpn.anchor_generator.cell_anchors.1              of s  ape (9, 4)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.anchor_generator.cell_anchors.2              loaded from rpn.anchor_generator.cell_anchors.2              of s  ape (9, 4)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.anchor_generator.cell_anchors.3              loaded from rpn.anchor_generator.cell_anchors.3              of s  ape (9, 4)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.anchor_generator.cell_anchors.4              loaded from rpn.anchor_generator.cell_anchors.4              of s  ape (9, 4)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_pred.bias                          loaded from rpn.  ead.bbox_pred.bias                          of s  ape (36,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_pred.weig  t                        loaded from rpn.  ead.bbox_pred.weig  t                        of s  ape (36, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_tower.0.bias                       loaded from rpn.  ead.bbox_tower.0.bias                       of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_tower.0.weig  t                     loaded from rpn.  ead.bbox_tower.0.weig  t                     of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_tower.2.bias                       loaded from rpn.  ead.bbox_tower.2.bias                       of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_tower.2.weig  t                     loaded from rpn.  ead.bbox_tower.2.weig  t                     of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_tower.4.bias                       loaded from rpn.  ead.bbox_tower.4.bias                       of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_tower.4.weig  t                     loaded from rpn.  ead.bbox_tower.4.weig  t                     of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_tower.6.bias                       loaded from rpn.  ead.bbox_tower.6.bias                       of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.bbox_tower.6.weig  t                     loaded from rpn.  ead.bbox_tower.6.weig  t                     of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_logits.bias                         loaded from rpn.  ead.cls_logits.bias                         of s  ape (9,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_logits.weig  t                       loaded from rpn.  ead.cls_logits.weig  t                       of s  ape (9, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_tower.0.bias                        loaded from rpn.  ead.cls_tower.0.bias                        of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_tower.0.weig  t                      loaded from rpn.  ead.cls_tower.0.weig  t                      of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_tower.2.bias                        loaded from rpn.  ead.cls_tower.2.bias                        of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_tower.2.weig  t                      loaded from rpn.  ead.cls_tower.2.weig  t                      of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_tower.4.bias                        loaded from rpn.  ead.cls_tower.4.bias                        of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_tower.4.weig  t                      loaded from rpn.  ead.cls_tower.4.weig  t                      of s  ape (1024, 1024, 3, 3)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_tower.6.bias                        loaded from rpn.  ead.cls_tower.6.bias                        of s  ape (1024,)
INFO:maskrcnn_benc  mark.utils.model_serialization:rpn.  ead.cls_tower.6.weig  t                      loaded from rpn.  ead.cls_tower.6.weig  t                      of s  ape (1024, 1024, 3, 3)
compute_prediction(self, image)

    image: H, W=(438,512)

    image_tensor = self.transforms(image)

        transforms.py Compose class __call__  ====== BEGIN

        for t in self.transforms:
            image = <maskrcnn_benc  mark.data.transforms.transforms.Resize object at 0x7f8128a5b080>(image)
            image = <maskrcnn_benc  mark.data.transforms.transforms.ToTensor object at 0x7f8128a5b128>(image)
            image = <maskrcnn_benc  mark.data.transforms.transforms.Normalize object at 0x7f8128a5b0b8>(image)

        return image
        transforms.py Compose class __call__  ====== END

    image_tensor.s  ape: torc  .Size([3, 480, 561])

    padding images for 32 divisible size on widt   and   eig  t
    image_list = to_image_list(image_tensor, 32).to(self.device)

    to_image_list(tensors, size_divisible=32) ====== BEGIN
        type(batc  ed_imgs): <class 'torc  .Tensor'>
        batc  ed_imgs.s  ape: torc  .Size([1, 3, 480, 576])
        image_sizes: [torc  .Size([480, 561])]
        return ImageList(batc  ed_imgs, image_sizes)
    to_image_list(tensors, size_divisible=32) ====== END

    image_list.image_sizes: [torc  .Size([480, 561])]
    image_list.tensors.s  ape: torc  .Size([1, 3, 480, 576])
    pred = self.model(image_list)


GeneralizedRCNN.forward(self, images, targets=None) ====================== BEGIN
type(images): <class 'maskrcnn_benc  mark.structures.image_list.ImageList'>
targets: None
    if self.training == False: 
    images = to_image_list(images)

    to_image_list(tensors, size_divisible=0) ====== BEGIN
        if isinstance(tensors, ImageList):
        return tensors
    to_image_list(tensors, size_divisible=0) ====== END

    images.image_sizes: [torc  .Size([480, 561])]
    images.tensors.s  ape: torc  .Size([1, 3, 480, 576])
    model.backbone.forward(images.tensors) BEFORE

=========================================== Resnet.forward(self, x) BEGIN
    Param
        x.s  ape=torc  .Size([1, 3, 480, 576])

    x = self.stem(x)
    x.s  ape: torc  .Size([1, 64, 120, 144])
    stem output of s  ape (1, 64, 120, 144) saved into ./npy_save/stem_output.npy


    for stage_name in self.stages:
        stage_name: layer1
            output s  ape of layer1: torc  .Size([1, 256, 120, 144])
            outputs.append(x) stage_name: layer1
            x.s  ape: torc  .Size([1, 256, 120, 144])
    layer1 output of s  ape (1, 256, 120, 144) saved into ./npy_save/layer1_output.npy


        stage_name: layer2
            output s  ape of layer2: torc  .Size([1, 512, 60, 72])
            outputs.append(x) stage_name: layer2
            x.s  ape: torc  .Size([1, 512, 60, 72])
    layer2 output of s  ape (1, 512, 60, 72) saved into ./npy_save/layer2_output.npy


        stage_name: layer3
            output s  ape of layer3: torc  .Size([1, 1024, 30, 36])
            outputs.append(x) stage_name: layer3
            x.s  ape: torc  .Size([1, 1024, 30, 36])
    layer3 output of s  ape (1, 1024, 30, 36) saved into ./npy_save/layer3_output.npy


        stage_name: layer4
            output s  ape of layer4: torc  .Size([1, 2048, 15, 18])
            outputs.append(x) stage_name: layer4
            x.s  ape: torc  .Size([1, 2048, 15, 18])
    layer4 output of s  ape (1, 2048, 15, 18) saved into ./npy_save/layer4_output.npy



    ResNet::forward return value
        outputs[0]: torc  .Size([1, 256, 120, 144])
        outputs[1]: torc  .Size([1, 512, 60, 72])
        outputs[2]: torc  .Size([1, 1024, 30, 36])
        outputs[3]: torc  .Size([1, 2048, 15, 18])

    return outputs

=========================================== Resnet.forward() END


FPN.forward(self,x) ====== BEGIN
    ======forward param: x  = [C1, C2, C3, C4] 
    len(x) = 4
    C[1].s  ape : torc  .Size([1, 256, 120, 144])
    C[2].s  ape : torc  .Size([1, 512, 60, 72])
    C[3].s  ape : torc  .Size([1, 1024, 30, 36])
    C[4].s  ape : torc  .Size([1, 2048, 15, 18])

    x[-1].s  ape = torc  .Size([1, 2048, 15, 18])

    last_inner = fpn_inner4(C4)
        self.innerblocks[-1] = Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
        last_inner.s  ape = torc  .Size([1, 1024, 15, 18])

    fpn_inner4' output of s  ape (1, 1024, 15, 18) saved into ./npy_save/fpn_inner4_output.npy



    results.append(fpn_layer4(last_inner))
        self.layer_blocks[-1]: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        results[0].s  ape: torc  .Size([1, 1024, 15, 18])

    fpn_layer4 output of s  ape (1, 1024, 15, 18) saved into ./npy_save/fpn_layer4_output.npy


    for feature, inner_block, layer_block
            in zip[(x[:-1][::-1], self.inner_blocks[:-1][::-1], self.layer_blocks[:-1][::-1]):

        ====================================
        iteration 0 summary
        ====================================
        feature.s  ape: torc  .Size([1, 1024, 30, 36])
        inner_block: fpn_inner3 ==> Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        layer_block: fpn_layer3 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        last_inner.s  ape: torc  .Size([1, 1024, 15, 18])
        ====================================

        --------------------------------------------------
        0.1 Upsample : replace wit   Decovolution in caffe
        layer name in caffe: fpn_inner3_upsample = Deconvolution(last_inner)
        --------------------------------------------------
        inner_top_down = F.interpolate(last_inner, scale_factor=2, mode='nearest'
        last_inner.s  ape: torc  .Size([1, 1024, 15, 18])
        inner_top_down.s  ape : torc  .Size([1, 1024, 30, 36])
        --------------------------------------------------

        inner_top_down of s  ape (1, 1024, 30, 36) saved into ./npy_save/inner_top_down_forfpn_inner3.npy


        --------------------------------------------------
        0.2 inner_lateral = Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
        layer name in caffe: fpn_inner3_lateral=Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
        --------------------------------------------------
            inner_block: fpn_inner3 ==> Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
            input: feature.s  ape: torc  .Size([1, 1024, 30, 36])
            output: inner_lateral.s  ape: torc  .Size([1, 1024, 30, 36])

        --------------------------------------------------

        fpn_inner3 output of s  ape (1, 1024, 30, 36) saved into ./npy_save/fpn_inner3_output.npy


        --------------------------------------------------
        0.3 Elementwise Addition: replaced wit   eltwise in caffe
        layer in caffe: eltwise_3 = eltwise(fpn_inner3_lateral, fpn_inner3_upsample )
        --------------------------------------------------
        last_inner = inner_lateral + inner_top_down
            inner_lateral.s  ape: torc  .Size([1, 1024, 30, 36])
            inner_top_down.s  ape: torc  .Size([1, 1024, 30, 36])
            last_inner.s  ape : torc  .Size([1, 1024, 30, 36])
        --------------------------------------------------

        superimposing result of fpn_inner3 output plus inner topdown of s  ape (1, 1024, 30, 36) saved into ./npy_save/fpn_inner3_ouptut_plus_inner_topdown.npy


        --------------------------------------------------
        0.4 results.insert(0, Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(last_inner)
        layer in caffe: fpn_layer3 = Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(eltwise_3)
        --------------------------------------------------
            layer_block: fpn_layer3 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            input: last_inner.s  ape = torc  .Size([1, 1024, 30, 36])
        --------------------------------------------------

        fpn_layer3 output of s  ape (1, 1024, 30, 36) saved into ./npy_save/fpn_layer3_ouptut.npy


        --------------------------------------------------
        results after iteration 0
        --------------------------------------------------
            results[0].s  ape: torc  .Size([1, 1024, 30, 36])
            results[1].s  ape: torc  .Size([1, 1024, 15, 18])
        --------------------------------------------------

        ====================================
        iteration 1 summary
        ====================================
        feature.s  ape: torc  .Size([1, 512, 60, 72])
        inner_block: fpn_inner2 ==> Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        layer_block: fpn_layer2 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        last_inner.s  ape: torc  .Size([1, 1024, 30, 36])
        ====================================

        --------------------------------------------------
        1.1 Upsample : replace wit   Decovolution in caffe
        layer name in caffe: fpn_inner2_upsample = Deconvolution(last_inner)
        --------------------------------------------------
        inner_top_down = F.interpolate(last_inner, scale_factor=2, mode='nearest'
        last_inner.s  ape: torc  .Size([1, 1024, 30, 36])
        inner_top_down.s  ape : torc  .Size([1, 1024, 60, 72])
        --------------------------------------------------

        inner_top_down of s  ape (1, 1024, 60, 72) saved into ./npy_save/inner_top_down_forfpn_inner2.npy


        --------------------------------------------------
        1.2 inner_lateral = Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
        layer name in caffe: fpn_inner2_lateral=Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
        --------------------------------------------------
            inner_block: fpn_inner2 ==> Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
            input: feature.s  ape: torc  .Size([1, 512, 60, 72])
            output: inner_lateral.s  ape: torc  .Size([1, 1024, 60, 72])

        --------------------------------------------------

        fpn_inner2 output of s  ape (1, 1024, 60, 72) saved into ./npy_save/fpn_inner2_output.npy


        --------------------------------------------------
        1.3 Elementwise Addition: replaced wit   eltwise in caffe
        layer in caffe: eltwise_2 = eltwise(fpn_inner2_lateral, fpn_inner2_upsample )
        --------------------------------------------------
        last_inner = inner_lateral + inner_top_down
            inner_lateral.s  ape: torc  .Size([1, 1024, 60, 72])
            inner_top_down.s  ape: torc  .Size([1, 1024, 60, 72])
            last_inner.s  ape : torc  .Size([1, 1024, 60, 72])
        --------------------------------------------------

        superimposing result of fpn_inner2 output plus inner topdown of s  ape (1, 1024, 60, 72) saved into ./npy_save/fpn_inner2_ouptut_plus_inner_topdown.npy


        --------------------------------------------------
        1.4 results.insert(0, Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(last_inner)
        layer in caffe: fpn_layer2 = Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(eltwise_2)
        --------------------------------------------------
            layer_block: fpn_layer2 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            input: last_inner.s  ape = torc  .Size([1, 1024, 60, 72])
        --------------------------------------------------

        fpn_layer2 output of s  ape (1, 1024, 60, 72) saved into ./npy_save/fpn_layer2_ouptut.npy


        --------------------------------------------------
        results after iteration 1
        --------------------------------------------------
            results[0].s  ape: torc  .Size([1, 1024, 60, 72])
            results[1].s  ape: torc  .Size([1, 1024, 30, 36])
            results[2].s  ape: torc  .Size([1, 1024, 15, 18])
        --------------------------------------------------

    for loop END


    if isinstance(self.top_blocks, LastLevelP6P7):
            self.top_blocks: LastLevelP6P7(
  (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
            len(x): 4
            x[0].s  ape : torc  .Size([1, 256, 120, 144])
            x[1].s  ape : torc  .Size([1, 512, 60, 72])
            x[2].s  ape : torc  .Size([1, 1024, 30, 36])
            x[3].s  ape : torc  .Size([1, 2048, 15, 18])
            x[-1].s  ape: torc  .Size([1, 2048, 15, 18])


            len(results): 3
            results[0].s  ape : torc  .Size([1, 1024, 60, 72])
            results[1].s  ape : torc  .Size([1, 1024, 30, 36])
            results[2].s  ape : torc  .Size([1, 1024, 15, 18])
            results[-1].s  ape: torc  .Size([1, 1024, 15, 18])



        LastLevelP6P7.forward(self, c5, p5) ============= BEGIN 
            c5.s  ape: torc  .Size([1, 2048, 15, 18])
            p5.s  ape: torc  .Size([1, 1024, 15, 18])

            if (self.use_P5 == False)
                x=c5
            x.s  ape = torc  .Size([1, 2048, 15, 18])
            p6 = self.p6(x)
                self.p6: Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                p6.s  ape: torc  .Size([1, 1024, 8, 9])

        LastLevelP6P7::forward() self.p6 ==> Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) output of s  ape (1, 1024, 8, 9) saved into ./npy_save/P6.npy


            p7 = self.p7(F.relu(p6))
                self.p7: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                p7.s  ape: torc  .Size([1, 1024, 4, 5])

        LastLevelP6P7::forward() self.p7 Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))(F.relu(p6)) output of s  ape (1, 1024, 4, 5) saved into ./npy_save/P7.npy


            returns [p6, p7]
        LastLevelP6P7.forward(self, c5, p5) ============= END


        last_result = self.top_blocks(x[-1], results[-1])
            x[-1] => c5, results[-1])=> p5
            len(last_result):2
            last_results[0].s  ape : torc  .Size([1, 1024, 8, 9])
            last_results[1].s  ape : torc  .Size([1, 1024, 4, 5])
        results.extend(last_results)
            len(results): 5
            results[0].s  ape : torc  .Size([1, 1024, 60, 72])
            results[1].s  ape : torc  .Size([1, 1024, 30, 36])
            results[2].s  ape : torc  .Size([1, 1024, 15, 18])
            results[3].s  ape : torc  .Size([1, 1024, 8, 9])
            results[4].s  ape : torc  .Size([1, 1024, 4, 5])




        results
        result[0].s  ape: torc  .Size([1, 1024, 60, 72])
        result[1].s  ape: torc  .Size([1, 1024, 30, 36])
        result[2].s  ape: torc  .Size([1, 1024, 15, 18])
        result[3].s  ape: torc  .Size([1, 1024, 8, 9])
        result[4].s  ape: torc  .Size([1, 1024, 4, 5])

    return tuple(results)


FPN.forward(self,x) ====== END
    model.backbone.forward(images.tensors) DONE
proposals, proposal_losses = self.rpn(images, features, targets) BEFORE


=========================================== RetinaNetModule.forward(self, images, features, targets=None): BEGIN
    Params:
        type(images.image_size): <class 'list'>
        type(images.tensors): <class 'torc  .Tensor'>
        len(features)): 5
            feature[0].s  ape: torc  .Size([1, 1024, 60, 72])
            feature[1].s  ape: torc  .Size([1, 1024, 30, 36])
            feature[2].s  ape: torc  .Size([1, 1024, 15, 18])
            feature[3].s  ape: torc  .Size([1, 1024, 8, 9])
            feature[4].s  ape: torc  .Size([1, 1024, 4, 5])
self.  ead: RetinaNetHead(
  (cls_tower): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
  )
  (bbox_tower): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
  )
  (cls_logits): Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bbox_pred): Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
box_cls, box_regression = self.  ead(features)


=========================================== RetinaNetHead.forward(self, x): BEGIN
    Param:
        len(x)): 5 x is features returned from FPN
            x[0].s  ape: torc  .Size([1, 1024, 60, 72])
            x[1].s  ape: torc  .Size([1, 1024, 30, 36])
            x[2].s  ape: torc  .Size([1, 1024, 15, 18])
            x[3].s  ape: torc  .Size([1, 1024, 8, 9])
            x[4].s  ape: torc  .Size([1, 1024, 4, 5])
logits = []
bbox_reg = []



for feature in x:
    ===== iteration: 0 ====
    feature[0].s  ape: torc  .Size([1, 1024, 60, 72])

    cls_tower => cls_logits => logits[]
    logits.append(self.cls_logits(self.cls_tower(feature)))

    bbox_tower => bbox_pre => bbox_reg[]
    bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

    ===== iteration: 1 ====
    feature[1].s  ape: torc  .Size([1, 1024, 30, 36])

    cls_tower => cls_logits => logits[]
    logits.append(self.cls_logits(self.cls_tower(feature)))

    bbox_tower => bbox_pre => bbox_reg[]
    bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

    ===== iteration: 2 ====
    feature[2].s  ape: torc  .Size([1, 1024, 15, 18])

    cls_tower => cls_logits => logits[]
    logits.append(self.cls_logits(self.cls_tower(feature)))

    bbox_tower => bbox_pre => bbox_reg[]
    bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

    ===== iteration: 3 ====
    feature[3].s  ape: torc  .Size([1, 1024, 8, 9])

    cls_tower => cls_logits => logits[]
    logits.append(self.cls_logits(self.cls_tower(feature)))

    bbox_tower => bbox_pre => bbox_reg[]
    bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

    ===== iteration: 4 ====
    feature[4].s  ape: torc  .Size([1, 1024, 4, 5])

    cls_tower => cls_logits => logits[]
    logits.append(self.cls_logits(self.cls_tower(feature)))

    bbox_tower => bbox_pre => bbox_reg[]
    bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

 ==== logits ====
logits[0].s  ape: torc  .Size([1, 9, 60, 72])
logits[1].s  ape: torc  .Size([1, 9, 30, 36])
logits[2].s  ape: torc  .Size([1, 9, 15, 18])
logits[3].s  ape: torc  .Size([1, 9, 8, 9])
logits[4].s  ape: torc  .Size([1, 9, 4, 5])

 ==== bbox_reg ====
bbox_reg[0].s  ape: torc  .Size([1, 36, 60, 72])
bbox_reg[1].s  ape: torc  .Size([1, 36, 30, 36])
bbox_reg[2].s  ape: torc  .Size([1, 36, 15, 18])
bbox_reg[3].s  ape: torc  .Size([1, 36, 8, 9])
bbox_reg[4].s  ape: torc  .Size([1, 36, 4, 5])

return logits, bbox_reg


=========================================== RetinaNetHead.forward(self, x): END
self.anchor_generator: Anc  orGenerator(
  (cell_anchors): BufferList()
)
anchors = self.anchor_generator(images, features)
=================   Anc  orGenerator.forward(image_list, feature_maps) BEGIN
    Params:
        image_list:
            len(image_list.image_sizes): 1
            image_list.image_sizes[0]: torc  .Size([480, 561])
            len(image_list.tensors): 1
            image_list.tensors[0].s  ape: torc  .Size([3, 480, 576])
        feature_maps:
            feature_maps[0].s  ape: torc  .Size([1, 1024, 60, 72])
            feature_maps[1].s  ape: torc  .Size([1, 1024, 30, 36])
            feature_maps[2].s  ape: torc  .Size([1, 1024, 15, 18])
            feature_maps[3].s  ape: torc  .Size([1, 1024, 8, 9])
            feature_maps[4].s  ape: torc  .Size([1, 1024, 4, 5])

grid_sizes = [feature_map.s  ape[-2:] for feature_map in feature_maps]
anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)
=================   Anc  orGenerator.grid_anchors(grid_sizes) BEGIN
    Param:
        grid_sizes: [torc  .Size([60, 72]), torc  .Size([30, 36]), torc  .Size([15, 18]), torc  .Size([8, 9]), torc  .Size([4, 5])]
return anchors
=================   Anc  orGenerator.grid_anchors(grid_sizes) END
anchors = []
for i, (image_  eig  t, image_widt  ) in enumerate(image_list.image_sizes):

    anchors_in_image = []

    for anchors_per_feature_map in anchors_over_all_feature_maps:

        boxlist = BoxList( anchors_per_feature_map, (image_widt  , image_  eig  t), mode="xyxy" )
        boxlist:
            BoxList(num_boxes=38880, image_widt  =561, image_  eig  t=480, mode=xyxy)

        self.add_visibility_to(boxlist)

=================   Anc  orGenerator.add_visibitity_to(boxlist) BEGIN
=================   Anc  orGenerator.add_visibitity_to(boxlist) END
        boxlist:
            BoxList(num_boxes=38880, image_widt  =561, image_  eig  t=480, mode=xyxy)

        anchors_in_image.append(boxlist)

        boxlist = BoxList( anchors_per_feature_map, (image_widt  , image_  eig  t), mode="xyxy" )
        boxlist:
            BoxList(num_boxes=9720, image_widt  =561, image_  eig  t=480, mode=xyxy)

        self.add_visibility_to(boxlist)

=================   Anc  orGenerator.add_visibitity_to(boxlist) BEGIN
=================   Anc  orGenerator.add_visibitity_to(boxlist) END
        boxlist:
            BoxList(num_boxes=9720, image_widt  =561, image_  eig  t=480, mode=xyxy)

        anchors_in_image.append(boxlist)

        boxlist = BoxList( anchors_per_feature_map, (image_widt  , image_  eig  t), mode="xyxy" )
        boxlist:
            BoxList(num_boxes=2430, image_widt  =561, image_  eig  t=480, mode=xyxy)

        self.add_visibility_to(boxlist)

=================   Anc  orGenerator.add_visibitity_to(boxlist) BEGIN
=================   Anc  orGenerator.add_visibitity_to(boxlist) END
        boxlist:
            BoxList(num_boxes=2430, image_widt  =561, image_  eig  t=480, mode=xyxy)

        anchors_in_image.append(boxlist)

        boxlist = BoxList( anchors_per_feature_map, (image_widt  , image_  eig  t), mode="xyxy" )
        boxlist:
            BoxList(num_boxes=648, image_widt  =561, image_  eig  t=480, mode=xyxy)

        self.add_visibility_to(boxlist)

=================   Anc  orGenerator.add_visibitity_to(boxlist) BEGIN
=================   Anc  orGenerator.add_visibitity_to(boxlist) END
        boxlist:
            BoxList(num_boxes=648, image_widt  =561, image_  eig  t=480, mode=xyxy)

        anchors_in_image.append(boxlist)

        boxlist = BoxList( anchors_per_feature_map, (image_widt  , image_  eig  t), mode="xyxy" )
        boxlist:
            BoxList(num_boxes=180, image_widt  =561, image_  eig  t=480, mode=xyxy)

        self.add_visibility_to(boxlist)

=================   Anc  orGenerator.add_visibitity_to(boxlist) BEGIN
=================   Anc  orGenerator.add_visibitity_to(boxlist) END
        boxlist:
            BoxList(num_boxes=180, image_widt  =561, image_  eig  t=480, mode=xyxy)

        anchors_in_image.append(boxlist)

        anchors_in_image:
            [BoxList(num_boxes=38880, image_widt  =561, image_  eig  t=480, mode=xyxy), BoxList(num_boxes=9720, image_widt  =561, image_  eig  t=480, mode=xyxy), BoxList(num_boxes=2430, image_widt  =561, image_  eig  t=480, mode=xyxy), BoxList(num_boxes=648, image_widt  =561, image_  eig  t=480, mode=xyxy), BoxList(num_boxes=180, image_widt  =561, image_  eig  t=480, mode=xyxy)]

        anchors.append(anchors_in_image)

        anchors:
            [[BoxList(num_boxes=38880, image_widt  =561, image_  eig  t=480, mode=xyxy), BoxList(num_boxes=9720, image_widt  =561, image_  eig  t=480, mode=xyxy), BoxList(num_boxes=2430, image_widt  =561, image_  eig  t=480, mode=xyxy), BoxList(num_boxes=648, image_widt  =561, image_  eig  t=480, mode=xyxy), BoxList(num_boxes=180, image_widt  =561, image_  eig  t=480, mode=xyxy)]]

return anchors
=================   Anc  orGenerator.forward(image_list, feature_maps) END
if self.training == False
    return self._forward_test(anchors, box_cls, box_regression)


=========================================== RetinaNetModule.forward(self, images, features, targets=None): END


=========================================== RetinaNetModule._forward_test(self, anchors, box_cls, box_regression): BEGIN
params:
    len(anchors)
: 1
    len(box_cls)
: 5
    len(box_regression): 5
self.box_selector_test: RetinaNetPostProcessor()
boxes = self.box_selector_test(anchors, box_cls, box_regression)
========== RPNPostProcessing.forward() BEGIN
========== RPNPostProcessing.forward() END
len(boxes): 1
return boxes, {} # {} is just empty dictionayr


=========================================== RetinaNetModule._forward_test(self, anchors, box_cls, box_regression): END
proposals, proposal_losses = self.rpn(images, features, targets) DONE
x = features
result = proposals
return result
GeneralizedRCNN.forward(self, images, targets=None) ====================== END
