compute_prediction(self, image)
        # PIL input image shape (BGR)
        #  [438, 512] : H, W
        image: H, W=(438,512)
        image_tensor = self.transforms(image)
                transforms.py Compose class __call__  ====== BEGIN
                for t in self.transforms:
                        image = <maskrcnn_benchmark.data.transforms.transforms.Resize object at 0x7f2652315080>(image)
                        image = <maskrcnn_benchmark.data.transforms.transforms.ToTensor object at 0x7f2652315128>(image)
                        image = <maskrcnn_benchmark.data.transforms.transforms.Normalize object at 0x7f26523150b8>(image)
                return image
                transforms.py Compose class __call__  ====== END

        # image shape as a tensor  after Resize, ToTensor, Normalize
        # [3, 480, 561] : C,H,W
        image_tensor.shape: torch.Size([3, 480, 561])

        #padding images for 32 divisible size on width and height
        image_list = to_image_list(image_tensor, 32).to(self.device)

        to_image_list(tensors, size_divisible=32) ====== BEGIN
                type(batched_imgs): <class 'torch.Tensor'>
                batched_imgs.shape: torch.Size([1, 3, 480, 576])
                image_sizes: [torch.Size([480, 561])]
                return ImageList(batched_imgs, image_sizes)
        to_image_list(tensors, size_divisible=32) ====== END

        # image_list.image_size keeps image size after resize
        # [480, 561] : H, W
        image_list.image_sizes: [torch.Size([480, 561])]
        # image_list.tensors used as input to model
        # - image size after zero-padding for 32 divisibility
        # [1, 3, 480, 576] : B, C, H, W
        image_list.tensors.shape: torch.Size([1, 3, 480, 576]) 

        # self.model is instance of GeneralizedRCNN
        # GeneralizeRCNN = backbone (resnet + fpn) + rpn
        pred = self.model(image_list)



GeneralizedRCNN.forward(self, images, targets=None) ====================== BEGIN
        Params:
            type(images): <class 'maskrcnn_benchmark.structures.image_list.ImageList'>
            targets: None

    if self.training == False:
    images = to_image_list(images)

    to_image_list(tensors, size_divisible=0) ====== BEGIN
        if isinstance(tensors, ImageList):
        return tensors
    to_image_list(tensors, size_divisible=0) ====== END

    images.image_sizes: [torch.Size([480, 561])]
    images.tensors.shape: torch.Size([1, 3, 480, 576])


        # self.model.backbone is a list of instance of ResNet and FPN
        # note that input to backbone (actually, input to resnet50) is images.tensors,
        # which is tensor after zero padding for 32 divisibility
    model.backbone.forward(images.tensors) ==========================  BEGIN

            -------------------------------------------------------------------------------------------

            Resnet.forward(self, x) ==========================  BEGIN
                Params:
                    x.shape=torch.Size([1, 3, 480, 576])  # tensor after zero paded for 32 divisibility

                x = self.stem(x)
                x.shape: torch.Size([1, 64, 120, 144])

                for stage_name in self.stages:
                        stage_name: layer1
                        output shape of layer1: torch.Size([1, 256, 120, 144])
                                outputs.append(x) stage_name: layer1
                                x.shape: torch.Size([1, 256, 120, 144])

                        stage_name: layer2
                        output shape of layer2: torch.Size([1, 512, 60, 72])
                                outputs.append(x) stage_name: layer2
                                x.shape: torch.Size([1, 512, 60, 72])

                        stage_name: layer3
                        output shape of layer3: torch.Size([1, 1024, 30, 36])
                                outputs.append(x) stage_name: layer3
                                x.shape: torch.Size([1, 1024, 30, 36])

                        stage_name: layer4
                        output shape of layer4: torch.Size([1, 2048, 15, 18])
                                outputs.append(x) stage_name: layer4
                                x.shape: torch.Size([1, 2048, 15, 18])


                ResNet::forward return value
                        outputs[0]: torch.Size([1, 256, 120, 144])  # C1 : output of layer 1 of resnet50
                        outputs[1]: torch.Size([1, 512, 60, 72])    # C2 : output of layer 2 of resnet50
                        outputs[2]: torch.Size([1, 1024, 30, 36])   # C3 : output of layer 3 of resnet50
                        outputs[3]: torch.Size([1, 2048, 15, 18])   # C4 : output of layer 4 of resnet50

                return outputs
            Resnet.forward(self, x) ==========================  END

            -------------------------------------------------------------------------------------------

            FPN.forward(self,x) ====== BEGIN
                    Pram:
                        x  = [C1, C2, C3, C4]
                        len(x) = 4
                        C[1].shape : torch.Size([1, 256, 120, 144]) # C1: output of layer 1 of resnet50
                        C[2].shape : torch.Size([1, 512, 60, 72])   # C2: output of layer 2 of resnet50
                        C[3].shape : torch.Size([1, 1024, 30, 36])  # C3: output of layer 3 of resnet50
                        C[4].shape : torch.Size([1, 2048, 15, 18])  # C4: output of layer 4 of resnet50

                    x[-1].shape = torch.Size([1, 2048, 15, 18])     # C4

                    last_inner = fpn_inner4(C4)
                            self.innerblocks[-1] = Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
                            last_inner.shape = torch.Size([1, 1024, 15, 18])


                    results.append(fpn_layer4(last_inner))
                            self.layer_blocks[-1]: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                            results[0].shape: torch.Size([1, 1024, 15, 18])  # P4

                    for feature, inner_block, layer_block
                                    in zip[(x[:-1][::-1], self.inner_blocks[:-1][::-1], self.layer_blocks[:-1][::-1]):

                            ====================================
                            iteration 0 summary
                            ====================================
                            feature.shape: torch.Size([1, 1024, 30, 36])  # C3
                            inner_block: fpn_inner3 ==> Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                            layer_block: fpn_layer3 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                            last_inner.shape: torch.Size([1, 1024, 15, 18])
                            ====================================

                            --------------------------------------------------
                            0.1 Upsample : replace with Decovolution in caffe
                            # layer name in caffe: fpn_inner3_upsample = Deconvolution(last_inner)
                            --------------------------------------------------
                            inner_top_down = interpolate(last_inner, scale_factor=2, mode='nearest')

                            last_inner.shape: torch.Size([1, 1024, 15, 18])
                            inner_top_down.shape : torch.Size([1, 1024, 30, 36])
                            --------------------------------------------------

                            --------------------------------------------------
                            0.2 inner_lateral = Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
                            # layer name in caffe: fpn_inner3_lateral=Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
                            --------------------------------------------------
                                    inner_block: fpn_inner3 ==> Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
                                    input: feature.shape: torch.Size([1, 1024, 30, 36])
                                    output: inner_lateral.shape: torch.Size([1, 1024, 30, 36])

                            --------------------------------------------------

                            --------------------------------------------------
                            0.3 Elementwise Addition: replaced with eltwise in caffe
                            # layer in caffe: eltwise_3 = eltwise(fpn_inner3_lateral, fpn_inner3_upsample )
                            --------------------------------------------------
                            last_inner = inner_lateral + inner_top_down
                                    inner_lateral.shape: torch.Size([1, 1024, 30, 36])
                                    inner_top_down.shape: torch.Size([1, 1024, 30, 36])
                                    last_inner.shape : torch.Size([1, 1024, 30, 36])
                            --------------------------------------------------

                            --------------------------------------------------
                            0.4 results.insert(0, Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(last_inner)
                            #layer in caffe: fpn_layer3 = Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(eltwise_3)
                            # note that results.insert(0, ...) used for prepend instead of append
                            --------------------------------------------------
                                    layer_block: fpn_layer3 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                                    input: last_inner.shape = torch.Size([1, 1024, 30, 36])  # P3
                            --------------------------------------------------

                            --------------------------------------------------
                            results after iteration 0
                            --------------------------------------------------
                                    results[0].shape: torch.Size([1, 1024, 30, 36])  # P3
                                    results[1].shape: torch.Size([1, 1024, 15, 18])  # P4
                            --------------------------------------------------


                            ====================================
                            iteration 1 summary
                            ====================================
                            feature.shape: torch.Size([1, 512, 60, 72])  # C2
                            inner_block: fpn_inner2 ==> Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
                            layer_block: fpn_layer2 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                            last_inner.shape: torch.Size([1, 1024, 30, 36])
                            ====================================

                            --------------------------------------------------
                            1.1 Upsample : replace with Decovolution in caffe
                            # layer name in caffe: fpn_inner2_upsample = Deconvolution(last_inner)
                            --------------------------------------------------
                            inner_top_down = interpolate(last_inner, scale_factor=2, mode='nearest')

                            last_inner.shape: torch.Size([1, 1024, 30, 36])
                            inner_top_down.shape : torch.Size([1, 1024, 60, 72])
                            --------------------------------------------------

                            --------------------------------------------------
                            1.2 inner_lateral = Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
                            # layer name in caffe: fpn_inner2_lateral=Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
                            --------------------------------------------------
                                    inner_block: fpn_inner2 ==> Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
                                    input: feature.shape: torch.Size([1, 512, 60, 72])
                                    output: inner_lateral.shape: torch.Size([1, 1024, 60, 72])

                            --------------------------------------------------

                            --------------------------------------------------
                            1.3 Elementwise Addition: replaced with eltwise in caffe
                            # layer in caffe: eltwise_2 = eltwise(fpn_inner2_lateral, fpn_inner2_upsample )
                            --------------------------------------------------
                            last_inner = inner_lateral + inner_top_down
                                    inner_lateral.shape: torch.Size([1, 1024, 60, 72])
                                    inner_top_down.shape: torch.Size([1, 1024, 60, 72])
                                    last_inner.shape : torch.Size([1, 1024, 60, 72])
                            --------------------------------------------------

                            --------------------------------------------------
                            1.4 results.insert(0, Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(last_inner)
                            #layer in caffe: fpn_layer2 = Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(eltwise_2)
                            # note that results.insert(0, ...) used for prepend instead of append
                            --------------------------------------------------
                                    layer_block: fpn_layer2 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                                    input: last_inner.shape = torch.Size([1, 1024, 60, 72])  # P2
                            --------------------------------------------------

                            --------------------------------------------------
                            results after iteration 1
                            --------------------------------------------------
                                    results[0].shape: torch.Size([1, 1024, 60, 72])  # P2
                                    results[1].shape: torch.Size([1, 1024, 30, 36])  # P3
                                    results[2].shape: torch.Size([1, 1024, 15, 18])  # P4
                            --------------------------------------------------

                    #for loop END


                    if isinstance(self.top_blocks, LastLevelP6P7):

                            # self.top.blocks is instance of LastLevelP6P7 module
                            # x[-1]: C4 output of layer 4 of resnet50
                            # results[-1]: P4

                            last_result = self.top_blocks(x[-1], results[-1])

                            LastLevelP6P7.forward(self, c5, p5) ============= BEGIN
                                    Params:
                                        c5.shape: torch.Size([1, 2048, 15, 18])   # c5 corresponds to C4
                                        p5.shape: torch.Size([1, 1024, 15, 18])   # p5 corresponds to P4

                                    if (self.use_P5 == False)
                                            x=c5     # use C4 as input to next conv

                                    x.shape = torch.Size([1, 2048, 15, 18])
                                    p6 = self.p6(x)
                                            self.p6: Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                                            p6.shape: torch.Size([1, 1024, 8, 9])  # P6

                                    p7 = self.p7(F.relu(p6))
                                            self.p7: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
                                            p7.shape: torch.Size([1, 1024, 4, 5])  # P7

                                    returns [p6, p7]
                            LastLevelP6P7.forward(self, c5, p5) ============= END


                            results.extend(last_results)

                            results
                            result[0].shape: torch.Size([1, 1024, 60, 72])   # P2
                            result[1].shape: torch.Size([1, 1024, 30, 36])   # P3
                            result[2].shape: torch.Size([1, 1024, 15, 18])   # P4
                            result[3].shape: torch.Size([1, 1024, 8, 9])     # P6
                            result[4].shape: torch.Size([1, 1024, 4, 5])     # P7

                    return tuple(results)


            FPN.forward(self,x) ====== END

            -------------------------------------------------------------------------------------------


    model.backbone.forward(images.tensors) ==========================  END

        ------------------------------------------------------------------------------------------------

    # self.rpn is instance of RetinaNetModule
    # defined in maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py
    proposals, proposal_losses = self.rpn(images, features, targets)


        RetinaNetModule.forward(self, images, features, targets=None): =============== BEGIN
        # defined in maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py
            Params:
                    type(images.image_size): <class 'list'>                    # ImageList
                    type(images.tensors): <class 'torch.Tensor'>               # ImageList
                    len(features)): 5
                            feature[0].shape: torch.Size([1, 1024, 60, 72])    # P2 from backbone.fpn
                            feature[1].shape: torch.Size([1, 1024, 30, 36])    # P3 from backbone.fpn
                            feature[2].shape: torch.Size([1, 1024, 15, 18])    # P4 from backbone.fpn
                            feature[3].shape: torch.Size([1, 1024, 8, 9])      # P6 from backbone.fpn
                            feature[4].shape: torch.Size([1, 1024, 4, 5])      # P7 from backbone.fpn

            self.head: RetinaNetHead(
              (cls_tower): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU()
                (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ReLU()
                (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (5): ReLU()
                (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (7): ReLU()
              )
              (bbox_tower): Sequential(
                (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (1): ReLU()
                (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (3): ReLU()
                (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (5): ReLU()
                (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
                (7): ReLU()
              )
              (cls_logits): Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (bbox_pred): Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )

            #-----------------------
            # 1. rpn head
            #-----------------------
            # self.head is instance of RetinaNetHead class
            # defined in maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py

            box_cls, box_regression = self.head(features)


            RetinaNetHead.forward(self, x): ========================================== BEGIN
                Param:
                        len(x)): 5 x is features returned from FPN
                                x[0].shape: torch.Size([1, 1024, 60, 72])  # P2 from backbone.fpn
                                x[1].shape: torch.Size([1, 1024, 30, 36])  # P3 from backbone.fpn
                                x[2].shape: torch.Size([1, 1024, 15, 18])  # P4 from backbone.fpn
                                x[3].shape: torch.Size([1, 1024, 8, 9])    # P6 from backbone.fpn
                                x[4].shape: torch.Size([1, 1024, 4, 5])    # P7 from backbone.fpn
                logits = []
                bbox_reg = []


                for feature in x:
                        ===== iteration: 0 ====
                        # P2
                        feature[0].shape: torch.Size([1, 1024, 60, 72])

                        # append cls_logits( cls_tower (P2)) into logits[]
                        logits.append(self.cls_logits(self.cls_tower(feature)))

                        bbox_tower => bbox_pre => bbox_reg[]
                        # append box_pred( bbox_tower (P2)) into bbox_reg[]
                        bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))


                        ===== iteration: 1 ====
                        # P3
                        feature[1].shape: torch.Size([1, 1024, 30, 36])

                        # append cls_logits( cls_tower (P3)) into logits[]
                        logits.append(self.cls_logits(self.cls_tower(feature)))

                        # append box_pred( bbox_tower (P3)) into bbox_reg[]
                        bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))


                        ===== iteration: 2 ====
                        # P4
                        feature[2].shape: torch.Size([1, 1024, 15, 18])

                        # append cls_logits( cls_tower (P4)) into logits[]
                        logits.append(self.cls_logits(self.cls_tower(feature)))

                        # append box_pred( bbox_tower (P4)) into bbox_reg[]
                        bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))


                        ===== iteration: 3 ====
                        # P6
                        feature[3].shape: torch.Size([1, 1024, 8, 9])     # P6

                        # append cls_logits( cls_tower (P6)) into logits[]
                        logits.append(self.cls_logits(self.cls_tower(feature)))

                        # append box_pred( bbox_tower (P6)) into bbox_reg[]
                        bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))


                        ===== iteration: 4 ====
                        # P7
                        feature[4].shape: torch.Size([1, 1024, 4, 5])

                        # append cls_logits( cls_tower (P7)) into logits[]
                        logits.append(self.cls_logits(self.cls_tower(feature)))

                        # append box_pred( bbox_tower (P7)) into bbox_reg[]
                        bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

                 ==== logits ====
                logits[0].shape: torch.Size([1, 9, 60, 72])    # cls_logits( cls_tower (P2))
                logits[1].shape: torch.Size([1, 9, 30, 36])    # cls_logits( cls_tower (P3))
                logits[2].shape: torch.Size([1, 9, 15, 18])    # cls_logits( cls_tower (P4))
                logits[3].shape: torch.Size([1, 9, 8, 9])      # cls_logits( cls_tower (P6))
                logits[4].shape: torch.Size([1, 9, 4, 5])      # cls_logits( cls_tower (P7))

                 ==== bbox_reg ====
                bbox_reg[0].shape: torch.Size([1, 36, 60, 72]) # box_pred( bbox_tower (P2))
                bbox_reg[1].shape: torch.Size([1, 36, 30, 36]) # box_pred( bbox_tower (P3))
                bbox_reg[2].shape: torch.Size([1, 36, 15, 18]) # box_pred( bbox_tower (P4))
                bbox_reg[3].shape: torch.Size([1, 36, 8, 9])   # box_pred( bbox_tower (P6))
                bbox_reg[4].shape: torch.Size([1, 36, 4, 5])   # box_pred( bbox_tower (P7))

                return logits, bbox_reg

            RetinaNetHead.forward(self, x): ========================================== END


            #-----------------------
            # 2. anchor_generator
            #-----------------------
            # self.anchor_generator is instance of AnchorGenerator class
            # defined in maskrcnn_benchmark/modeling/rpn/anchor_generator.py
            anchors = self.anchor_generator(images, features)

            self.anchor_generator: AnchorGenerator(
                 (cell_anchors): BufferList()
            )

            AnchorGenerator.forward(image_list, feature_maps) ======================= BEGIN
                Params:
                        image_list:
                                len(image_list.image_sizes): 1
                                image_list.image_sizes[0]: torch.Size([480, 561])
                                len(image_list.tensors): 1
                                image_list.tensors[0].shape: torch.Size([3, 480, 576])
                        feature_maps:
                                feature_maps[0].shape: torch.Size([1, 1024, 60, 72])
                                feature_maps[1].shape: torch.Size([1, 1024, 30, 36])
                                feature_maps[2].shape: torch.Size([1, 1024, 15, 18])
                                feature_maps[3].shape: torch.Size([1, 1024, 8, 9])
                                feature_maps[4].shape: torch.Size([1, 1024, 4, 5])

                grid_sizes = [feature_map.shape[-2:] for feature_map in feature_maps]

                # self.grid_anchors is a function defined in AnchroGenerator class
                anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)

                --------------------------------------------------------------------------

                AnchorGenerator.grid_anchors(grid_sizes) ============================ BEGIN
                    Param:
                        grid_sizes: [torch.Size([60, 72]), torch.Size([30, 36]), torch.Size([15, 18]), torch.Size([8, 9]), torch.Size([4, 5])]
                    return anchors
                AnchorGenerator.grid_anchors(grid_sizes) ============================ END

                --------------------------------------------------------------------------

                anchors = []
                for i, (image_height, image_width) in enumerate(image_list.image_sizes):

                        anchors_in_image = []

                        for anchors_per_feature_map in anchors_over_all_feature_maps:

                                boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
                                boxlist:
                                        BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy)

                                self.add_visibility_to(boxlist)

                                AnchorGenerator.add_visibitity_to(boxlist) ====== BEGIN
                                AnchorGenerator.add_visibitity_to(boxlist) ====== END

                                boxlist:
                                        BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy)

                                anchors_in_image.append(boxlist)

                                # ------------------------------------------

                                boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
                                boxlist:
                                        BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy)

                                self.add_visibility_to(boxlist)

                                AnchorGenerator.add_visibitity_to(boxlist) ====== BEGIN
                                AnchorGenerator.add_visibitity_to(boxlist) ====== END

                                boxlist:
                                        BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy)

                                anchors_in_image.append(boxlist)

                                # ------------------------------------------

                                boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
                                boxlist:
                                        BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy)

                                self.add_visibility_to(boxlist)

                                AnchorGenerator.add_visibitity_to(boxlist) ====== BEGIN
                                AnchorGenerator.add_visibitity_to(boxlist) ====== END

                                boxlist:
                                        BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy)

                                anchors_in_image.append(boxlist)

                                # ------------------------------------------

                                boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
                                boxlist:
                                        BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy)

                                self.add_visibility_to(boxlist)

                                AnchorGenerator.add_visibitity_to(boxlist) ====== BEGIN
                                AnchorGenerator.add_visibitity_to(boxlist) ====== END

                                boxlist:
                                        BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy)

                                anchors_in_image.append(boxlist)

                                # ------------------------------------------

                                boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
                                boxlist:
                                        BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)

                                self.add_visibility_to(boxlist)

                                AnchorGenerator.add_visibitity_to(boxlist) ====== BEGIN
                                AnchorGenerator.add_visibitity_to(boxlist) ====== END

                                boxlist:
                                        BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)

                                anchors_in_image.append(boxlist)


                                # end of for loop

                        anchors_in_image: # list of element BoxList
                                    [BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy),
                                     BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy),
                                     BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy),
                                     BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy),
                                     BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)]

                        anchors.append(anchors_in_image)

                                anchors: # list of list of element BoxList
                                        [[BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy),
                                          BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy),
                                          BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy),
                                          BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy),
                                          BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)]]

                return anchors

            AnchorGenerator.forward(image_list, feature_maps) ======================= END

            #-----------------------
            # 3. _forward_test
            #-----------------------
            if self.training == False
                return self._forward_test(anchors, box_cls, box_regression)


                RetinaNetModule._forward_test(self, anchors, box_cls, box_regression): ====== BEGIN
                   Params:
                       len(anchors): 1
                       len(box_cls): 5
                       len(box_regression): 5


                   #-----------------------
                   # 4. box_selector_test
                   #-----------------------
                   # self.box_selector_test: RetinaNetPostProcessor()
                   boxes = self.box_selector_test(anchors, box_cls, box_regression)

                   RPNPostProcessing.forward() ==== BEGIN
                   RPNPostProcessing.forward() ==== END

                   len(boxes): 1
                   return boxes, {} # {} is just empty dictionayr

                RetinaNetModule._forward_test(self, anchors, box_cls, box_regression): ====== END

        RetinaNetModule.forward(self, images, features, targets=None): =============== END

        ----------------------------------------------------------------------------------

    # `proposals` correspond to return value `boxes` in RetinaNetModule._forward_test()
    # `proposal_loss` correspond to return value `{}` - empty dict - in RetinaNetModule._forward_test()
    proposals, proposal_losses = self.rpn(images, features, targets) == DONE

    x = features      # what for this code? is it undeleted by mistake from Lomin?
    result = proposals
    return result

GeneralizedRCNN.forward(self, images, targets=None) ====================== END
