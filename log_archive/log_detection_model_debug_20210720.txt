compute_prediction(self, image)
        image: H, W=(438,512)    <== PIL input image shape (BGR)
        image_tensor = self.transforms(image)
                transforms.py Compose class __call__  ====== BEGIN
                for t in self.transforms:
                        image = <maskrcnn_benchmark.data.transforms.transforms.Resize object at 0x7f2652315080>(image)
                        image = <maskrcnn_benchmark.data.transforms.transforms.ToTensor object at 0x7f2652315128>(image)
                        image = <maskrcnn_benchmark.data.transforms.transforms.Normalize object at 0x7f26523150b8>(image)
                return image
                transforms.py Compose class __call__  ====== END

        image_tensor.shape: torch.Size([3, 480, 561])  <==== image shape after Resize, ToTensor, Normalize

        padding images for 32 divisible size on width and height
        image_list = to_image_list(image_tensor, 32).to(self.device)

        to_image_list(tensors, size_divisible=32) ====== BEGIN
                type(batched_imgs): <class 'torch.Tensor'>
                batched_imgs.shape: torch.Size([1, 3, 480, 576])
                image_sizes: [torch.Size([480, 561])]
                return ImageList(batched_imgs, image_sizes)
        to_image_list(tensors, size_divisible=32) ====== END

        image_list.image_sizes: [torch.Size([480, 561])]         <===== image size after resize
        image_list.tensors.shape: torch.Size([1, 3, 480, 576])   <===== image size after zero-padding for 32 divisibility
        pred = self.model(image_list)  # model is instance of GeneralizedRCNN

compute_prediction(self, image)

	image: H, W=(438,512)

	image_tensor = self.transforms(image)

		transforms.py Compose class __call__  ====== BEGIN

		for t in self.transforms:
			image = <maskrcnn_benchmark.data.transforms.transforms.Resize object at 0x7f2652315080>(image)
			image = <maskrcnn_benchmark.data.transforms.transforms.ToTensor object at 0x7f2652315128>(image)
			image = <maskrcnn_benchmark.data.transforms.transforms.Normalize object at 0x7f26523150b8>(image)

		return image
		transforms.py Compose class __call__  ====== END

	image_tensor.shape: torch.Size([3, 480, 561])

	padding images for 32 divisible size on width and height
	image_list = to_image_list(image_tensor, 32).to(self.device)

	to_image_list(tensors, size_divisible=32) ====== BEGIN
		type(batched_imgs): <class 'torch.Tensor'>
		batched_imgs.shape: torch.Size([1, 3, 480, 576])
		image_sizes: [torch.Size([480, 561])]
		return ImageList(batched_imgs, image_sizes)
	to_image_list(tensors, size_divisible=32) ====== END

	image_list.image_sizes: [torch.Size([480, 561])]
	image_list.tensors.shape: torch.Size([1, 3, 480, 576])
	pred = self.model(image_list)


GeneralizedRCNN.forward(self, images, targets=None) ====================== BEGIN
type(images): <class 'maskrcnn_benchmark.structures.image_list.ImageList'>
targets: None
	if self.training == False: 
	images = to_image_list(images)

	to_image_list(tensors, size_divisible=0) ====== BEGIN
		if isinstance(tensors, ImageList):
		return tensors
	to_image_list(tensors, size_divisible=0) ====== END

	images.image_sizes: [torch.Size([480, 561])]
	images.tensors.shape: torch.Size([1, 3, 480, 576])
	model.backbone.forward(images.tensors) BEFORE

=========================================== Resnet.forward(self, x) BEGIN
	param x.shape=torch.Size([1, 3, 480, 576]) 
	x = self.stem(x)
	x.shape: torch.Size([1, 64, 120, 144])
	for stage_name in self.stages:
		stage_name: layer1
		toutput shape of layer1: torch.Size([1, 256, 120, 144])
			outputs.append(x) stage_name: layer1
			x.shape: torch.Size([1, 256, 120, 144])
		stage_name: layer2
		toutput shape of layer2: torch.Size([1, 512, 60, 72])
			outputs.append(x) stage_name: layer2
			x.shape: torch.Size([1, 512, 60, 72])
		stage_name: layer3
		toutput shape of layer3: torch.Size([1, 1024, 30, 36])
			outputs.append(x) stage_name: layer3
			x.shape: torch.Size([1, 1024, 30, 36])
		stage_name: layer4
		toutput shape of layer4: torch.Size([1, 2048, 15, 18])
			outputs.append(x) stage_name: layer4
			x.shape: torch.Size([1, 2048, 15, 18])

	ResNet::forward return value
		outputs[0]: torch.Size([1, 256, 120, 144])
		outputs[1]: torch.Size([1, 512, 60, 72])
		outputs[2]: torch.Size([1, 1024, 30, 36])
		outputs[3]: torch.Size([1, 2048, 15, 18])

	return outputs

=========================================== Resnet.forward() END


FPN.forward(self,x) ====== BEGIN
	======forward param: x  = [C1, C2, C3, C4] 
	len(x) = 4
	C[1].shape : torch.Size([1, 256, 120, 144])
	C[2].shape : torch.Size([1, 512, 60, 72])
	C[3].shape : torch.Size([1, 1024, 30, 36])
	C[4].shape : torch.Size([1, 2048, 15, 18])

	x[-1].shape = torch.Size([1, 2048, 15, 18])

	last_inner = fpn_inner4(C4)
		self.innerblocks[-1] = Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
		last_inner.shape = torch.Size([1, 1024, 15, 18])


	results.append(fpn_layer4(last_inner))
		self.layer_blocks[-1]: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
		results[0].shape: torch.Size([1, 1024, 15, 18])

	for feature, inner_block, layer_block
			in zip[(x[:-1][::-1], self.inner_blocks[:-1][::-1], self.layer_blocks[:-1][::-1]):

		====================================
		iteration 0 summary
		====================================
		feature.shape: torch.Size([1, 1024, 30, 36])
		inner_block: fpn_inner3 ==> Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
		layer_block: fpn_layer3 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
		last_inner.shape: torch.Size([1, 1024, 15, 18])
		====================================

		--------------------------------------------------
		0.1 Upsample : replace with Decovolution in caffe
		layer name in caffe: fpn_inner3_upsample = Deconvolution(last_inner)
		--------------------------------------------------
		inner_top_down = interpolate(last_inner, scale_factor=2, mode='nearest')

		last_inner.shape: torch.Size([1, 1024, 15, 18])
		inner_top_down.shape : torch.Size([1, 1024, 30, 36])
		--------------------------------------------------

		--------------------------------------------------
		0.2 inner_lateral = Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
		layer name in caffe: fpn_inner3_lateral=Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
		--------------------------------------------------
			inner_block: fpn_inner3 ==> Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
			input: feature.shape: torch.Size([1, 1024, 30, 36])
			output: inner_lateral.shape: torch.Size([1, 1024, 30, 36])

		--------------------------------------------------

		--------------------------------------------------
		0.3 Elementwise Addition: replaced with eltwise in caffe
		layer in caffe: eltwise_3 = eltwise(fpn_inner3_lateral, fpn_inner3_upsample )
		--------------------------------------------------
		last_inner = inner_lateral + inner_top_down
			inner_lateral.shape: torch.Size([1, 1024, 30, 36])
			inner_top_down.shape: torch.Size([1, 1024, 30, 36])
			last_inner.shape : torch.Size([1, 1024, 30, 36])
		--------------------------------------------------

		--------------------------------------------------
		0.4 results.insert(0, Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(last_inner)
		layer in caffe: fpn_layer3 = Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(eltwise_3)
		--------------------------------------------------
			layer_block: fpn_layer3 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
			input: last_inner.shape = torch.Size([1, 1024, 30, 36])
		--------------------------------------------------

		--------------------------------------------------
		results after iteration 0
		--------------------------------------------------
			results[0].shape: torch.Size([1, 1024, 30, 36])
			results[1].shape: torch.Size([1, 1024, 15, 18])
		--------------------------------------------------

	for loop END

		====================================
		iteration 1 summary
		====================================
		feature.shape: torch.Size([1, 512, 60, 72])
		inner_block: fpn_inner2 ==> Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
		layer_block: fpn_layer2 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
		last_inner.shape: torch.Size([1, 1024, 30, 36])
		====================================

		--------------------------------------------------
		1.1 Upsample : replace with Decovolution in caffe
		layer name in caffe: fpn_inner2_upsample = Deconvolution(last_inner)
		--------------------------------------------------
		inner_top_down = interpolate(last_inner, scale_factor=2, mode='nearest')

		last_inner.shape: torch.Size([1, 1024, 30, 36])
		inner_top_down.shape : torch.Size([1, 1024, 60, 72])
		--------------------------------------------------

		--------------------------------------------------
		1.2 inner_lateral = Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
		layer name in caffe: fpn_inner2_lateral=Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
		--------------------------------------------------
			inner_block: fpn_inner2 ==> Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
			input: feature.shape: torch.Size([1, 512, 60, 72])
			output: inner_lateral.shape: torch.Size([1, 1024, 60, 72])

		--------------------------------------------------

		--------------------------------------------------
		1.3 Elementwise Addition: replaced with eltwise in caffe
		layer in caffe: eltwise_2 = eltwise(fpn_inner2_lateral, fpn_inner2_upsample )
		--------------------------------------------------
		last_inner = inner_lateral + inner_top_down
			inner_lateral.shape: torch.Size([1, 1024, 60, 72])
			inner_top_down.shape: torch.Size([1, 1024, 60, 72])
			last_inner.shape : torch.Size([1, 1024, 60, 72])
		--------------------------------------------------

		--------------------------------------------------
		1.4 results.insert(0, Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(last_inner)
		layer in caffe: fpn_layer2 = Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(eltwise_2)
		--------------------------------------------------
			layer_block: fpn_layer2 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
			input: last_inner.shape = torch.Size([1, 1024, 60, 72])
		--------------------------------------------------

		--------------------------------------------------
		results after iteration 1
		--------------------------------------------------
			results[0].shape: torch.Size([1, 1024, 60, 72])
			results[1].shape: torch.Size([1, 1024, 30, 36])
			results[2].shape: torch.Size([1, 1024, 15, 18])
		--------------------------------------------------

	for loop END


	if isinstance(self.top_blocks, LastLevelP6P7):
		last_result = self.top_blocks(x[-1], results[-1])

		LastLevelP6P7.forward(self, c5, p5) ============= BEGIN 
			c5.shape: torch.Size([1, 2048, 15, 18])
			p5.shape: torch.Size([1, 1024, 15, 18])

			if (self.use_P5 == False)
				x=c5
			x.shape = torch.Size([1, 2048, 15, 18])
			p6 = self.p6(x)
				self.p6: Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
				p6.shape: torch.Size([1, 1024, 8, 9])

			p7 = self.p7(F.relu(p6))
				self.p7: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
				p7.shape: torch.Size([1, 1024, 4, 5])

			returns [p6, p7]
		LastLevelP6P7.forward(self, c5, p5) ============= END


		results.extend(last_results)

		results
		result[0].shape: torch.Size([1, 1024, 60, 72])
		result[1].shape: torch.Size([1, 1024, 30, 36])
		result[2].shape: torch.Size([1, 1024, 15, 18])
		result[3].shape: torch.Size([1, 1024, 8, 9])
		result[4].shape: torch.Size([1, 1024, 4, 5])

	return tuple(results)


FPN.forward(self,x) ====== END
	model.backbone.forward(images.tensors) DONE
proposals, proposal_losses = self.rpn(images, features, targets) BEFORE


=========================================== RetinaNetModule.forward(self, images, features, targets=None): BEGIN
	Params:
		type(images.image_size): <class 'list'>
		type(images.tensors): <class 'torch.Tensor'>
		len(features)): 5
			feature[0].shape: torch.Size([1, 1024, 60, 72])
			feature[1].shape: torch.Size([1, 1024, 30, 36])
			feature[2].shape: torch.Size([1, 1024, 15, 18])
			feature[3].shape: torch.Size([1, 1024, 8, 9])
			feature[4].shape: torch.Size([1, 1024, 4, 5])
self.head: RetinaNetHead(
  (cls_tower): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
  )
  (bbox_tower): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
  )
  (cls_logits): Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bbox_pred): Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
box_cls, box_regression = self.head(features)


=========================================== RetinaNetHead.forward(self, x): BEGIN
	Param:
		len(x)): 5 x is features returned from FPN
			x[0].shape: torch.Size([1, 1024, 60, 72])
			x[1].shape: torch.Size([1, 1024, 30, 36])
			x[2].shape: torch.Size([1, 1024, 15, 18])
			x[3].shape: torch.Size([1, 1024, 8, 9])
			x[4].shape: torch.Size([1, 1024, 4, 5])
logits = []
bbox_reg = []



for feature in x:
	===== iteration: 0 ====
	feature[0].shape: torch.Size([1, 1024, 60, 72])

	cls_tower => cls_logits => logits[]
	logits.append(self.cls_logits(self.cls_tower(feature)))

	bbox_tower => bbox_pre => bbox_reg[]
	bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

	===== iteration: 1 ====
	feature[1].shape: torch.Size([1, 1024, 30, 36])

	cls_tower => cls_logits => logits[]
	logits.append(self.cls_logits(self.cls_tower(feature)))

	bbox_tower => bbox_pre => bbox_reg[]
	bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

	===== iteration: 2 ====
	feature[2].shape: torch.Size([1, 1024, 15, 18])

	cls_tower => cls_logits => logits[]
	logits.append(self.cls_logits(self.cls_tower(feature)))

	bbox_tower => bbox_pre => bbox_reg[]
	bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

	===== iteration: 3 ====
	feature[3].shape: torch.Size([1, 1024, 8, 9])

	cls_tower => cls_logits => logits[]
	logits.append(self.cls_logits(self.cls_tower(feature)))

	bbox_tower => bbox_pre => bbox_reg[]
	bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

	===== iteration: 4 ====
	feature[4].shape: torch.Size([1, 1024, 4, 5])

	cls_tower => cls_logits => logits[]
	logits.append(self.cls_logits(self.cls_tower(feature)))

	bbox_tower => bbox_pre => bbox_reg[]
	bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))

 ==== logits ====
logits[0].shape: torch.Size([1, 9, 60, 72])
logits[1].shape: torch.Size([1, 9, 30, 36])
logits[2].shape: torch.Size([1, 9, 15, 18])
logits[3].shape: torch.Size([1, 9, 8, 9])
logits[4].shape: torch.Size([1, 9, 4, 5])

 ==== bbox_reg ====
bbox_reg[0].shape: torch.Size([1, 36, 60, 72])
bbox_reg[1].shape: torch.Size([1, 36, 30, 36])
bbox_reg[2].shape: torch.Size([1, 36, 15, 18])
bbox_reg[3].shape: torch.Size([1, 36, 8, 9])
bbox_reg[4].shape: torch.Size([1, 36, 4, 5])

return logits, bbox_reg


=========================================== RetinaNetHead.forward(self, x): END
self.anchor_generator: AnchorGenerator(
  (cell_anchors): BufferList()
)
anchors = self.anchor_generator(images, features)
=================   AnchorGenerator.forward(image_list, feature_maps) BEGIN
	Params:
		image_list:
			len(image_list.image_sizes): 1
			image_list.image_sizes[0]: torch.Size([480, 561])
			len(image_list.tensors): 1
			image_list.tensors[0].shape: torch.Size([3, 480, 576])
		feature_maps:
			feature_maps[0].shape: torch.Size([1, 1024, 60, 72])
			feature_maps[1].shape: torch.Size([1, 1024, 30, 36])
			feature_maps[2].shape: torch.Size([1, 1024, 15, 18])
			feature_maps[3].shape: torch.Size([1, 1024, 8, 9])
			feature_maps[4].shape: torch.Size([1, 1024, 4, 5])

grid_sizes = [feature_map.shape[-2:] for feature_map in feature_maps]
anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)
=================   AnchorGenerator.grid_anchors(grid_sizes) BEGIN
	Param:
		grid_sizes: [torch.Size([60, 72]), torch.Size([30, 36]), torch.Size([15, 18]), torch.Size([8, 9]), torch.Size([4, 5])]
return anchors
=================   AnchorGenerator.grid_anchors(grid_sizes) END
anchors = []
for i, (image_height, image_width) in enumerate(image_list.image_sizes):

	anchors_in_image = []

	for anchors_per_feature_map in anchors_over_all_feature_maps:

		boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
		boxlist:
			BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy)

		self.add_visibility_to(boxlist)

=================   AnchorGenerator.add_visibitity_to(boxlist) BEGIN
=================   AnchorGenerator.add_visibitity_to(boxlist) END
		boxlist:
			BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy)

		anchors_in_image.append(boxlist)

		boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
		boxlist:
			BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy)

		self.add_visibility_to(boxlist)

=================   AnchorGenerator.add_visibitity_to(boxlist) BEGIN
=================   AnchorGenerator.add_visibitity_to(boxlist) END
		boxlist:
			BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy)

		anchors_in_image.append(boxlist)

		boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
		boxlist:
			BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy)

		self.add_visibility_to(boxlist)

=================   AnchorGenerator.add_visibitity_to(boxlist) BEGIN
=================   AnchorGenerator.add_visibitity_to(boxlist) END
		boxlist:
			BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy)

		anchors_in_image.append(boxlist)

		boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
		boxlist:
			BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy)

		self.add_visibility_to(boxlist)

=================   AnchorGenerator.add_visibitity_to(boxlist) BEGIN
=================   AnchorGenerator.add_visibitity_to(boxlist) END
		boxlist:
			BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy)

		anchors_in_image.append(boxlist)

		boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" )
		boxlist:
			BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)

		self.add_visibility_to(boxlist)

=================   AnchorGenerator.add_visibitity_to(boxlist) BEGIN
=================   AnchorGenerator.add_visibitity_to(boxlist) END
		boxlist:
			BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)

		anchors_in_image.append(boxlist)

		anchors_in_image:
			[BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)]

		anchors.append(anchors_in_image)

		anchors:
			[[BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)]]

return anchors
=================   AnchorGenerator.forward(image_list, feature_maps) END
if self.training == False
	return self._forward_test(anchors, box_cls, box_regression)


=========================================== RetinaNetModule.forward(self, images, features, targets=None): END


=========================================== RetinaNetModule._forward_test(self, anchors, box_cls, box_regression): BEGIN
params:
	len(anchors)
: 1
	len(box_cls)
: 5
	len(box_regression): 5
self.box_selector_test: RetinaNetPostProcessor()
boxes = self.box_selector_test(anchors, box_cls, box_regression)
========== RPNPostProcessing.forward() BEGIN
========== RPNPostProcessing.forward() END
len(boxes): 1
return boxes, {} # {} is just empty dictionayr


=========================================== RetinaNetModule._forward_test(self, anchors, box_cls, box_regression): END
proposals, proposal_losses = self.rpn(images, features, targets) DONE
x = features
result = proposals
return result
GeneralizedRCNN.forward(self, images, targets=None) ====================== END
