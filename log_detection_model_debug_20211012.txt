# ======================
# Registered Modules
# ======================
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py
  // _TRANSFORMATION_MODULES: {'BottleneckWithFixedBatchNorm': <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>}

  // _STEM_MODULES : {'BottleneckWithFixedBatchNorm': <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>}


# Networks Stage Specs:
  'R-50-C4':
  (
    StageSpec(index=1, block_count=3, return_features=False)
    StageSpec(index=2, block_count=4, return_features=False)
    StageSpec(index=3, block_count=6, return_features=True)
  )

  'R-50-C5':
  (
    StageSpec(index=1, block_count=3, return_features=False)
    StageSpec(index=2, block_count=4, return_features=False)
    StageSpec(index=3, block_count=6, return_features=False)
    StageSpec(index=4, block_count=3, return_features=True)
  )

  'R-101-C4':
  (
    StageSpec(index=1, block_count=3, return_features=False)
    StageSpec(index=2, block_count=4, return_features=False)
    StageSpec(index=3, block_count=23, return_features=True)
  )

  'R-101-C5':
  (
    StageSpec(index=1, block_count=3, return_features=False)
    StageSpec(index=2, block_count=4, return_features=False)
    StageSpec(index=3, block_count=23, return_features=False)
    StageSpec(index=4, block_count=3, return_features=True)
  )

  'R-50-FPN':
  (
    StageSpec(index=1, block_count=3, return_features=True)
    StageSpec(index=2, block_count=4, return_features=True)
    StageSpec(index=3, block_count=6, return_features=True)
    StageSpec(index=4, block_count=3, return_features=True)
  )

  'R-50-FPN-RETINANET':
  (
    StageSpec(index=1, block_count=3, return_features=True)
    StageSpec(index=2, block_count=4, return_features=True)
    StageSpec(index=3, block_count=6, return_features=True)
    StageSpec(index=4, block_count=3, return_features=True)
  )

  'R-101-FPN':
  (
    StageSpec(index=1, block_count=3, return_features=True)
    StageSpec(index=2, block_count=4, return_features=True)
    StageSpec(index=3, block_count=23, return_features=True)
    StageSpec(index=4, block_count=3, return_features=True)
  )

  'R-101-FPN-RETINANET':
  (
    StageSpec(index=1, block_count=3, return_features=True)
    StageSpec(index=2, block_count=4, return_features=True)
    StageSpec(index=3, block_count=23, return_features=True)
    StageSpec(index=4, block_count=3, return_features=True)
  )

  'R-152-FPN':
  (
    StageSpec(index=1, block_count=3, return_features=True)
    StageSpec(index=2, block_count=8, return_features=True)
    StageSpec(index=3, block_count=36, return_features=True)
    StageSpec(index=4, block_count=3, return_features=True)
  )




# =======================================
# I. Detection Model Build and Init
# =======================================
DetectionDemo.__init__(self, cfg, weight, is_recognition=False) { // BEGIN
  // defined in detection_model_debug.py

  // Params:
    // cfg.MODEL.DEVICE: cuda
    // weight: ./model/detection/model_det_v2_200924_002_180k.pth
    // is_recognition: False

  self.model = build_detection_model(self.cfg) // CALL
  {

GeneralizedRCNN.__init__(self, cfg) { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py

    // Params:
      // cfg:

  super(GeneralizedRCNN, self).__init__()

  # ===========================================
  # 1.1 Backbone(Resnet50 + FPN) build
  # ===========================================
  { // BEGIN of 1.1

  self.backbone = build_backbone(cfg) // CALL
  {


  build_backbone(cfg) { // BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/backbone.py

    // Params:
      // cfg:

    // cfg.MODEL.BACKBONE.CONV_BODY: R-50-FPN-RETINANET
    // registry.BACKBONES[cfg.MODEL.BACKBONE.CONV_BODY]:
    // <function build_resnet_fpn_p3p7_backbone at 0x7f5cef2b52f0>

    return registry.BACKBONES[cfg.MODEL.BACKBONE.CONV_BODY](cfg)

  build_resnet_fpn_p3p7_backbone(cfg) { // BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/backbone.py

    // Params:
      // cfg:

    # ================================
    # 1-1-1 ResNet50 build
    # ================================
    body = resnet.ResNet(cfg) // CALL
    {

  Resnet.__init__(self, cfg) { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    // Params:
      // cfg:

    super(ResNet, self).__init__()

    // _STEM_MODULES: {'StemWithFixedBatchNorm': <class 'maskrcnn_benchmark.modeling.backbone.resnet.StemWithFixedBatchNorm'>}

    // _cfg.MODEL.RESNETS.STEM_FUNC: StemWithFixedBatchNorm
    stem_module = _STEM_MODULES[cfg.MODEL.RESNETS.STEM_FUNC]
    // stem_module: <class 'maskrcnn_benchmark.modeling.backbone.resnet.StemWithFixedBatchNorm'>

    // cfg.MODEL.BACKBONE.CONV_BODY: R-50-FPN-RETINANET
    stage_specs = _STAGE_SPECS[cfg.MODEL.BACKBONE.CONV_BODY=R-50-FPN-RETINANET]
    // stage_specs: (StageSpec(index=1, block_count=3, return_features=True), StageSpec(index=2, block_count=4, return_features=True), StageSpec(index=3, block_count=6, return_features=True), StageSpec(index=4, block_count=3, return_features=True))

    // _TRANSFORMATION_MODULES: {'BottleneckWithFixedBatchNorm': <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>}
    // cfg.MODEL.RESNETS.TRANS_FUNC: BottleneckWithFixedBatchNorm
    transformation_module = _TRANSFORMATION_MODULES[cfg.MODEL.RESNETS.TRANS_FUNC=BottleneckWithFixedBatchNorm]
    // transformation_module: <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>
    self.stem = stem_module(cfg) // CALL
    {

  StemWithFixedBatchNorm.__init__() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py


  BaseStem.__init__() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py


  } // END BaseStem.__init__()

  } // END StemWithFixedBatchNorm.__init__()

    }
    self.stem = stem_module(cfg) // RETURNED
    // self.stem: StemWithFixedBatchNorm(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): FrozenBatchNorm2d()
)
    num_groups = cfg.MODEL.RESNETS.NUM_GROUPS
    // num_groups.stem: 1
    width_per_group = cfg.MODEL.RESNETS.WIDTH_PER_GROUP
    // width_per_group: 64
    in_channels = cfg.MODEL.RESNETS.STEM_OUT_CHANNELS
    // in_channels: 64
    stage2_bottleneck_channels = num_groups * width_per_group
    // stage2_bottleneck_channels: 64
    stage2_out_channels = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
    stage2_out_channels: 256
    self.stages = []
    self.return_features = {}
    for stage_spec in stage_specs {


      {
      # ---------------------------------------------------------------
      # iteration 1/4
      #  1-th stage_spec: StageSpec(index=1, block_count=3, return_features=True)
      # ---------------------------------------------------------------
      name = "layer" + str(stage_spec.index)
      // name: layer1

      stage2_relative_factor = 2 ** (stage_spec.index - 1)
      // stage2_relative_factor: 1

      bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
      // bottlenec_channels: 64

      out_channels = stage2_out_channels * stage2_relative_factor
      // out_channels: 256

      stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
      // stage_with_dcn: False

      module = _make_stage(
        transformation_module = <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
        in_channels = 64,
        bottleneck_channels = 64,
        out_channels = 256,
        stage_spec.block_count = 3,
        num_groups = 1,
        cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
        first_stride=int(stage_spec.index > 1) + 1: 1,
        dcn_config={
          'stage_with_dcn': False,
          'with_modulated_dcn': False,
          'deformable_groups': 1,
          }
        ) // CALL
        {

  _make_stage() { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    // Params:
      // transformation_module: <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>
      // in_channels: 64
      // bottleneck_channels: 64
      // out_channels: 256
      // block_count: 3
      // num_groups: 3
      // stride_in_1x1: True
      // first_stride: 1
      // dilation: 1
      // dcn_config: {'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}

    blocks = []

    stride = first_stride

    for _ in range(block_count):  {
      {
      # --------------
      # blocks.append iteration 1/3
      # --------------
      blocks.append(
          transformation_module(
              in_channels=64,
              bottleneck_channels=64,
              out_channels=256,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (downsample): Sequential(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): FrozenBatchNorm2d()
  )
  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 2/3
      # --------------
      blocks.append(
          transformation_module(
              in_channels=256,
              bottleneck_channels=64,
              out_channels=256,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 3/3
      # --------------
      blocks.append(
          transformation_module(
              in_channels=256,
              bottleneck_channels=64,
              out_channels=256,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
    }// END for _ in range(block_count):

    return nn.Sequential(*blocks)


  } // END _make_stage()
        }
      module = _make_stage(
        transformation_module = <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
        in_channels = 64,
        bottleneck_channels = 64,
        out_channels = 256,
        stage_spec.block_count = 3,
        num_groups = 1,
        cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
        first_stride=int(stage_spec.index > 1) + 1: 1,
        dcn_config={
          'stage_with_dcn': False,
          'with_modulated_dcn': False,
          'deformable_groups': 1,
          }
        ) // RETURNED

      in_channels = out_channels
      // in_channels: 256

      self.add_module(name=layer1, module)

      self.stages.append(name=layer1)

      // name: layer1
      // stage_spec.return_features: True
      self.return_features[name] = stage_spec.return_features

      }  // END of iteration 1/4


      {
      # ---------------------------------------------------------------
      # iteration 2/4
      #  2-th stage_spec: StageSpec(index=2, block_count=4, return_features=True)
      # ---------------------------------------------------------------
      name = "layer" + str(stage_spec.index)
      // name: layer2

      stage2_relative_factor = 2 ** (stage_spec.index - 1)
      // stage2_relative_factor: 2

      bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
      // bottlenec_channels: 128

      out_channels = stage2_out_channels * stage2_relative_factor
      // out_channels: 512

      stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
      // stage_with_dcn: False

      module = _make_stage(
        transformation_module = <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
        in_channels = 256,
        bottleneck_channels = 128,
        out_channels = 512,
        stage_spec.block_count = 4,
        num_groups = 1,
        cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
        first_stride=int(stage_spec.index > 1) + 1: 2,
        dcn_config={
          'stage_with_dcn': False,
          'with_modulated_dcn': False,
          'deformable_groups': 1,
          }
        ) // CALL
        {

  _make_stage() { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    // Params:
      // transformation_module: <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>
      // in_channels: 256
      // bottleneck_channels: 128
      // out_channels: 512
      // block_count: 4
      // num_groups: 4
      // stride_in_1x1: True
      // first_stride: 2
      // dilation: 1
      // dcn_config: {'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}

    blocks = []

    stride = first_stride

    for _ in range(block_count):  {
      {
      # --------------
      # blocks.append iteration 1/4
      # --------------
      blocks.append(
          transformation_module(
              in_channels=256,
              bottleneck_channels=128,
              out_channels=512,
              num_groups=1,
              stride_in_1x1=True,
              stride=2,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (downsample): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): FrozenBatchNorm2d()
  )
  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 2/4
      # --------------
      blocks.append(
          transformation_module(
              in_channels=512,
              bottleneck_channels=128,
              out_channels=512,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 3/4
      # --------------
      blocks.append(
          transformation_module(
              in_channels=512,
              bottleneck_channels=128,
              out_channels=512,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 4/4
      # --------------
      blocks.append(
          transformation_module(
              in_channels=512,
              bottleneck_channels=128,
              out_channels=512,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
    }// END for _ in range(block_count):

    return nn.Sequential(*blocks)


  } // END _make_stage()
        }
      module = _make_stage(
        transformation_module = <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
        in_channels = 256,
        bottleneck_channels = 128,
        out_channels = 512,
        stage_spec.block_count = 4,
        num_groups = 1,
        cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
        first_stride=int(stage_spec.index > 1) + 1: 2,
        dcn_config={
          'stage_with_dcn': False,
          'with_modulated_dcn': False,
          'deformable_groups': 1,
          }
        ) // RETURNED

      in_channels = out_channels
      // in_channels: 512

      self.add_module(name=layer2, module)

      self.stages.append(name=layer2)

      // name: layer2
      // stage_spec.return_features: True
      self.return_features[name] = stage_spec.return_features

      }  // END of iteration 2/4


      {
      # ---------------------------------------------------------------
      # iteration 3/4
      #  3-th stage_spec: StageSpec(index=3, block_count=6, return_features=True)
      # ---------------------------------------------------------------
      name = "layer" + str(stage_spec.index)
      // name: layer3

      stage2_relative_factor = 2 ** (stage_spec.index - 1)
      // stage2_relative_factor: 4

      bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
      // bottlenec_channels: 256

      out_channels = stage2_out_channels * stage2_relative_factor
      // out_channels: 1024

      stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
      // stage_with_dcn: False

      module = _make_stage(
        transformation_module = <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
        in_channels = 512,
        bottleneck_channels = 256,
        out_channels = 1024,
        stage_spec.block_count = 6,
        num_groups = 1,
        cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
        first_stride=int(stage_spec.index > 1) + 1: 2,
        dcn_config={
          'stage_with_dcn': False,
          'with_modulated_dcn': False,
          'deformable_groups': 1,
          }
        ) // CALL
        {

  _make_stage() { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    // Params:
      // transformation_module: <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>
      // in_channels: 512
      // bottleneck_channels: 256
      // out_channels: 1024
      // block_count: 6
      // num_groups: 6
      // stride_in_1x1: True
      // first_stride: 2
      // dilation: 1
      // dcn_config: {'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}

    blocks = []

    stride = first_stride

    for _ in range(block_count):  {
      {
      # --------------
      # blocks.append iteration 1/6
      # --------------
      blocks.append(
          transformation_module(
              in_channels=512,
              bottleneck_channels=256,
              out_channels=1024,
              num_groups=1,
              stride_in_1x1=True,
              stride=2,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (downsample): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): FrozenBatchNorm2d()
  )
  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 2/6
      # --------------
      blocks.append(
          transformation_module(
              in_channels=1024,
              bottleneck_channels=256,
              out_channels=1024,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 3/6
      # --------------
      blocks.append(
          transformation_module(
              in_channels=1024,
              bottleneck_channels=256,
              out_channels=1024,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 4/6
      # --------------
      blocks.append(
          transformation_module(
              in_channels=1024,
              bottleneck_channels=256,
              out_channels=1024,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 5/6
      # --------------
      blocks.append(
          transformation_module(
              in_channels=1024,
              bottleneck_channels=256,
              out_channels=1024,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 6/6
      # --------------
      blocks.append(
          transformation_module(
              in_channels=1024,
              bottleneck_channels=256,
              out_channels=1024,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
    }// END for _ in range(block_count):

    return nn.Sequential(*blocks)


  } // END _make_stage()
        }
      module = _make_stage(
        transformation_module = <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
        in_channels = 512,
        bottleneck_channels = 256,
        out_channels = 1024,
        stage_spec.block_count = 6,
        num_groups = 1,
        cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
        first_stride=int(stage_spec.index > 1) + 1: 2,
        dcn_config={
          'stage_with_dcn': False,
          'with_modulated_dcn': False,
          'deformable_groups': 1,
          }
        ) // RETURNED

      in_channels = out_channels
      // in_channels: 1024

      self.add_module(name=layer3, module)

      self.stages.append(name=layer3)

      // name: layer3
      // stage_spec.return_features: True
      self.return_features[name] = stage_spec.return_features

      }  // END of iteration 3/4


      {
      # ---------------------------------------------------------------
      # iteration 4/4
      #  4-th stage_spec: StageSpec(index=4, block_count=3, return_features=True)
      # ---------------------------------------------------------------
      name = "layer" + str(stage_spec.index)
      // name: layer4

      stage2_relative_factor = 2 ** (stage_spec.index - 1)
      // stage2_relative_factor: 8

      bottleneck_channels = stage2_bottleneck_channels * stage2_relative_factor
      // bottlenec_channels: 512

      out_channels = stage2_out_channels * stage2_relative_factor
      // out_channels: 2048

      stage_with_dcn = cfg.MODEL.RESNETS.STAGE_WITH_DCN[stage_spec.index - 1]
      // stage_with_dcn: False

      module = _make_stage(
        transformation_module = <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
        in_channels = 1024,
        bottleneck_channels = 512,
        out_channels = 2048,
        stage_spec.block_count = 3,
        num_groups = 1,
        cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
        first_stride=int(stage_spec.index > 1) + 1: 2,
        dcn_config={
          'stage_with_dcn': False,
          'with_modulated_dcn': False,
          'deformable_groups': 1,
          }
        ) // CALL
        {

  _make_stage() { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    // Params:
      // transformation_module: <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>
      // in_channels: 1024
      // bottleneck_channels: 512
      // out_channels: 2048
      // block_count: 3
      // num_groups: 3
      // stride_in_1x1: True
      // first_stride: 2
      // dilation: 1
      // dcn_config: {'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}

    blocks = []

    stride = first_stride

    for _ in range(block_count):  {
      {
      # --------------
      # blocks.append iteration 1/3
      # --------------
      blocks.append(
          transformation_module(
              in_channels=1024,
              bottleneck_channels=512,
              out_channels=2048,
              num_groups=1,
              stride_in_1x1=True,
              stride=2,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (downsample): Sequential(
    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): FrozenBatchNorm2d()
  )
  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 2/3
      # --------------
      blocks.append(
          transformation_module(
              in_channels=2048,
              bottleneck_channels=512,
              out_channels=2048,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
      {
      # --------------
      # blocks.append iteration 3/3
      # --------------
      blocks.append(
          transformation_module(
              in_channels=2048,
              bottleneck_channels=512,
              out_channels=2048,
              num_groups=1,
              stride_in_1x1=True,
              stride=1,
              dilation=1,
              dcn_config={'stage_with_dcn': False, 'with_modulated_dcn': False, 'deformable_groups': 1}
         )
      )

      blocks[-1]:BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)
      stride = 1
      in_channels = out_channels

      }
    }// END for _ in range(block_count):

    return nn.Sequential(*blocks)


  } // END _make_stage()
        }
      module = _make_stage(
        transformation_module = <class 'maskrcnn_benchmark.modeling.backbone.resnet.BottleneckWithFixedBatchNorm'>,
        in_channels = 1024,
        bottleneck_channels = 512,
        out_channels = 2048,
        stage_spec.block_count = 3,
        num_groups = 1,
        cfg.MODEL.RESNETS.STRIDE_IN_1X1 : True,
        first_stride=int(stage_spec.index > 1) + 1: 2,
        dcn_config={
          'stage_with_dcn': False,
          'with_modulated_dcn': False,
          'deformable_groups': 1,
          }
        ) // RETURNED

      in_channels = out_channels
      // in_channels: 2048

      self.add_module(name=layer4, module)

      self.stages.append(name=layer4)

      // name: layer4
      // stage_spec.return_features: True
      self.return_features[name] = stage_spec.return_features

      }  // END of iteration 4/4

} // END for stage_spec in stage_specs:

      // cfg.MODEL.BACKBONE.FREEZE_CONV_BODY_AT: 2)
      self._freeze_backbone(cfg.MODEL.BACKBONE.FREEZE_CONV_BODY_AT)

  Resnet.__freeze_backbone(self, freeze_at) { // BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    // Params:
      // freeze_at: 2
  } // END Resnet.__freeze_backbone(self, freeze_at)

} // END Resnet.__init__(self, cfg)


    }
    body = resnet.ResNet(cfg) // RETURNED



    # ================================
    # 1-1-2 FPN build
    # ================================

    # get the channels parameters required by fpn
    // cfg.MODEL.RESNETS.RES2_OUT_CHANNELS: 256
    in_channels_stage2 = cfg.MODEL.RESNETS.RES2_OUT_CHANNELS
    // in_channels_stage2: 256

    // cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS:1024
    out_channels = cfg.MODEL.RESNETS.BACKBONE_OUT_CHANNELS
    // out_channels: 1024

    // in_channels_stage2: 256
    // out_channels: 1024
    // cfg.MODEL.RETINANET.USE_C5: True
    in_channels_p6p7 = in_channels_stage2 * 8 if cfg.MODEL.RETINANET.USE_C5 else out_channels
    // in_channels_p6p7: 2048


    fpn = fpn_module.FPN(
        in_channels_list = [0, 512, 1024, 2048],
        out_channels = 1024,
        conv_block=conv_with_kaiming_uniform( cfg.MODEL.FPN.USE_GN =False, cfg.MODEL.FPN.USE_RELU =False ),
        top_blocks=fpn_module.LastLevelP6P7(in_channels_p6p7=2048, out_channels=1024,) // CALL

      conv_with_kaiming_uniform(use_gn=False, use_relut=False) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/make_layers.py
      } // END conv_with_kaiming_uniform(use_gn=False, use_relu=False)

    # =================================
    # 1-1-2-1 FPN.LastLevelP6P7 build
    # =================================

      LastLevelP6P7.__init__(self, in_channels, out_channels) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/fpn.py

        > Param:
          >in_channels: 2048
          >out_channels: 1024

        super(LastLevelP6P7, self).__init__()
        self.p6 = nn.Conv2d(in_channels=2048, out_channels=1024, 3, 2, 1)
        // self.p6: Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))

        self.p7 = nn.Conv2d(out_channels=1024, out_channels=1024, 3, 2, 1)
        // self.p7: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))

        for module in [self.p6, self.p7] {
          module=Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          nn.init.kaiming_uniform_(module.weight=module.weight, a=1)

          nn.init.constant_(module.bias=module.bias, 0)

          module=Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          nn.init.kaiming_uniform_(module.weight=module.weight, a=1)

          nn.init.constant_(module.bias=module.bias, 0)

        } // END for module in [self.p6, self.p7]

        self.use_P5 = in_channels == out_channels
          // self.use_p5: False

        } // END LastLevelP6P7.__init__(self, in_channels, out_channels)


    # =================================
    # 1-1-2-2 FPN build
    # =================================



FPN.__init__ { // BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/fpn.py

    // Params
      // in_channels_list: [0, 512, 1024, 2048]
      // out_channels: 1024
      // conv_block: <function conv_with_kaiming_uniform.<locals>.make_conv at 0x7f5cc64dcd90>
      // top_blocks: LastLevelP6P7(
  (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)

    super(FPN, self).__init__()

    # create two empty lists
    self.inner_blocks = []
    self.layer_block = []
    for idx, in_channels in enumerate(in_channels_list, 1) {

      {
        # -----------------------------------------------------
        # in_channels:0, iteration 1/4 BEGIN
        # -----------------------------------------------------
        inner_block = "fpn_inner{}".format(idx)
        // inner_block: {inner_block}

        layer_block = "fpn_layer{}".format(idx)
        // layer_block: {layer_block}

        if in_channels ==0, skip

      }
      # iteration 1/4 END

      {
        # -----------------------------------------------------
        # in_channels:512, iteration 2/4 BEGIN
        # -----------------------------------------------------
        inner_block = "fpn_inner{}".format(idx)
        // inner_block: {inner_block}

        layer_block = "fpn_layer{}".format(idx)
        // layer_block: {layer_block}

        // inner_block: fpn_inner2
        // layer_block: fpn_layer2


        make_conv(in_channels, out_channels, kernel_size, stride=1, dilation=1) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/make_layers.py

        // Params:
          // in_channels: 512
          // out_channels: 1024
          // kernel_size: 1
          // stride: 1
          // dilation: 1

        conv = Conv2d(in_channles=512, out_channels=1024, kernel_size=1, stride=1
               padding=0, dilation=1, bias=True, )
        // conv: Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))

        nn.init.kaiming_uniform_(conv.weight, a=1)

        if not use_gn:
          nn.init.constant_(conv.bias, 0)

        module = [conv,]
        // module: [Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))]

        conv: Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        return conv

        } // END conv_with_kaiming_uniform().make_conv()

        inner_block_module = conv_block(in_channels=512, out_channels=1024, 1)
        // inner_block_module: Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))


        make_conv(in_channels, out_channels, kernel_size, stride=1, dilation=1) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/make_layers.py

        // Params:
          // in_channels: 1024
          // out_channels: 1024
          // kernel_size: 3
          // stride: 1
          // dilation: 1

        conv = Conv2d(in_channles=1024, out_channels=1024, kernel_size=3, stride=1
               padding=1, dilation=1, bias=True, )
        // conv: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

        nn.init.kaiming_uniform_(conv.weight, a=1)

        if not use_gn:
          nn.init.constant_(conv.bias, 0)

        module = [conv,]
        // module: [Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

        conv: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        return conv

        } // END conv_with_kaiming_uniform().make_conv()

        layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)
        // layer_block_module: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

        self.add_module(inner_block, inner_block_module)

        self.add_module(layer_block, layer_block_module)

        self.inner_blocks.append(fpn_inner2)

        self.layer_blocks.append(fpn_layer2)

      }
      # iteration 2/4 END

      {
        # -----------------------------------------------------
        # in_channels:1024, iteration 3/4 BEGIN
        # -----------------------------------------------------
        inner_block = "fpn_inner{}".format(idx)
        // inner_block: {inner_block}

        layer_block = "fpn_layer{}".format(idx)
        // layer_block: {layer_block}

        // inner_block: fpn_inner3
        // layer_block: fpn_layer3


        make_conv(in_channels, out_channels, kernel_size, stride=1, dilation=1) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/make_layers.py

        // Params:
          // in_channels: 1024
          // out_channels: 1024
          // kernel_size: 1
          // stride: 1
          // dilation: 1

        conv = Conv2d(in_channles=1024, out_channels=1024, kernel_size=1, stride=1
               padding=0, dilation=1, bias=True, )
        // conv: Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))

        nn.init.kaiming_uniform_(conv.weight, a=1)

        if not use_gn:
          nn.init.constant_(conv.bias, 0)

        module = [conv,]
        // module: [Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))]

        conv: Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        return conv

        } // END conv_with_kaiming_uniform().make_conv()

        inner_block_module = conv_block(in_channels=1024, out_channels=1024, 1)
        // inner_block_module: Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))


        make_conv(in_channels, out_channels, kernel_size, stride=1, dilation=1) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/make_layers.py

        // Params:
          // in_channels: 1024
          // out_channels: 1024
          // kernel_size: 3
          // stride: 1
          // dilation: 1

        conv = Conv2d(in_channles=1024, out_channels=1024, kernel_size=3, stride=1
               padding=1, dilation=1, bias=True, )
        // conv: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

        nn.init.kaiming_uniform_(conv.weight, a=1)

        if not use_gn:
          nn.init.constant_(conv.bias, 0)

        module = [conv,]
        // module: [Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

        conv: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        return conv

        } // END conv_with_kaiming_uniform().make_conv()

        layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)
        // layer_block_module: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

        self.add_module(inner_block, inner_block_module)

        self.add_module(layer_block, layer_block_module)

        self.inner_blocks.append(fpn_inner3)

        self.layer_blocks.append(fpn_layer3)

      }
      # iteration 3/4 END

      {
        # -----------------------------------------------------
        # in_channels:2048, iteration 4/4 BEGIN
        # -----------------------------------------------------
        inner_block = "fpn_inner{}".format(idx)
        // inner_block: {inner_block}

        layer_block = "fpn_layer{}".format(idx)
        // layer_block: {layer_block}

        // inner_block: fpn_inner4
        // layer_block: fpn_layer4


        make_conv(in_channels, out_channels, kernel_size, stride=1, dilation=1) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/make_layers.py

        // Params:
          // in_channels: 2048
          // out_channels: 1024
          // kernel_size: 1
          // stride: 1
          // dilation: 1

        conv = Conv2d(in_channles=2048, out_channels=1024, kernel_size=1, stride=1
               padding=0, dilation=1, bias=True, )
        // conv: Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))

        nn.init.kaiming_uniform_(conv.weight, a=1)

        if not use_gn:
          nn.init.constant_(conv.bias, 0)

        module = [conv,]
        // module: [Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))]

        conv: Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
        return conv

        } // END conv_with_kaiming_uniform().make_conv()

        inner_block_module = conv_block(in_channels=2048, out_channels=1024, 1)
        // inner_block_module: Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))


        make_conv(in_channels, out_channels, kernel_size, stride=1, dilation=1) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/make_layers.py

        // Params:
          // in_channels: 1024
          // out_channels: 1024
          // kernel_size: 3
          // stride: 1
          // dilation: 1

        conv = Conv2d(in_channles=1024, out_channels=1024, kernel_size=3, stride=1
               padding=1, dilation=1, bias=True, )
        // conv: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

        nn.init.kaiming_uniform_(conv.weight, a=1)

        if not use_gn:
          nn.init.constant_(conv.bias, 0)

        module = [conv,]
        // module: [Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))]

        conv: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        return conv

        } // END conv_with_kaiming_uniform().make_conv()

        layer_block_module = conv_block(out_channels=1024, out_channels=1024, 3,1)
        // layer_block_module: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

        self.add_module(inner_block, inner_block_module)

        self.add_module(layer_block, layer_block_module)

        self.inner_blocks.append(fpn_inner4)

        self.layer_blocks.append(fpn_layer4)

      }
      # iteration 4/4 END


    } // END for idx, in_channels in enumerate(in_channels_list, 1)
    self.top_blocks = top_blocks

    // self.inner_blocks: ['fpn_inner2', 'fpn_inner3', 'fpn_inner4']
    // self.fpn_inner2: Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
    // self.fpn_inner3: Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
    // self.fpn_inner4: Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))

  // self.layer_blocks: ['fpn_layer2', 'fpn_layer3', 'fpn_layer4']
    // self.fpn_layer2: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    // self.fpn_layer3: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    // self.fpn_layer4: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))

  // self.top_blocks: LastLevelP6P7(
  (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)

} // END FPN.__init__



    fpn = fpn_module.FPN(

        in_channels_list = [0, 512, 1024, 2048],

        out_channels = 1024,

        conv_block=conv_with_kaiming_uniform( cfg.MODEL.FPN.USE_GN =False, cfg.MODEL.FPN.USE_RELU =False ),

        top_blocks=fpn_module.LastLevelP6P7(in_channels_p6p7=2048, out_channels=1024,) // RETURNED

      // fpn: FPN(
  (fpn_inner2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
  (fpn_layer2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (fpn_inner3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
  (fpn_layer3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (fpn_inner4): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
  (fpn_layer4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (top_blocks): LastLevelP6P7(
    (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  )
)


    model = nn.Sequential(OrderedDict([("body", body), ("fpn", fpn)]))

    model.out_channels = out_channels
      // model.out_channels: 1024

    return model

  } // END build_resnet_fpn_p3p7_backbone(cfg)


  } // END build_backbone(cfg)

  }
  self.backbone = build_backbone(cfg) // RETURNED


  } // END of 1.1 

  # ===========================================
  # 1.2 RPN (Region Proposal Network) build
  # ===========================================
  { // BEGIN of 1.2

  // self.backbone.out_channels: 1024
  self.rpn = build_rpn(cfg, self.backbone.out_channels) // CALL
  build_retinanet(cfg, in_channels) { // BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py

    // Param:
      // cfg:
      // in_channels: 1024
  return RetinaNetModule(cfg, in_channels) // CALL
    RetinaNetModule.__init__(self, cfg, in_channels) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py

      // Params:
        // cfg:
        // in_channels: 1024

  super(RetinaNetModule, self).__init__()
  self.cfg = cfg.clone()
  #============================
  # 1.2.1 anchor generator build
  #============================
  anchor_generator = make_anchor_generator_retinanet(cfg) // CALL
  {

    make_anchor_generator_retinanet(config) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

        // Params
          // anchor_sizes: (32, 64, 128, 256, 512)
          // aspect_ratios: (0.5, 1.0, 2.0)
          // anchor_strides: (8, 16, 32, 64, 128)
          // straddle_thresh: -1
          // octave: 2.0
          // scales_per_octave: 3

        new_anchor_sizes = []


        for size in anchor_sizes {
          {
          //------------------------
          // size: 32
          //------------------------

      per_layer_anchor_sizes = []

          for scale_per_octave in range(scales_per_octave=3) { // BEGIN
            {
            // ------------------------
            // scale_per_octave: 0, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.0
            // size: 32
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 32.0

            }

            {
            // ------------------------
            // scale_per_octave: 1, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.2599210498948732
            // size: 32
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 40.31747359663594

            }

            {
            // ------------------------
            // scale_per_octave: 2, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.5874010519681994
            // size: 32
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 50.79683366298238

            }

            } // END for scale_per_octave in range(scales_per_octave)

          new_anchor_sizes.append(tuple(per_layer_anchor_sizes))
          // new_anchor_sizes[-1]: (32.0, 40.31747359663594, 50.79683366298238)
          }
          {
          //------------------------
          // size: 64
          //------------------------

      per_layer_anchor_sizes = []

          for scale_per_octave in range(scales_per_octave=3) { // BEGIN
            {
            // ------------------------
            // scale_per_octave: 0, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.0
            // size: 64
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 64.0

            }

            {
            // ------------------------
            // scale_per_octave: 1, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.2599210498948732
            // size: 64
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 80.63494719327188

            }

            {
            // ------------------------
            // scale_per_octave: 2, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.5874010519681994
            // size: 64
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 101.59366732596476

            }

            } // END for scale_per_octave in range(scales_per_octave)

          new_anchor_sizes.append(tuple(per_layer_anchor_sizes))
          // new_anchor_sizes[-1]: (64.0, 80.63494719327188, 101.59366732596476)
          }
          {
          //------------------------
          // size: 128
          //------------------------

      per_layer_anchor_sizes = []

          for scale_per_octave in range(scales_per_octave=3) { // BEGIN
            {
            // ------------------------
            // scale_per_octave: 0, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.0
            // size: 128
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 128.0

            }

            {
            // ------------------------
            // scale_per_octave: 1, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.2599210498948732
            // size: 128
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 161.26989438654377

            }

            {
            // ------------------------
            // scale_per_octave: 2, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.5874010519681994
            // size: 128
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 203.18733465192952

            }

            } // END for scale_per_octave in range(scales_per_octave)

          new_anchor_sizes.append(tuple(per_layer_anchor_sizes))
          // new_anchor_sizes[-1]: (128.0, 161.26989438654377, 203.18733465192952)
          }
          {
          //------------------------
          // size: 256
          //------------------------

      per_layer_anchor_sizes = []

          for scale_per_octave in range(scales_per_octave=3) { // BEGIN
            {
            // ------------------------
            // scale_per_octave: 0, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.0
            // size: 256
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 256.0

            }

            {
            // ------------------------
            // scale_per_octave: 1, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.2599210498948732
            // size: 256
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 322.53978877308754

            }

            {
            // ------------------------
            // scale_per_octave: 2, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.5874010519681994
            // size: 256
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 406.37466930385904

            }

            } // END for scale_per_octave in range(scales_per_octave)

          new_anchor_sizes.append(tuple(per_layer_anchor_sizes))
          // new_anchor_sizes[-1]: (256.0, 322.53978877308754, 406.37466930385904)
          }
          {
          //------------------------
          // size: 512
          //------------------------

      per_layer_anchor_sizes = []

          for scale_per_octave in range(scales_per_octave=3) { // BEGIN
            {
            // ------------------------
            // scale_per_octave: 0, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.0
            // size: 512
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 512.0

            }

            {
            // ------------------------
            // scale_per_octave: 1, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.2599210498948732
            // size: 512
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 645.0795775461751

            }

            {
            // ------------------------
            // scale_per_octave: 2, octave: 2.0
            // ------------------------
            octave_scale = octave ** (scale_per_octave / float(scales_per_octave))

            // octave_scale: 1.5874010519681994
            // size: 512
            per_layer_anchor_sizes.append(octave_scale * size)
            // per_layer_anchor_sizes[-1]: 812.7493386077181

            }

            } // END for scale_per_octave in range(scales_per_octave)

          new_anchor_sizes.append(tuple(per_layer_anchor_sizes))
          // new_anchor_sizes[-1]: (512.0, 645.0795775461751, 812.7493386077181)
          }

        } // END for size in anchor_sizes

        new_anchor_sizes:[(32.0, 40.31747359663594, 50.79683366298238), (64.0, 80.63494719327188, 101.59366732596476), (128.0, 161.26989438654377, 203.18733465192952), (256.0, 322.53978877308754, 406.37466930385904), (512.0, 645.0795775461751, 812.7493386077181)]
        aspect_ratios:(0.5, 1.0, 2.0)
        anchor_strides:(8, 16, 32, 64, 128)
        straddle_thresh:-1
        anchor_generator = AnchorGenerator( tuple(new_anchor_sizes), aspect_ratios, anchor_strides, straddle_thresh ) { //CALL
    AnchorGenerator.__init__(sizes, aspect_ratios, anchor_strides, straddle_thresh) { //BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

      // Params
        // sizes: ((32.0, 40.31747359663594, 50.79683366298238), (64.0, 80.63494719327188, 101.59366732596476), (128.0, 161.26989438654377, 203.18733465192952), (256.0, 322.53978877308754, 406.37466930385904), (512.0, 645.0795775461751, 812.7493386077181))
        // aspect_ratios: (0.5, 1.0, 2.0)
        // anchor_strides: (8, 16, 32, 64, 128)
        // straddle_thresh: -1

    if len(anchor_strides) == 1: 
    else: i.e, len(anchor_strides) !=1
      anchor_stride = anchor_strides[0]
      len(anchor_strides):5, len(size): 5
    else: i.e, len(anchor_strides) == len(sizes)
    cell_anchors = [ generate_anchors( anchor_stride,
                     size if isinstance(size, (tuple, list)) else (size,), 
                     aspect_ratios).float()
    for anchor_stride, size in zip(anchor_strides, sizes)
        generate_anchors(stride, sizes, aspect_ratios) { //BEGIN
      //defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Params:
            // stride: 8
            // sizes: (32.0, 40.31747359663594, 50.79683366298238)
            // aspect_ratios: (0.5, 1.0, 2.0)

          return _generate_anchors(stride,
                 np.array(sizes, dtype=np.float) / stride,
                 np.array(aspect_ratios, dtype=np.float),)
            _generate_anchors(base_size, scales, aspect_ratios) { //BEGIN
            // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

              // Params:
                // base_size: 8
                // scales: [4.         5.0396842  6.34960421]
                // aspect_ratios: [0.5 1.  2. ]

            anchor = np.array([1, 1, base_size, base_size], dtype=np.float) - 1
            // anchor: [0. 0. 7. 7.]
        _ratio_enum(anchor, ratios) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params:
            // anchor: [0. 0. 7. 7.]
            // ratios: [0.5 1.  2. ]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [0. 0. 7. 7.]

            w = anchor[2] - anchor[0] + 1
            w: 8.0
            h = anchor[3] - anchor[1] + 1
            h: 8.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 3.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 3.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 8.0, h: 8.0, x_ctr: 3.5, y_ctr: 3.5
          size = w * h
          // size: 64.0
          size_ratios = size / ratios
          // size_ratios: [128.  64.  32.]
          ws = np.round(np.sqrt(size_ratios))
          // ws: [11.  8.  6.]
          hs = np.round(ws * ratios)
          // hs: [ 6.  8. 12.]
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [11.  8.  6.]
          // hs: [ 6.  8. 12.]
          // x_ctr: 3.5
          // y_ctr: 3.5

        ws = ws[:, np.newaxis]
          // ws: [[11.]
 [ 8.]
 [ 6.]]
        hs = hs[:, np.newaxis]
          // hs: [[ 6.]
 [ 8.]
 [12.]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-1.5  1.   8.5  6. ]
 [ 0.   0.   7.   7. ]
 [ 1.  -2.   6.   9. ]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)
          anchors = _mkanchors(ws, hs, x_ctr, y_ctr)
          // anchors: [[-1.5  1.   8.5  6. ]
 [ 0.   0.   7.   7. ]
 [ 1.  -2.   6.   9. ]]

          return anchors
        } // END _ratio_enum(anchor, ratios)
            anchors = _ratio_enum(anchor, aspect_ratios)
            // anchors: [[-1.5  1.   8.5  6. ]
 [ 0.   0.   7.   7. ]
 [ 1.  -2.   6.   9. ]]
            anchors = np.vstack(
            [_scale_enum(anchors[i, :], scales) for i in range(anchors.shape[0])]
            )
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [-1.5  1.   8.5  6. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [-1.5  1.   8.5  6. ]

            w = anchor[2] - anchor[0] + 1
            w: 11.0
            h = anchor[3] - anchor[1] + 1
            h: 6.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 3.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 3.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 11.0, h: 6.0, x_ctr: 3.5, y_ctr: 3.5

          ws = w * scales
          // ws: [44.         55.4365262  69.84564629]

          hs = h * scales
          // hs: [24.         30.2381052  38.09762525]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [44.         55.4365262  69.84564629]
          // hs: [24.         30.2381052  38.09762525]
          // x_ctr: 3.5
          // y_ctr: 3.5

        ws = ws[:, np.newaxis]
          // ws: [[44.        ]
 [55.4365262 ]
 [69.84564629]]
        hs = hs[:, np.newaxis]
          // hs: [[24.        ]
 [30.2381052 ]
 [38.09762525]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-18.          -8.          25.          15.        ]
 [-23.7182631  -11.1190526   30.7182631   18.1190526 ]
 [-30.92282314 -15.04881262  37.92282314  22.04881262]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-18.          -8.          25.          15.        ]
 [-23.7182631  -11.1190526   30.7182631   18.1190526 ]
 [-30.92282314 -15.04881262  37.92282314  22.04881262]]

          return anchors: [[-18.          -8.          25.          15.        ]
 [-23.7182631  -11.1190526   30.7182631   18.1190526 ]
 [-30.92282314 -15.04881262  37.92282314  22.04881262]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [0. 0. 7. 7.]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [0. 0. 7. 7.]

            w = anchor[2] - anchor[0] + 1
            w: 8.0
            h = anchor[3] - anchor[1] + 1
            h: 8.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 3.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 3.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 8.0, h: 8.0, x_ctr: 3.5, y_ctr: 3.5

          ws = w * scales
          // ws: [32.         40.3174736  50.79683366]

          hs = h * scales
          // hs: [32.         40.3174736  50.79683366]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [32.         40.3174736  50.79683366]
          // hs: [32.         40.3174736  50.79683366]
          // x_ctr: 3.5
          // y_ctr: 3.5

        ws = ws[:, np.newaxis]
          // ws: [[32.        ]
 [40.3174736 ]
 [50.79683366]]
        hs = hs[:, np.newaxis]
          // hs: [[32.        ]
 [40.3174736 ]
 [50.79683366]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-12.         -12.          19.          19.        ]
 [-16.1587368  -16.1587368   23.1587368   23.1587368 ]
 [-21.39841683 -21.39841683  28.39841683  28.39841683]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-12.         -12.          19.          19.        ]
 [-16.1587368  -16.1587368   23.1587368   23.1587368 ]
 [-21.39841683 -21.39841683  28.39841683  28.39841683]]

          return anchors: [[-12.         -12.          19.          19.        ]
 [-16.1587368  -16.1587368   23.1587368   23.1587368 ]
 [-21.39841683 -21.39841683  28.39841683  28.39841683]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [ 1. -2.  6.  9.]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 1. -2.  6.  9.]

            w = anchor[2] - anchor[0] + 1
            w: 6.0
            h = anchor[3] - anchor[1] + 1
            h: 12.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 3.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 3.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 6.0, h: 12.0, x_ctr: 3.5, y_ctr: 3.5

          ws = w * scales
          // ws: [24.         30.2381052  38.09762525]

          hs = h * scales
          // hs: [48.         60.47621039 76.19525049]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [24.         30.2381052  38.09762525]
          // hs: [48.         60.47621039 76.19525049]
          // x_ctr: 3.5
          // y_ctr: 3.5

        ws = ws[:, np.newaxis]
          // ws: [[24.        ]
 [30.2381052 ]
 [38.09762525]]
        hs = hs[:, np.newaxis]
          // hs: [[48.        ]
 [60.47621039]
 [76.19525049]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[ -8.         -20.          15.          27.        ]
 [-11.1190526  -26.2381052   18.1190526   33.2381052 ]
 [-15.04881262 -34.09762525  22.04881262  41.09762525]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[ -8.         -20.          15.          27.        ]
 [-11.1190526  -26.2381052   18.1190526   33.2381052 ]
 [-15.04881262 -34.09762525  22.04881262  41.09762525]]

          return anchors: [[ -8.         -20.          15.          27.        ]
 [-11.1190526  -26.2381052   18.1190526   33.2381052 ]
 [-15.04881262 -34.09762525  22.04881262  41.09762525]]
        } // END _scale_enum(anchor, scales)
            // anchors: [[-18.          -8.          25.          15.        ]
 [-23.7182631  -11.1190526   30.7182631   18.1190526 ]
 [-30.92282314 -15.04881262  37.92282314  22.04881262]
 [-12.         -12.          19.          19.        ]
 [-16.1587368  -16.1587368   23.1587368   23.1587368 ]
 [-21.39841683 -21.39841683  28.39841683  28.39841683]
 [ -8.         -20.          15.          27.        ]
 [-11.1190526  -26.2381052   18.1190526   33.2381052 ]
 [-15.04881262 -34.09762525  22.04881262  41.09762525]]
              return torch.from_numpy(anchors)
            } // END _generate_anchors(base_size, scales, apect_ratios) END
          } // END generate_anchors(stride, sizes, aspect_ratios)
        generate_anchors(stride, sizes, aspect_ratios) { //BEGIN
      //defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Params:
            // stride: 16
            // sizes: (64.0, 80.63494719327188, 101.59366732596476)
            // aspect_ratios: (0.5, 1.0, 2.0)

          return _generate_anchors(stride,
                 np.array(sizes, dtype=np.float) / stride,
                 np.array(aspect_ratios, dtype=np.float),)
            _generate_anchors(base_size, scales, aspect_ratios) { //BEGIN
            // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

              // Params:
                // base_size: 16
                // scales: [4.         5.0396842  6.34960421]
                // aspect_ratios: [0.5 1.  2. ]

            anchor = np.array([1, 1, base_size, base_size], dtype=np.float) - 1
            // anchor: [ 0.  0. 15. 15.]
        _ratio_enum(anchor, ratios) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params:
            // anchor: [ 0.  0. 15. 15.]
            // ratios: [0.5 1.  2. ]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 0.  0. 15. 15.]

            w = anchor[2] - anchor[0] + 1
            w: 16.0
            h = anchor[3] - anchor[1] + 1
            h: 16.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 7.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 7.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 16.0, h: 16.0, x_ctr: 7.5, y_ctr: 7.5
          size = w * h
          // size: 256.0
          size_ratios = size / ratios
          // size_ratios: [512. 256. 128.]
          ws = np.round(np.sqrt(size_ratios))
          // ws: [23. 16. 11.]
          hs = np.round(ws * ratios)
          // hs: [12. 16. 22.]
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [23. 16. 11.]
          // hs: [12. 16. 22.]
          // x_ctr: 7.5
          // y_ctr: 7.5

        ws = ws[:, np.newaxis]
          // ws: [[23.]
 [16.]
 [11.]]
        hs = hs[:, np.newaxis]
          // hs: [[12.]
 [16.]
 [22.]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-3.5  2.  18.5 13. ]
 [ 0.   0.  15.  15. ]
 [ 2.5 -3.  12.5 18. ]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)
          anchors = _mkanchors(ws, hs, x_ctr, y_ctr)
          // anchors: [[-3.5  2.  18.5 13. ]
 [ 0.   0.  15.  15. ]
 [ 2.5 -3.  12.5 18. ]]

          return anchors
        } // END _ratio_enum(anchor, ratios)
            anchors = _ratio_enum(anchor, aspect_ratios)
            // anchors: [[-3.5  2.  18.5 13. ]
 [ 0.   0.  15.  15. ]
 [ 2.5 -3.  12.5 18. ]]
            anchors = np.vstack(
            [_scale_enum(anchors[i, :], scales) for i in range(anchors.shape[0])]
            )
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [-3.5  2.  18.5 13. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [-3.5  2.  18.5 13. ]

            w = anchor[2] - anchor[0] + 1
            w: 23.0
            h = anchor[3] - anchor[1] + 1
            h: 12.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 7.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 7.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 23.0, h: 12.0, x_ctr: 7.5, y_ctr: 7.5

          ws = w * scales
          // ws: [ 92.         115.91273659 146.04089678]

          hs = h * scales
          // hs: [48.         60.47621039 76.19525049]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [ 92.         115.91273659 146.04089678]
          // hs: [48.         60.47621039 76.19525049]
          // x_ctr: 7.5
          // y_ctr: 7.5

        ws = ws[:, np.newaxis]
          // ws: [[ 92.        ]
 [115.91273659]
 [146.04089678]]
        hs = hs[:, np.newaxis]
          // hs: [[48.        ]
 [60.47621039]
 [76.19525049]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-38.         -16.          53.          31.        ]
 [-49.9563683  -22.2381052   64.9563683   37.2381052 ]
 [-65.02044839 -30.09762525  80.02044839  45.09762525]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-38.         -16.          53.          31.        ]
 [-49.9563683  -22.2381052   64.9563683   37.2381052 ]
 [-65.02044839 -30.09762525  80.02044839  45.09762525]]

          return anchors: [[-38.         -16.          53.          31.        ]
 [-49.9563683  -22.2381052   64.9563683   37.2381052 ]
 [-65.02044839 -30.09762525  80.02044839  45.09762525]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [ 0.  0. 15. 15.]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 0.  0. 15. 15.]

            w = anchor[2] - anchor[0] + 1
            w: 16.0
            h = anchor[3] - anchor[1] + 1
            h: 16.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 7.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 7.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 16.0, h: 16.0, x_ctr: 7.5, y_ctr: 7.5

          ws = w * scales
          // ws: [ 64.          80.63494719 101.59366733]

          hs = h * scales
          // hs: [ 64.          80.63494719 101.59366733]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [ 64.          80.63494719 101.59366733]
          // hs: [ 64.          80.63494719 101.59366733]
          // x_ctr: 7.5
          // y_ctr: 7.5

        ws = ws[:, np.newaxis]
          // ws: [[ 64.        ]
 [ 80.63494719]
 [101.59366733]]
        hs = hs[:, np.newaxis]
          // hs: [[ 64.        ]
 [ 80.63494719]
 [101.59366733]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-24.         -24.          39.          39.        ]
 [-32.3174736  -32.3174736   47.3174736   47.3174736 ]
 [-42.79683366 -42.79683366  57.79683366  57.79683366]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-24.         -24.          39.          39.        ]
 [-32.3174736  -32.3174736   47.3174736   47.3174736 ]
 [-42.79683366 -42.79683366  57.79683366  57.79683366]]

          return anchors: [[-24.         -24.          39.          39.        ]
 [-32.3174736  -32.3174736   47.3174736   47.3174736 ]
 [-42.79683366 -42.79683366  57.79683366  57.79683366]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [ 2.5 -3.  12.5 18. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 2.5 -3.  12.5 18. ]

            w = anchor[2] - anchor[0] + 1
            w: 11.0
            h = anchor[3] - anchor[1] + 1
            h: 22.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 7.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 7.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 11.0, h: 22.0, x_ctr: 7.5, y_ctr: 7.5

          ws = w * scales
          // ws: [44.         55.4365262  69.84564629]

          hs = h * scales
          // hs: [ 88.         110.87305239 139.69129257]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [44.         55.4365262  69.84564629]
          // hs: [ 88.         110.87305239 139.69129257]
          // x_ctr: 7.5
          // y_ctr: 7.5

        ws = ws[:, np.newaxis]
          // ws: [[44.        ]
 [55.4365262 ]
 [69.84564629]]
        hs = hs[:, np.newaxis]
          // hs: [[ 88.        ]
 [110.87305239]
 [139.69129257]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-14.         -36.          29.          51.        ]
 [-19.7182631  -47.4365262   34.7182631   62.4365262 ]
 [-26.92282314 -61.84564629  41.92282314  76.84564629]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-14.         -36.          29.          51.        ]
 [-19.7182631  -47.4365262   34.7182631   62.4365262 ]
 [-26.92282314 -61.84564629  41.92282314  76.84564629]]

          return anchors: [[-14.         -36.          29.          51.        ]
 [-19.7182631  -47.4365262   34.7182631   62.4365262 ]
 [-26.92282314 -61.84564629  41.92282314  76.84564629]]
        } // END _scale_enum(anchor, scales)
            // anchors: [[-38.         -16.          53.          31.        ]
 [-49.9563683  -22.2381052   64.9563683   37.2381052 ]
 [-65.02044839 -30.09762525  80.02044839  45.09762525]
 [-24.         -24.          39.          39.        ]
 [-32.3174736  -32.3174736   47.3174736   47.3174736 ]
 [-42.79683366 -42.79683366  57.79683366  57.79683366]
 [-14.         -36.          29.          51.        ]
 [-19.7182631  -47.4365262   34.7182631   62.4365262 ]
 [-26.92282314 -61.84564629  41.92282314  76.84564629]]
              return torch.from_numpy(anchors)
            } // END _generate_anchors(base_size, scales, apect_ratios) END
          } // END generate_anchors(stride, sizes, aspect_ratios)
        generate_anchors(stride, sizes, aspect_ratios) { //BEGIN
      //defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Params:
            // stride: 32
            // sizes: (128.0, 161.26989438654377, 203.18733465192952)
            // aspect_ratios: (0.5, 1.0, 2.0)

          return _generate_anchors(stride,
                 np.array(sizes, dtype=np.float) / stride,
                 np.array(aspect_ratios, dtype=np.float),)
            _generate_anchors(base_size, scales, aspect_ratios) { //BEGIN
            // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

              // Params:
                // base_size: 32
                // scales: [4.         5.0396842  6.34960421]
                // aspect_ratios: [0.5 1.  2. ]

            anchor = np.array([1, 1, base_size, base_size], dtype=np.float) - 1
            // anchor: [ 0.  0. 31. 31.]
        _ratio_enum(anchor, ratios) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params:
            // anchor: [ 0.  0. 31. 31.]
            // ratios: [0.5 1.  2. ]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 0.  0. 31. 31.]

            w = anchor[2] - anchor[0] + 1
            w: 32.0
            h = anchor[3] - anchor[1] + 1
            h: 32.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 15.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 15.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 32.0, h: 32.0, x_ctr: 15.5, y_ctr: 15.5
          size = w * h
          // size: 1024.0
          size_ratios = size / ratios
          // size_ratios: [2048. 1024.  512.]
          ws = np.round(np.sqrt(size_ratios))
          // ws: [45. 32. 23.]
          hs = np.round(ws * ratios)
          // hs: [22. 32. 46.]
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [45. 32. 23.]
          // hs: [22. 32. 46.]
          // x_ctr: 15.5
          // y_ctr: 15.5

        ws = ws[:, np.newaxis]
          // ws: [[45.]
 [32.]
 [23.]]
        hs = hs[:, np.newaxis]
          // hs: [[22.]
 [32.]
 [46.]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-6.5  5.  37.5 26. ]
 [ 0.   0.  31.  31. ]
 [ 4.5 -7.  26.5 38. ]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)
          anchors = _mkanchors(ws, hs, x_ctr, y_ctr)
          // anchors: [[-6.5  5.  37.5 26. ]
 [ 0.   0.  31.  31. ]
 [ 4.5 -7.  26.5 38. ]]

          return anchors
        } // END _ratio_enum(anchor, ratios)
            anchors = _ratio_enum(anchor, aspect_ratios)
            // anchors: [[-6.5  5.  37.5 26. ]
 [ 0.   0.  31.  31. ]
 [ 4.5 -7.  26.5 38. ]]
            anchors = np.vstack(
            [_scale_enum(anchors[i, :], scales) for i in range(anchors.shape[0])]
            )
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [-6.5  5.  37.5 26. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [-6.5  5.  37.5 26. ]

            w = anchor[2] - anchor[0] + 1
            w: 45.0
            h = anchor[3] - anchor[1] + 1
            h: 22.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 15.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 15.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 45.0, h: 22.0, x_ctr: 15.5, y_ctr: 15.5

          ws = w * scales
          // ws: [180.         226.78578898 285.73218935]

          hs = h * scales
          // hs: [ 88.         110.87305239 139.69129257]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [180.         226.78578898 285.73218935]
          // hs: [ 88.         110.87305239 139.69129257]
          // x_ctr: 15.5
          // y_ctr: 15.5

        ws = ws[:, np.newaxis]
          // ws: [[180.        ]
 [226.78578898]
 [285.73218935]]
        hs = hs[:, np.newaxis]
          // hs: [[ 88.        ]
 [110.87305239]
 [139.69129257]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[ -74.          -28.          105.           59.        ]
 [ -97.39289449  -39.4365262   128.39289449   70.4365262 ]
 [-126.86609468  -53.84564629  157.86609468   84.84564629]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[ -74.          -28.          105.           59.        ]
 [ -97.39289449  -39.4365262   128.39289449   70.4365262 ]
 [-126.86609468  -53.84564629  157.86609468   84.84564629]]

          return anchors: [[ -74.          -28.          105.           59.        ]
 [ -97.39289449  -39.4365262   128.39289449   70.4365262 ]
 [-126.86609468  -53.84564629  157.86609468   84.84564629]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [ 0.  0. 31. 31.]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 0.  0. 31. 31.]

            w = anchor[2] - anchor[0] + 1
            w: 32.0
            h = anchor[3] - anchor[1] + 1
            h: 32.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 15.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 15.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 32.0, h: 32.0, x_ctr: 15.5, y_ctr: 15.5

          ws = w * scales
          // ws: [128.         161.26989439 203.18733465]

          hs = h * scales
          // hs: [128.         161.26989439 203.18733465]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [128.         161.26989439 203.18733465]
          // hs: [128.         161.26989439 203.18733465]
          // x_ctr: 15.5
          // y_ctr: 15.5

        ws = ws[:, np.newaxis]
          // ws: [[128.        ]
 [161.26989439]
 [203.18733465]]
        hs = hs[:, np.newaxis]
          // hs: [[128.        ]
 [161.26989439]
 [203.18733465]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-48.         -48.          79.          79.        ]
 [-64.63494719 -64.63494719  95.63494719  95.63494719]
 [-85.59366733 -85.59366733 116.59366733 116.59366733]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-48.         -48.          79.          79.        ]
 [-64.63494719 -64.63494719  95.63494719  95.63494719]
 [-85.59366733 -85.59366733 116.59366733 116.59366733]]

          return anchors: [[-48.         -48.          79.          79.        ]
 [-64.63494719 -64.63494719  95.63494719  95.63494719]
 [-85.59366733 -85.59366733 116.59366733 116.59366733]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [ 4.5 -7.  26.5 38. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 4.5 -7.  26.5 38. ]

            w = anchor[2] - anchor[0] + 1
            w: 23.0
            h = anchor[3] - anchor[1] + 1
            h: 46.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 15.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 15.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 23.0, h: 46.0, x_ctr: 15.5, y_ctr: 15.5

          ws = w * scales
          // ws: [ 92.         115.91273659 146.04089678]

          hs = h * scales
          // hs: [184.         231.82547318 292.08179356]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [ 92.         115.91273659 146.04089678]
          // hs: [184.         231.82547318 292.08179356]
          // x_ctr: 15.5
          // y_ctr: 15.5

        ws = ws[:, np.newaxis]
          // ws: [[ 92.        ]
 [115.91273659]
 [146.04089678]]
        hs = hs[:, np.newaxis]
          // hs: [[184.        ]
 [231.82547318]
 [292.08179356]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[ -30.          -76.           61.          107.        ]
 [ -41.9563683   -99.91273659   72.9563683   130.91273659]
 [ -57.02044839 -130.04089678   88.02044839  161.04089678]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[ -30.          -76.           61.          107.        ]
 [ -41.9563683   -99.91273659   72.9563683   130.91273659]
 [ -57.02044839 -130.04089678   88.02044839  161.04089678]]

          return anchors: [[ -30.          -76.           61.          107.        ]
 [ -41.9563683   -99.91273659   72.9563683   130.91273659]
 [ -57.02044839 -130.04089678   88.02044839  161.04089678]]
        } // END _scale_enum(anchor, scales)
            // anchors: [[ -74.          -28.          105.           59.        ]
 [ -97.39289449  -39.4365262   128.39289449   70.4365262 ]
 [-126.86609468  -53.84564629  157.86609468   84.84564629]
 [ -48.          -48.           79.           79.        ]
 [ -64.63494719  -64.63494719   95.63494719   95.63494719]
 [ -85.59366733  -85.59366733  116.59366733  116.59366733]
 [ -30.          -76.           61.          107.        ]
 [ -41.9563683   -99.91273659   72.9563683   130.91273659]
 [ -57.02044839 -130.04089678   88.02044839  161.04089678]]
              return torch.from_numpy(anchors)
            } // END _generate_anchors(base_size, scales, apect_ratios) END
          } // END generate_anchors(stride, sizes, aspect_ratios)
        generate_anchors(stride, sizes, aspect_ratios) { //BEGIN
      //defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Params:
            // stride: 64
            // sizes: (256.0, 322.53978877308754, 406.37466930385904)
            // aspect_ratios: (0.5, 1.0, 2.0)

          return _generate_anchors(stride,
                 np.array(sizes, dtype=np.float) / stride,
                 np.array(aspect_ratios, dtype=np.float),)
            _generate_anchors(base_size, scales, aspect_ratios) { //BEGIN
            // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

              // Params:
                // base_size: 64
                // scales: [4.         5.0396842  6.34960421]
                // aspect_ratios: [0.5 1.  2. ]

            anchor = np.array([1, 1, base_size, base_size], dtype=np.float) - 1
            // anchor: [ 0.  0. 63. 63.]
        _ratio_enum(anchor, ratios) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params:
            // anchor: [ 0.  0. 63. 63.]
            // ratios: [0.5 1.  2. ]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 0.  0. 63. 63.]

            w = anchor[2] - anchor[0] + 1
            w: 64.0
            h = anchor[3] - anchor[1] + 1
            h: 64.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 31.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 31.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 64.0, h: 64.0, x_ctr: 31.5, y_ctr: 31.5
          size = w * h
          // size: 4096.0
          size_ratios = size / ratios
          // size_ratios: [8192. 4096. 2048.]
          ws = np.round(np.sqrt(size_ratios))
          // ws: [91. 64. 45.]
          hs = np.round(ws * ratios)
          // hs: [46. 64. 90.]
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [91. 64. 45.]
          // hs: [46. 64. 90.]
          // x_ctr: 31.5
          // y_ctr: 31.5

        ws = ws[:, np.newaxis]
          // ws: [[91.]
 [64.]
 [45.]]
        hs = hs[:, np.newaxis]
          // hs: [[46.]
 [64.]
 [90.]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-13.5   9.   76.5  54. ]
 [  0.    0.   63.   63. ]
 [  9.5 -13.   53.5  76. ]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)
          anchors = _mkanchors(ws, hs, x_ctr, y_ctr)
          // anchors: [[-13.5   9.   76.5  54. ]
 [  0.    0.   63.   63. ]
 [  9.5 -13.   53.5  76. ]]

          return anchors
        } // END _ratio_enum(anchor, ratios)
            anchors = _ratio_enum(anchor, aspect_ratios)
            // anchors: [[-13.5   9.   76.5  54. ]
 [  0.    0.   63.   63. ]
 [  9.5 -13.   53.5  76. ]]
            anchors = np.vstack(
            [_scale_enum(anchors[i, :], scales) for i in range(anchors.shape[0])]
            )
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [-13.5   9.   76.5  54. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [-13.5   9.   76.5  54. ]

            w = anchor[2] - anchor[0] + 1
            w: 91.0
            h = anchor[3] - anchor[1] + 1
            h: 46.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 31.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 31.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 91.0, h: 46.0, x_ctr: 31.5, y_ctr: 31.5

          ws = w * scales
          // ws: [364.         458.61126216 577.81398292]

          hs = h * scales
          // hs: [184.         231.82547318 292.08179356]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [364.         458.61126216 577.81398292]
          // hs: [184.         231.82547318 292.08179356]
          // x_ctr: 31.5
          // y_ctr: 31.5

        ws = ws[:, np.newaxis]
          // ws: [[364.        ]
 [458.61126216]
 [577.81398292]]
        hs = hs[:, np.newaxis]
          // hs: [[184.        ]
 [231.82547318]
 [292.08179356]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-150.          -60.          213.          123.        ]
 [-197.30563108  -83.91273659  260.30563108  146.91273659]
 [-256.90699146 -114.04089678  319.90699146  177.04089678]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-150.          -60.          213.          123.        ]
 [-197.30563108  -83.91273659  260.30563108  146.91273659]
 [-256.90699146 -114.04089678  319.90699146  177.04089678]]

          return anchors: [[-150.          -60.          213.          123.        ]
 [-197.30563108  -83.91273659  260.30563108  146.91273659]
 [-256.90699146 -114.04089678  319.90699146  177.04089678]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [ 0.  0. 63. 63.]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 0.  0. 63. 63.]

            w = anchor[2] - anchor[0] + 1
            w: 64.0
            h = anchor[3] - anchor[1] + 1
            h: 64.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 31.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 31.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 64.0, h: 64.0, x_ctr: 31.5, y_ctr: 31.5

          ws = w * scales
          // ws: [256.         322.53978877 406.3746693 ]

          hs = h * scales
          // hs: [256.         322.53978877 406.3746693 ]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [256.         322.53978877 406.3746693 ]
          // hs: [256.         322.53978877 406.3746693 ]
          // x_ctr: 31.5
          // y_ctr: 31.5

        ws = ws[:, np.newaxis]
          // ws: [[256.        ]
 [322.53978877]
 [406.3746693 ]]
        hs = hs[:, np.newaxis]
          // hs: [[256.        ]
 [322.53978877]
 [406.3746693 ]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[ -96.          -96.          159.          159.        ]
 [-129.26989439 -129.26989439  192.26989439  192.26989439]
 [-171.18733465 -171.18733465  234.18733465  234.18733465]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[ -96.          -96.          159.          159.        ]
 [-129.26989439 -129.26989439  192.26989439  192.26989439]
 [-171.18733465 -171.18733465  234.18733465  234.18733465]]

          return anchors: [[ -96.          -96.          159.          159.        ]
 [-129.26989439 -129.26989439  192.26989439  192.26989439]
 [-171.18733465 -171.18733465  234.18733465  234.18733465]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [  9.5 -13.   53.5  76. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [  9.5 -13.   53.5  76. ]

            w = anchor[2] - anchor[0] + 1
            w: 45.0
            h = anchor[3] - anchor[1] + 1
            h: 90.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 31.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 31.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 45.0, h: 90.0, x_ctr: 31.5, y_ctr: 31.5

          ws = w * scales
          // ws: [180.         226.78578898 285.73218935]

          hs = h * scales
          // hs: [360.         453.57157796 571.46437871]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [180.         226.78578898 285.73218935]
          // hs: [360.         453.57157796 571.46437871]
          // x_ctr: 31.5
          // y_ctr: 31.5

        ws = ws[:, np.newaxis]
          // ws: [[180.        ]
 [226.78578898]
 [285.73218935]]
        hs = hs[:, np.newaxis]
          // hs: [[360.        ]
 [453.57157796]
 [571.46437871]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[ -58.         -148.          121.          211.        ]
 [ -81.39289449 -194.78578898  144.39289449  257.78578898]
 [-110.86609468 -253.73218935  173.86609468  316.73218935]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[ -58.         -148.          121.          211.        ]
 [ -81.39289449 -194.78578898  144.39289449  257.78578898]
 [-110.86609468 -253.73218935  173.86609468  316.73218935]]

          return anchors: [[ -58.         -148.          121.          211.        ]
 [ -81.39289449 -194.78578898  144.39289449  257.78578898]
 [-110.86609468 -253.73218935  173.86609468  316.73218935]]
        } // END _scale_enum(anchor, scales)
            // anchors: [[-150.          -60.          213.          123.        ]
 [-197.30563108  -83.91273659  260.30563108  146.91273659]
 [-256.90699146 -114.04089678  319.90699146  177.04089678]
 [ -96.          -96.          159.          159.        ]
 [-129.26989439 -129.26989439  192.26989439  192.26989439]
 [-171.18733465 -171.18733465  234.18733465  234.18733465]
 [ -58.         -148.          121.          211.        ]
 [ -81.39289449 -194.78578898  144.39289449  257.78578898]
 [-110.86609468 -253.73218935  173.86609468  316.73218935]]
              return torch.from_numpy(anchors)
            } // END _generate_anchors(base_size, scales, apect_ratios) END
          } // END generate_anchors(stride, sizes, aspect_ratios)
        generate_anchors(stride, sizes, aspect_ratios) { //BEGIN
      //defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Params:
            // stride: 128
            // sizes: (512.0, 645.0795775461751, 812.7493386077181)
            // aspect_ratios: (0.5, 1.0, 2.0)

          return _generate_anchors(stride,
                 np.array(sizes, dtype=np.float) / stride,
                 np.array(aspect_ratios, dtype=np.float),)
            _generate_anchors(base_size, scales, aspect_ratios) { //BEGIN
            // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

              // Params:
                // base_size: 128
                // scales: [4.         5.0396842  6.34960421]
                // aspect_ratios: [0.5 1.  2. ]

            anchor = np.array([1, 1, base_size, base_size], dtype=np.float) - 1
            // anchor: [  0.   0. 127. 127.]
        _ratio_enum(anchor, ratios) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params:
            // anchor: [  0.   0. 127. 127.]
            // ratios: [0.5 1.  2. ]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [  0.   0. 127. 127.]

            w = anchor[2] - anchor[0] + 1
            w: 128.0
            h = anchor[3] - anchor[1] + 1
            h: 128.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 63.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 63.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 128.0, h: 128.0, x_ctr: 63.5, y_ctr: 63.5
          size = w * h
          // size: 16384.0
          size_ratios = size / ratios
          // size_ratios: [32768. 16384.  8192.]
          ws = np.round(np.sqrt(size_ratios))
          // ws: [181. 128.  91.]
          hs = np.round(ws * ratios)
          // hs: [ 90. 128. 182.]
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [181. 128.  91.]
          // hs: [ 90. 128. 182.]
          // x_ctr: 63.5
          // y_ctr: 63.5

        ws = ws[:, np.newaxis]
          // ws: [[181.]
 [128.]
 [ 91.]]
        hs = hs[:, np.newaxis]
          // hs: [[ 90.]
 [128.]
 [182.]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-26.5  19.  153.5 108. ]
 [  0.    0.  127.  127. ]
 [ 18.5 -27.  108.5 154. ]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)
          anchors = _mkanchors(ws, hs, x_ctr, y_ctr)
          // anchors: [[-26.5  19.  153.5 108. ]
 [  0.    0.  127.  127. ]
 [ 18.5 -27.  108.5 154. ]]

          return anchors
        } // END _ratio_enum(anchor, ratios)
            anchors = _ratio_enum(anchor, aspect_ratios)
            // anchors: [[-26.5  19.  153.5 108. ]
 [  0.    0.  127.  127. ]
 [ 18.5 -27.  108.5 154. ]]
            anchors = np.vstack(
            [_scale_enum(anchors[i, :], scales) for i in range(anchors.shape[0])]
            )
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [-26.5  19.  153.5 108. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [-26.5  19.  153.5 108. ]

            w = anchor[2] - anchor[0] + 1
            w: 181.0
            h = anchor[3] - anchor[1] + 1
            h: 90.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 63.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 63.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 181.0, h: 90.0, x_ctr: 63.5, y_ctr: 63.5

          ws = w * scales
          // ws: [ 724.          912.18284012 1149.27836162]

          hs = h * scales
          // hs: [360.         453.57157796 571.46437871]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [ 724.          912.18284012 1149.27836162]
          // hs: [360.         453.57157796 571.46437871]
          // x_ctr: 63.5
          // y_ctr: 63.5

        ws = ws[:, np.newaxis]
          // ws: [[ 724.        ]
 [ 912.18284012]
 [1149.27836162]]
        hs = hs[:, np.newaxis]
          // hs: [[360.        ]
 [453.57157796]
 [571.46437871]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-298.         -116.          425.          243.        ]
 [-392.09142006 -162.78578898  519.09142006  289.78578898]
 [-510.63918081 -221.73218935  637.63918081  348.73218935]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-298.         -116.          425.          243.        ]
 [-392.09142006 -162.78578898  519.09142006  289.78578898]
 [-510.63918081 -221.73218935  637.63918081  348.73218935]]

          return anchors: [[-298.         -116.          425.          243.        ]
 [-392.09142006 -162.78578898  519.09142006  289.78578898]
 [-510.63918081 -221.73218935  637.63918081  348.73218935]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [  0.   0. 127. 127.]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [  0.   0. 127. 127.]

            w = anchor[2] - anchor[0] + 1
            w: 128.0
            h = anchor[3] - anchor[1] + 1
            h: 128.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 63.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 63.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 128.0, h: 128.0, x_ctr: 63.5, y_ctr: 63.5

          ws = w * scales
          // ws: [512.         645.07957755 812.74933861]

          hs = h * scales
          // hs: [512.         645.07957755 812.74933861]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [512.         645.07957755 812.74933861]
          // hs: [512.         645.07957755 812.74933861]
          // x_ctr: 63.5
          // y_ctr: 63.5

        ws = ws[:, np.newaxis]
          // ws: [[512.        ]
 [645.07957755]
 [812.74933861]]
        hs = hs[:, np.newaxis]
          // hs: [[512.        ]
 [645.07957755]
 [812.74933861]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-192.         -192.          319.          319.        ]
 [-258.53978877 -258.53978877  385.53978877  385.53978877]
 [-342.3746693  -342.3746693   469.3746693   469.3746693 ]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-192.         -192.          319.          319.        ]
 [-258.53978877 -258.53978877  385.53978877  385.53978877]
 [-342.3746693  -342.3746693   469.3746693   469.3746693 ]]

          return anchors: [[-192.         -192.          319.          319.        ]
 [-258.53978877 -258.53978877  385.53978877  385.53978877]
 [-342.3746693  -342.3746693   469.3746693   469.3746693 ]]
        } // END _scale_enum(anchor, scales)
          _scale_enum(anchor, scales) { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Param:
            // anchor: [ 18.5 -27.  108.5 154. ]
            // scales: [4.         5.0396842  6.34960421]

        _whctrs(anchors) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
          // Param:
            // anchor: [ 18.5 -27.  108.5 154. ]

            w = anchor[2] - anchor[0] + 1
            w: 91.0
            h = anchor[3] - anchor[1] + 1
            h: 182.0
            x_ctr = anchor[0] + 0.5 * (w - 1)
            x_ctr: 63.5
            y_ctr = anchor[1] + 0.5 * (h - 1)
            y_ctr: 63.5
            return w, h, x_ctr, y_ctr
          } // END _whctrs(anchors)
          w, h, x_ctr, y_ctr = _whctrs(anchor)
          // w: 91.0, h: 182.0, x_ctr: 63.5, y_ctr: 63.5

          ws = w * scales
          // ws: [364.         458.61126216 577.81398292]

          hs = h * scales
          // hs: [ 728.          917.22252432 1155.62796583]

          anchors = _mkanchors(ws, hs, x_ctr, y_ctr) { // CALL
      _mkanchors(ws, hs, x_ctr, y_ctr) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
        // Param:
          // ws: [364.         458.61126216 577.81398292]
          // hs: [ 728.          917.22252432 1155.62796583]
          // x_ctr: 63.5
          // y_ctr: 63.5

        ws = ws[:, np.newaxis]
          // ws: [[364.        ]
 [458.61126216]
 [577.81398292]]
        hs = hs[:, np.newaxis]
          // hs: [[ 728.        ]
 [ 917.22252432]
 [1155.62796583]]
        anchors = np.hstack(
            (
                x_ctr - 0.5 * (ws - 1),
                y_ctr - 0.5 * (hs - 1),
                x_ctr + 0.5 * (ws - 1),
                y_ctr + 0.5 * (hs - 1),
            )
        )
        // anchors: [[-118.         -300.          245.          427.        ]
 [-165.30563108 -394.61126216  292.30563108  521.61126216]
 [-224.90699146 -513.81398292  351.90699146  640.81398292]]

        return anchors
      } // END _mkanchors(ws, hs, x_ctr, y_ctr)


          }anchors = _mkanchors(ws, hs, x_ctr, y_ctr) // RETURNED
          // anchors: [[-118.         -300.          245.          427.        ]
 [-165.30563108 -394.61126216  292.30563108  521.61126216]
 [-224.90699146 -513.81398292  351.90699146  640.81398292]]

          return anchors: [[-118.         -300.          245.          427.        ]
 [-165.30563108 -394.61126216  292.30563108  521.61126216]
 [-224.90699146 -513.81398292  351.90699146  640.81398292]]
        } // END _scale_enum(anchor, scales)
            // anchors: [[-298.         -116.          425.          243.        ]
 [-392.09142006 -162.78578898  519.09142006  289.78578898]
 [-510.63918081 -221.73218935  637.63918081  348.73218935]
 [-192.         -192.          319.          319.        ]
 [-258.53978877 -258.53978877  385.53978877  385.53978877]
 [-342.3746693  -342.3746693   469.3746693   469.3746693 ]
 [-118.         -300.          245.          427.        ]
 [-165.30563108 -394.61126216  292.30563108  521.61126216]
 [-224.90699146 -513.81398292  351.90699146  640.81398292]]
              return torch.from_numpy(anchors)
            } // END _generate_anchors(base_size, scales, apect_ratios) END
          } // END generate_anchors(stride, sizes, aspect_ratios)
      len(cell_anchors): 5
      cell_anchors[i].shape: torch.Size([9, 4])
      cell_anchors[i].shape: torch.Size([9, 4])
      cell_anchors[i].shape: torch.Size([9, 4])
      cell_anchors[i].shape: torch.Size([9, 4])
      cell_anchors[i].shape: torch.Size([9, 4])
  self.strides = anchor_strides
  // self.strides: (8, 16, 32, 64, 128)
  self.cell_anchors = BufferList(cell_anchors) // CALL
BufferList.__init__(slef, buffers=None) { // BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
  Params
    len(buffers): 5
  super(BufferList, self).__init__()
if buffers is not None:
BufferList.extend(self, buffers) { // BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
    Params
    len(buffers): 5

  offset = len(self)
  // offset: 0

  for i, buffer in enumerate(buffers) { // BEGIN
  t{ // BEGIN iteration 0
  #--------------------
buffer.shape: torch.Size([9, 4])
  #--------------------
self.register_buffer(str(offset + i), buffer)
    } // END iteration 0 
  t{ // BEGIN iteration 1
  #--------------------
buffer.shape: torch.Size([9, 4])
  #--------------------
self.register_buffer(str(offset + i), buffer)
    } // END iteration 1 
  t{ // BEGIN iteration 2
  #--------------------
buffer.shape: torch.Size([9, 4])
  #--------------------
self.register_buffer(str(offset + i), buffer)
    } // END iteration 2 
  t{ // BEGIN iteration 3
  #--------------------
buffer.shape: torch.Size([9, 4])
  #--------------------
self.register_buffer(str(offset + i), buffer)
    } // END iteration 3 
  t{ // BEGIN iteration 4
  #--------------------
buffer.shape: torch.Size([9, 4])
  #--------------------
self.register_buffer(str(offset + i), buffer)
    } // END iteration 4 
  } // END for i, buffer in enumerate(buffers)

  self: BufferList()
  return self
} // END BufferList.extend(self, buffers)
self.extend(buffers)
  len(buffers): 5
} // END BufferList.__init__(slef, buffers=None)
  self.cell_anchors = BufferList(cell_anchors) // RETURNED
  // self.cell_anchors: BufferList()
  self.straddle_thresh = straddle_thresh
  // self.straddle_thresh: -1
    } // END AnchorGenerator.__init__(sizes, aspect_ratios, anchor_strides, straddle_thresh)
        }
        anchor_generator = AnchorGenerator( tuple(new_anchor_sizes), aspect_ratios, anchor_strides, straddle_thresh ) // RETURNED
        // anchor_generator = AnchorGenerator(
  (cell_anchors): BufferList()
)


    return anchor_generator
    } // make_anchor_generator_retinanet(config) END

  }
  anchor_generator = make_anchor_generator_retinanet(cfg) // RETURNED

  // anchor_generator: AnchorGenerator(
  (cell_anchors): BufferList()
)
  #============================
  # 1.2.2 RPN head build 
  #============================
  head = RetinaNetHead(cfg, in_channels=1024) // CALL
  {


      RetinaNetHead.__init__(cfg, in_channels) { //BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py

        // Params:
          // cfg:
          //in_channles: 1024

        num_classes = cfg.MODEL.RETINANET.NUM_CLASSES - 1
        // num_classes: 1
        // cfg.MODEL.RETINANET.ASPECT_RATIOS: (0.5, 1.0, 2.0)
        // cfg.MODEL.RETINANET.SCALES_PER_OCTAVE: 3
        num_anchors = len(cfg.MODEL.RETINANET.ASPECT_RATIOS) \
                        * cfg.MODEL.RETINANET.SCALES_PER_OCTAVE
        // num_anchors: 9


} // END RetinaNetHead._init__(cfg, in_channels)
  }
  head = RetinaNetHead(cfg, in_channels=1024) // RETURNED
  // head: RetinaNetHead(
  (cls_tower): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
  )
  (bbox_tower): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
  )
  (cls_logits): Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bbox_pred): Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
  #============================
  # 1.2.3 RPN box_coder build
  #============================
  box_coder = BoxCoder(weights=(10., 10., 5., 5.)) // CALL
  {
    BoxCoder.__init__(self, weights, bbox_xfrom_clip)__ { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/box_coder.py
      // Params:
        weights: (10.0, 10.0, 5.0, 5.0)
        bbox_xform_clip: 4.135166556742356
        self.weights = weights
        self.bbox_xform_clip = bbox_xform_clip

    } // END BoxCoder.__init__(self, weights, bbox_xfrom_clip)__

  }
  box_coder = BoxCoder(weights=(10., 10., 5., 5.)) // RETURNED
  // box_coder: <maskrcnn_benchmark.modeling.box_coder.BoxCoder object at 0x7f5ca4729550>
  #============================
  # 1.2.4 RPN box_selector_test build
  #============================
  box_selector_test = make_retinanet_postprocessor(cfg, box_coder) // CALL
  {
make_retinanet_postprocessor() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/inference.py

  // Params:
    > config:
    > rpn_box_coder:

RPNPostProcessor.__init__() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/inference.py
} // END RPNPostProcessor.__init__()
RetinaNetPostProcessor.__init__() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/inference.py

  // Params:
    > pre_nms_top_n: 1000
    > post_nms_top_n: 1000
    > nms_thresh: 0.4
    > min_size: 0
    > box_coder: <maskrcnn_benchmark.modeling.box_coder.BoxCoder object at 0x7f5ca4729550>
    > fpn_post_nms_top_n: 100
  self.pre_nms_thresh = pre_nms_thresh
  self.pre_nms_top_n = pre_nms_top_n
  self.nms_thresh = nms_thresh
  self.fpn_post_nms_top_n = fpn_post_nms_top_n
  self.min_size = min_size
  self.num_classes = num_classes
  self.box_coder = box_coder

}// END RetinaNetPostProcessor.__init__()
} // END make_retinanet_postprocessor()
  }
  box_selector_test = make_retinanet_postprocessor(cfg, box_coder) // RETURNED
  // box_selector_test: RetinaNetPostProcessor()
  self.anchor_generator = anchor_generator
  self.head = head
  self.box_selector_test = box_selector_test


} // RetinaNetModule.__init__(self, cfg, in_channels) END
  } // END build_retinanet(cfg, in_channels)
  self.rpn = build_rpn(cfg, self.backbone.out_channels) // RETURNED


  // self.rpn: RetinaNetModule(
  (anchor_generator): AnchorGenerator(
    (cell_anchors): BufferList()
  )
  (head): RetinaNetHead(
    (cls_tower): Sequential(
      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
    )
    (bbox_tower): Sequential(
      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (5): ReLU()
      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (7): ReLU()
    )
    (cls_logits): Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bbox_pred): Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (box_selector_test): RetinaNetPostProcessor()
)

  } // END of 1.2

} // END GeneralizedRCNN.__init__(self, cfg)
  }

  self.model = build_detection_model(self.cfg) // RETURNED
  // self.model: GeneralizedRCNN(
  (backbone): Sequential(
    (body): ResNet(
      (stem): StemWithFixedBatchNorm(
        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
        (bn1): FrozenBatchNorm2d()
      )
      (layer1): Sequential(
        (0): BottleneckWithFixedBatchNorm(
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d()
          )
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (1): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (2): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
      )
      (layer2): Sequential(
        (0): BottleneckWithFixedBatchNorm(
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (1): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (2): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (3): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
      )
      (layer3): Sequential(
        (0): BottleneckWithFixedBatchNorm(
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (1): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (2): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (3): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (4): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (5): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
      )
      (layer4): Sequential(
        (0): BottleneckWithFixedBatchNorm(
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d()
          )
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (1): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
        (2): BottleneckWithFixedBatchNorm(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d()
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d()
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d()
        )
      )
    )
    (fpn): FPN(
      (fpn_inner2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fpn_layer2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (fpn_inner3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fpn_layer3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (fpn_inner4): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
      (fpn_layer4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (top_blocks): LastLevelP6P7(
        (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
    )
  )
  (rpn): RetinaNetModule(
    (anchor_generator): AnchorGenerator(
      (cell_anchors): BufferList()
    )
    (head): RetinaNetHead(
      (cls_tower): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (7): ReLU()
      )
      (bbox_tower): Sequential(
        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (7): ReLU()
      )
      (cls_logits): Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bbox_pred): Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (box_selector_test): RetinaNetPostProcessor()
  )
)


  # -----------------------------------------
  # 1.3 set to evaluation mode for interference
  # -----------------------------------------
  self.model.eval()

  // in detection_model_debug.py
  # -----------------------------------------
  # 1.4 Load Weight
  # -----------------------------------------
  checkpointer = DetectronCheckpointer(cfg, self.model, save_dir='/dev/null') // CALL
  checkpointer = DetectronCheckpointer(cfg, self.model, save_dir='/dev/null') // RETURNED
  _ = checkpointer.load(weight){// CALL
INFO:maskrcnn_benchmark.utils.checkpoint:Loading checkpoint from ./model/detection/model_det_v2_200924_002_180k.pth
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn1.bias                  loaded from backbone.body.layer1.0.bn1.bias                  of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn1.running_mean          loaded from backbone.body.layer1.0.bn1.running_mean          of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn1.running_var           loaded from backbone.body.layer1.0.bn1.running_var           of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn1.weight                loaded from backbone.body.layer1.0.bn1.weight                of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn2.bias                  loaded from backbone.body.layer1.0.bn2.bias                  of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn2.running_mean          loaded from backbone.body.layer1.0.bn2.running_mean          of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn2.running_var           loaded from backbone.body.layer1.0.bn2.running_var           of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn2.weight                loaded from backbone.body.layer1.0.bn2.weight                of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn3.bias                  loaded from backbone.body.layer1.0.bn3.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn3.running_mean          loaded from backbone.body.layer1.0.bn3.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn3.running_var           loaded from backbone.body.layer1.0.bn3.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.bn3.weight                loaded from backbone.body.layer1.0.bn3.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.conv1.weight              loaded from backbone.body.layer1.0.conv1.weight              of shape (64, 64, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.conv2.weight              loaded from backbone.body.layer1.0.conv2.weight              of shape (64, 64, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.conv3.weight              loaded from backbone.body.layer1.0.conv3.weight              of shape (256, 64, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.downsample.0.weight       loaded from backbone.body.layer1.0.downsample.0.weight       of shape (256, 64, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.downsample.1.bias         loaded from backbone.body.layer1.0.downsample.1.bias         of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.downsample.1.running_mean loaded from backbone.body.layer1.0.downsample.1.running_mean of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.downsample.1.running_var  loaded from backbone.body.layer1.0.downsample.1.running_var  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.0.downsample.1.weight       loaded from backbone.body.layer1.0.downsample.1.weight       of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn1.bias                  loaded from backbone.body.layer1.1.bn1.bias                  of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn1.running_mean          loaded from backbone.body.layer1.1.bn1.running_mean          of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn1.running_var           loaded from backbone.body.layer1.1.bn1.running_var           of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn1.weight                loaded from backbone.body.layer1.1.bn1.weight                of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn2.bias                  loaded from backbone.body.layer1.1.bn2.bias                  of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn2.running_mean          loaded from backbone.body.layer1.1.bn2.running_mean          of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn2.running_var           loaded from backbone.body.layer1.1.bn2.running_var           of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn2.weight                loaded from backbone.body.layer1.1.bn2.weight                of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn3.bias                  loaded from backbone.body.layer1.1.bn3.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn3.running_mean          loaded from backbone.body.layer1.1.bn3.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn3.running_var           loaded from backbone.body.layer1.1.bn3.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.bn3.weight                loaded from backbone.body.layer1.1.bn3.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.conv1.weight              loaded from backbone.body.layer1.1.conv1.weight              of shape (64, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.conv2.weight              loaded from backbone.body.layer1.1.conv2.weight              of shape (64, 64, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.1.conv3.weight              loaded from backbone.body.layer1.1.conv3.weight              of shape (256, 64, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn1.bias                  loaded from backbone.body.layer1.2.bn1.bias                  of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn1.running_mean          loaded from backbone.body.layer1.2.bn1.running_mean          of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn1.running_var           loaded from backbone.body.layer1.2.bn1.running_var           of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn1.weight                loaded from backbone.body.layer1.2.bn1.weight                of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn2.bias                  loaded from backbone.body.layer1.2.bn2.bias                  of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn2.running_mean          loaded from backbone.body.layer1.2.bn2.running_mean          of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn2.running_var           loaded from backbone.body.layer1.2.bn2.running_var           of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn2.weight                loaded from backbone.body.layer1.2.bn2.weight                of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn3.bias                  loaded from backbone.body.layer1.2.bn3.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn3.running_mean          loaded from backbone.body.layer1.2.bn3.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn3.running_var           loaded from backbone.body.layer1.2.bn3.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.bn3.weight                loaded from backbone.body.layer1.2.bn3.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.conv1.weight              loaded from backbone.body.layer1.2.conv1.weight              of shape (64, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.conv2.weight              loaded from backbone.body.layer1.2.conv2.weight              of shape (64, 64, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer1.2.conv3.weight              loaded from backbone.body.layer1.2.conv3.weight              of shape (256, 64, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn1.bias                  loaded from backbone.body.layer2.0.bn1.bias                  of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn1.running_mean          loaded from backbone.body.layer2.0.bn1.running_mean          of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn1.running_var           loaded from backbone.body.layer2.0.bn1.running_var           of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn1.weight                loaded from backbone.body.layer2.0.bn1.weight                of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn2.bias                  loaded from backbone.body.layer2.0.bn2.bias                  of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn2.running_mean          loaded from backbone.body.layer2.0.bn2.running_mean          of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn2.running_var           loaded from backbone.body.layer2.0.bn2.running_var           of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn2.weight                loaded from backbone.body.layer2.0.bn2.weight                of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn3.bias                  loaded from backbone.body.layer2.0.bn3.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn3.running_mean          loaded from backbone.body.layer2.0.bn3.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn3.running_var           loaded from backbone.body.layer2.0.bn3.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.bn3.weight                loaded from backbone.body.layer2.0.bn3.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.conv1.weight              loaded from backbone.body.layer2.0.conv1.weight              of shape (128, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.conv2.weight              loaded from backbone.body.layer2.0.conv2.weight              of shape (128, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.conv3.weight              loaded from backbone.body.layer2.0.conv3.weight              of shape (512, 128, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.downsample.0.weight       loaded from backbone.body.layer2.0.downsample.0.weight       of shape (512, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.downsample.1.bias         loaded from backbone.body.layer2.0.downsample.1.bias         of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.downsample.1.running_mean loaded from backbone.body.layer2.0.downsample.1.running_mean of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.downsample.1.running_var  loaded from backbone.body.layer2.0.downsample.1.running_var  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.0.downsample.1.weight       loaded from backbone.body.layer2.0.downsample.1.weight       of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn1.bias                  loaded from backbone.body.layer2.1.bn1.bias                  of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn1.running_mean          loaded from backbone.body.layer2.1.bn1.running_mean          of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn1.running_var           loaded from backbone.body.layer2.1.bn1.running_var           of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn1.weight                loaded from backbone.body.layer2.1.bn1.weight                of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn2.bias                  loaded from backbone.body.layer2.1.bn2.bias                  of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn2.running_mean          loaded from backbone.body.layer2.1.bn2.running_mean          of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn2.running_var           loaded from backbone.body.layer2.1.bn2.running_var           of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn2.weight                loaded from backbone.body.layer2.1.bn2.weight                of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn3.bias                  loaded from backbone.body.layer2.1.bn3.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn3.running_mean          loaded from backbone.body.layer2.1.bn3.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn3.running_var           loaded from backbone.body.layer2.1.bn3.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.bn3.weight                loaded from backbone.body.layer2.1.bn3.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.conv1.weight              loaded from backbone.body.layer2.1.conv1.weight              of shape (128, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.conv2.weight              loaded from backbone.body.layer2.1.conv2.weight              of shape (128, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.1.conv3.weight              loaded from backbone.body.layer2.1.conv3.weight              of shape (512, 128, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn1.bias                  loaded from backbone.body.layer2.2.bn1.bias                  of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn1.running_mean          loaded from backbone.body.layer2.2.bn1.running_mean          of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn1.running_var           loaded from backbone.body.layer2.2.bn1.running_var           of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn1.weight                loaded from backbone.body.layer2.2.bn1.weight                of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn2.bias                  loaded from backbone.body.layer2.2.bn2.bias                  of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn2.running_mean          loaded from backbone.body.layer2.2.bn2.running_mean          of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn2.running_var           loaded from backbone.body.layer2.2.bn2.running_var           of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn2.weight                loaded from backbone.body.layer2.2.bn2.weight                of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn3.bias                  loaded from backbone.body.layer2.2.bn3.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn3.running_mean          loaded from backbone.body.layer2.2.bn3.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn3.running_var           loaded from backbone.body.layer2.2.bn3.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.bn3.weight                loaded from backbone.body.layer2.2.bn3.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.conv1.weight              loaded from backbone.body.layer2.2.conv1.weight              of shape (128, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.conv2.weight              loaded from backbone.body.layer2.2.conv2.weight              of shape (128, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.2.conv3.weight              loaded from backbone.body.layer2.2.conv3.weight              of shape (512, 128, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn1.bias                  loaded from backbone.body.layer2.3.bn1.bias                  of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn1.running_mean          loaded from backbone.body.layer2.3.bn1.running_mean          of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn1.running_var           loaded from backbone.body.layer2.3.bn1.running_var           of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn1.weight                loaded from backbone.body.layer2.3.bn1.weight                of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn2.bias                  loaded from backbone.body.layer2.3.bn2.bias                  of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn2.running_mean          loaded from backbone.body.layer2.3.bn2.running_mean          of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn2.running_var           loaded from backbone.body.layer2.3.bn2.running_var           of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn2.weight                loaded from backbone.body.layer2.3.bn2.weight                of shape (128,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn3.bias                  loaded from backbone.body.layer2.3.bn3.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn3.running_mean          loaded from backbone.body.layer2.3.bn3.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn3.running_var           loaded from backbone.body.layer2.3.bn3.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.bn3.weight                loaded from backbone.body.layer2.3.bn3.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.conv1.weight              loaded from backbone.body.layer2.3.conv1.weight              of shape (128, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.conv2.weight              loaded from backbone.body.layer2.3.conv2.weight              of shape (128, 128, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer2.3.conv3.weight              loaded from backbone.body.layer2.3.conv3.weight              of shape (512, 128, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn1.bias                  loaded from backbone.body.layer3.0.bn1.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn1.running_mean          loaded from backbone.body.layer3.0.bn1.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn1.running_var           loaded from backbone.body.layer3.0.bn1.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn1.weight                loaded from backbone.body.layer3.0.bn1.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn2.bias                  loaded from backbone.body.layer3.0.bn2.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn2.running_mean          loaded from backbone.body.layer3.0.bn2.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn2.running_var           loaded from backbone.body.layer3.0.bn2.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn2.weight                loaded from backbone.body.layer3.0.bn2.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn3.bias                  loaded from backbone.body.layer3.0.bn3.bias                  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn3.running_mean          loaded from backbone.body.layer3.0.bn3.running_mean          of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn3.running_var           loaded from backbone.body.layer3.0.bn3.running_var           of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.bn3.weight                loaded from backbone.body.layer3.0.bn3.weight                of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.conv1.weight              loaded from backbone.body.layer3.0.conv1.weight              of shape (256, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.conv2.weight              loaded from backbone.body.layer3.0.conv2.weight              of shape (256, 256, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.conv3.weight              loaded from backbone.body.layer3.0.conv3.weight              of shape (1024, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.downsample.0.weight       loaded from backbone.body.layer3.0.downsample.0.weight       of shape (1024, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.downsample.1.bias         loaded from backbone.body.layer3.0.downsample.1.bias         of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.downsample.1.running_mean loaded from backbone.body.layer3.0.downsample.1.running_mean of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.downsample.1.running_var  loaded from backbone.body.layer3.0.downsample.1.running_var  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.0.downsample.1.weight       loaded from backbone.body.layer3.0.downsample.1.weight       of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn1.bias                  loaded from backbone.body.layer3.1.bn1.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn1.running_mean          loaded from backbone.body.layer3.1.bn1.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn1.running_var           loaded from backbone.body.layer3.1.bn1.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn1.weight                loaded from backbone.body.layer3.1.bn1.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn2.bias                  loaded from backbone.body.layer3.1.bn2.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn2.running_mean          loaded from backbone.body.layer3.1.bn2.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn2.running_var           loaded from backbone.body.layer3.1.bn2.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn2.weight                loaded from backbone.body.layer3.1.bn2.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn3.bias                  loaded from backbone.body.layer3.1.bn3.bias                  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn3.running_mean          loaded from backbone.body.layer3.1.bn3.running_mean          of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn3.running_var           loaded from backbone.body.layer3.1.bn3.running_var           of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.bn3.weight                loaded from backbone.body.layer3.1.bn3.weight                of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.conv1.weight              loaded from backbone.body.layer3.1.conv1.weight              of shape (256, 1024, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.conv2.weight              loaded from backbone.body.layer3.1.conv2.weight              of shape (256, 256, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.1.conv3.weight              loaded from backbone.body.layer3.1.conv3.weight              of shape (1024, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn1.bias                  loaded from backbone.body.layer3.2.bn1.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn1.running_mean          loaded from backbone.body.layer3.2.bn1.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn1.running_var           loaded from backbone.body.layer3.2.bn1.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn1.weight                loaded from backbone.body.layer3.2.bn1.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn2.bias                  loaded from backbone.body.layer3.2.bn2.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn2.running_mean          loaded from backbone.body.layer3.2.bn2.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn2.running_var           loaded from backbone.body.layer3.2.bn2.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn2.weight                loaded from backbone.body.layer3.2.bn2.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn3.bias                  loaded from backbone.body.layer3.2.bn3.bias                  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn3.running_mean          loaded from backbone.body.layer3.2.bn3.running_mean          of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn3.running_var           loaded from backbone.body.layer3.2.bn3.running_var           of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.bn3.weight                loaded from backbone.body.layer3.2.bn3.weight                of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.conv1.weight              loaded from backbone.body.layer3.2.conv1.weight              of shape (256, 1024, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.conv2.weight              loaded from backbone.body.layer3.2.conv2.weight              of shape (256, 256, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.2.conv3.weight              loaded from backbone.body.layer3.2.conv3.weight              of shape (1024, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn1.bias                  loaded from backbone.body.layer3.3.bn1.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn1.running_mean          loaded from backbone.body.layer3.3.bn1.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn1.running_var           loaded from backbone.body.layer3.3.bn1.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn1.weight                loaded from backbone.body.layer3.3.bn1.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn2.bias                  loaded from backbone.body.layer3.3.bn2.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn2.running_mean          loaded from backbone.body.layer3.3.bn2.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn2.running_var           loaded from backbone.body.layer3.3.bn2.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn2.weight                loaded from backbone.body.layer3.3.bn2.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn3.bias                  loaded from backbone.body.layer3.3.bn3.bias                  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn3.running_mean          loaded from backbone.body.layer3.3.bn3.running_mean          of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn3.running_var           loaded from backbone.body.layer3.3.bn3.running_var           of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.bn3.weight                loaded from backbone.body.layer3.3.bn3.weight                of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.conv1.weight              loaded from backbone.body.layer3.3.conv1.weight              of shape (256, 1024, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.conv2.weight              loaded from backbone.body.layer3.3.conv2.weight              of shape (256, 256, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.3.conv3.weight              loaded from backbone.body.layer3.3.conv3.weight              of shape (1024, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn1.bias                  loaded from backbone.body.layer3.4.bn1.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn1.running_mean          loaded from backbone.body.layer3.4.bn1.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn1.running_var           loaded from backbone.body.layer3.4.bn1.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn1.weight                loaded from backbone.body.layer3.4.bn1.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn2.bias                  loaded from backbone.body.layer3.4.bn2.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn2.running_mean          loaded from backbone.body.layer3.4.bn2.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn2.running_var           loaded from backbone.body.layer3.4.bn2.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn2.weight                loaded from backbone.body.layer3.4.bn2.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn3.bias                  loaded from backbone.body.layer3.4.bn3.bias                  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn3.running_mean          loaded from backbone.body.layer3.4.bn3.running_mean          of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn3.running_var           loaded from backbone.body.layer3.4.bn3.running_var           of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.bn3.weight                loaded from backbone.body.layer3.4.bn3.weight                of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.conv1.weight              loaded from backbone.body.layer3.4.conv1.weight              of shape (256, 1024, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.conv2.weight              loaded from backbone.body.layer3.4.conv2.weight              of shape (256, 256, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.4.conv3.weight              loaded from backbone.body.layer3.4.conv3.weight              of shape (1024, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn1.bias                  loaded from backbone.body.layer3.5.bn1.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn1.running_mean          loaded from backbone.body.layer3.5.bn1.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn1.running_var           loaded from backbone.body.layer3.5.bn1.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn1.weight                loaded from backbone.body.layer3.5.bn1.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn2.bias                  loaded from backbone.body.layer3.5.bn2.bias                  of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn2.running_mean          loaded from backbone.body.layer3.5.bn2.running_mean          of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn2.running_var           loaded from backbone.body.layer3.5.bn2.running_var           of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn2.weight                loaded from backbone.body.layer3.5.bn2.weight                of shape (256,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn3.bias                  loaded from backbone.body.layer3.5.bn3.bias                  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn3.running_mean          loaded from backbone.body.layer3.5.bn3.running_mean          of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn3.running_var           loaded from backbone.body.layer3.5.bn3.running_var           of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.bn3.weight                loaded from backbone.body.layer3.5.bn3.weight                of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.conv1.weight              loaded from backbone.body.layer3.5.conv1.weight              of shape (256, 1024, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.conv2.weight              loaded from backbone.body.layer3.5.conv2.weight              of shape (256, 256, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer3.5.conv3.weight              loaded from backbone.body.layer3.5.conv3.weight              of shape (1024, 256, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn1.bias                  loaded from backbone.body.layer4.0.bn1.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn1.running_mean          loaded from backbone.body.layer4.0.bn1.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn1.running_var           loaded from backbone.body.layer4.0.bn1.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn1.weight                loaded from backbone.body.layer4.0.bn1.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn2.bias                  loaded from backbone.body.layer4.0.bn2.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn2.running_mean          loaded from backbone.body.layer4.0.bn2.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn2.running_var           loaded from backbone.body.layer4.0.bn2.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn2.weight                loaded from backbone.body.layer4.0.bn2.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn3.bias                  loaded from backbone.body.layer4.0.bn3.bias                  of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn3.running_mean          loaded from backbone.body.layer4.0.bn3.running_mean          of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn3.running_var           loaded from backbone.body.layer4.0.bn3.running_var           of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.bn3.weight                loaded from backbone.body.layer4.0.bn3.weight                of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.conv1.weight              loaded from backbone.body.layer4.0.conv1.weight              of shape (512, 1024, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.conv2.weight              loaded from backbone.body.layer4.0.conv2.weight              of shape (512, 512, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.conv3.weight              loaded from backbone.body.layer4.0.conv3.weight              of shape (2048, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.downsample.0.weight       loaded from backbone.body.layer4.0.downsample.0.weight       of shape (2048, 1024, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.downsample.1.bias         loaded from backbone.body.layer4.0.downsample.1.bias         of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.downsample.1.running_mean loaded from backbone.body.layer4.0.downsample.1.running_mean of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.downsample.1.running_var  loaded from backbone.body.layer4.0.downsample.1.running_var  of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.0.downsample.1.weight       loaded from backbone.body.layer4.0.downsample.1.weight       of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn1.bias                  loaded from backbone.body.layer4.1.bn1.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn1.running_mean          loaded from backbone.body.layer4.1.bn1.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn1.running_var           loaded from backbone.body.layer4.1.bn1.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn1.weight                loaded from backbone.body.layer4.1.bn1.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn2.bias                  loaded from backbone.body.layer4.1.bn2.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn2.running_mean          loaded from backbone.body.layer4.1.bn2.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn2.running_var           loaded from backbone.body.layer4.1.bn2.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn2.weight                loaded from backbone.body.layer4.1.bn2.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn3.bias                  loaded from backbone.body.layer4.1.bn3.bias                  of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn3.running_mean          loaded from backbone.body.layer4.1.bn3.running_mean          of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn3.running_var           loaded from backbone.body.layer4.1.bn3.running_var           of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.bn3.weight                loaded from backbone.body.layer4.1.bn3.weight                of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.conv1.weight              loaded from backbone.body.layer4.1.conv1.weight              of shape (512, 2048, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.conv2.weight              loaded from backbone.body.layer4.1.conv2.weight              of shape (512, 512, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.1.conv3.weight              loaded from backbone.body.layer4.1.conv3.weight              of shape (2048, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn1.bias                  loaded from backbone.body.layer4.2.bn1.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn1.running_mean          loaded from backbone.body.layer4.2.bn1.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn1.running_var           loaded from backbone.body.layer4.2.bn1.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn1.weight                loaded from backbone.body.layer4.2.bn1.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn2.bias                  loaded from backbone.body.layer4.2.bn2.bias                  of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn2.running_mean          loaded from backbone.body.layer4.2.bn2.running_mean          of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn2.running_var           loaded from backbone.body.layer4.2.bn2.running_var           of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn2.weight                loaded from backbone.body.layer4.2.bn2.weight                of shape (512,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn3.bias                  loaded from backbone.body.layer4.2.bn3.bias                  of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn3.running_mean          loaded from backbone.body.layer4.2.bn3.running_mean          of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn3.running_var           loaded from backbone.body.layer4.2.bn3.running_var           of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.bn3.weight                loaded from backbone.body.layer4.2.bn3.weight                of shape (2048,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.conv1.weight              loaded from backbone.body.layer4.2.conv1.weight              of shape (512, 2048, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.conv2.weight              loaded from backbone.body.layer4.2.conv2.weight              of shape (512, 512, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.layer4.2.conv3.weight              loaded from backbone.body.layer4.2.conv3.weight              of shape (2048, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.stem.bn1.bias                      loaded from backbone.body.stem.bn1.bias                      of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.stem.bn1.running_mean              loaded from backbone.body.stem.bn1.running_mean              of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.stem.bn1.running_var               loaded from backbone.body.stem.bn1.running_var               of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.stem.bn1.weight                    loaded from backbone.body.stem.bn1.weight                    of shape (64,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.body.stem.conv1.weight                  loaded from backbone.body.stem.conv1.weight                  of shape (64, 3, 7, 7)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_inner2.bias                     loaded from backbone.fpn.fpn_inner2.bias                     of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_inner2.weight                   loaded from backbone.fpn.fpn_inner2.weight                   of shape (1024, 512, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_inner3.bias                     loaded from backbone.fpn.fpn_inner3.bias                     of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_inner3.weight                   loaded from backbone.fpn.fpn_inner3.weight                   of shape (1024, 1024, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_inner4.bias                     loaded from backbone.fpn.fpn_inner4.bias                     of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_inner4.weight                   loaded from backbone.fpn.fpn_inner4.weight                   of shape (1024, 2048, 1, 1)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_layer2.bias                     loaded from backbone.fpn.fpn_layer2.bias                     of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_layer2.weight                   loaded from backbone.fpn.fpn_layer2.weight                   of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_layer3.bias                     loaded from backbone.fpn.fpn_layer3.bias                     of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_layer3.weight                   loaded from backbone.fpn.fpn_layer3.weight                   of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_layer4.bias                     loaded from backbone.fpn.fpn_layer4.bias                     of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.fpn_layer4.weight                   loaded from backbone.fpn.fpn_layer4.weight                   of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.top_blocks.p6.bias                  loaded from backbone.fpn.top_blocks.p6.bias                  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.top_blocks.p6.weight                loaded from backbone.fpn.top_blocks.p6.weight                of shape (1024, 2048, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.top_blocks.p7.bias                  loaded from backbone.fpn.top_blocks.p7.bias                  of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:backbone.fpn.top_blocks.p7.weight                loaded from backbone.fpn.top_blocks.p7.weight                of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.anchor_generator.cell_anchors.0              loaded from rpn.anchor_generator.cell_anchors.0              of shape (9, 4)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.anchor_generator.cell_anchors.1              loaded from rpn.anchor_generator.cell_anchors.1              of shape (9, 4)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.anchor_generator.cell_anchors.2              loaded from rpn.anchor_generator.cell_anchors.2              of shape (9, 4)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.anchor_generator.cell_anchors.3              loaded from rpn.anchor_generator.cell_anchors.3              of shape (9, 4)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.anchor_generator.cell_anchors.4              loaded from rpn.anchor_generator.cell_anchors.4              of shape (9, 4)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_pred.bias                          loaded from rpn.head.bbox_pred.bias                          of shape (36,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_pred.weight                        loaded from rpn.head.bbox_pred.weight                        of shape (36, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_tower.0.bias                       loaded from rpn.head.bbox_tower.0.bias                       of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_tower.0.weight                     loaded from rpn.head.bbox_tower.0.weight                     of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_tower.2.bias                       loaded from rpn.head.bbox_tower.2.bias                       of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_tower.2.weight                     loaded from rpn.head.bbox_tower.2.weight                     of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_tower.4.bias                       loaded from rpn.head.bbox_tower.4.bias                       of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_tower.4.weight                     loaded from rpn.head.bbox_tower.4.weight                     of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_tower.6.bias                       loaded from rpn.head.bbox_tower.6.bias                       of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.bbox_tower.6.weight                     loaded from rpn.head.bbox_tower.6.weight                     of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_logits.bias                         loaded from rpn.head.cls_logits.bias                         of shape (9,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_logits.weight                       loaded from rpn.head.cls_logits.weight                       of shape (9, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_tower.0.bias                        loaded from rpn.head.cls_tower.0.bias                        of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_tower.0.weight                      loaded from rpn.head.cls_tower.0.weight                      of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_tower.2.bias                        loaded from rpn.head.cls_tower.2.bias                        of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_tower.2.weight                      loaded from rpn.head.cls_tower.2.weight                      of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_tower.4.bias                        loaded from rpn.head.cls_tower.4.bias                        of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_tower.4.weight                      loaded from rpn.head.cls_tower.4.weight                      of shape (1024, 1024, 3, 3)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_tower.6.bias                        loaded from rpn.head.cls_tower.6.bias                        of shape (1024,)
INFO:maskrcnn_benchmark.utils.model_serialization:rpn.head.cls_tower.6.weight                      loaded from rpn.head.cls_tower.6.weight                      of shape (1024, 1024, 3, 3)
  } _ = checkpointer.load(weight) // RETURNED
  # -----------------------------------------
  # 1.5 Build Transfroms
  # -----------------------------------------
  self.transforms = build_transforms(self.cfg, self.is_recognition) // CALL

    Compose.__init__(self, transforms { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/data/transforms/transforms.py

      // Params:
        transforms: [<maskrcnn_benchmark.data.transforms.transforms.Resize object at 0x7f5ca474c080>, <maskrcnn_benchmark.data.transforms.transforms.ToTensor object at 0x7f5ca474c128>, <maskrcnn_benchmark.data.transforms.transforms.Normalize object at 0x7f5ca474c0b8>]

      self.transforms = transforms
      // self.transforms: [<maskrcnn_benchmark.data.transforms.transforms.Resize object at 0x7f5ca474c080>, <maskrcnn_benchmark.data.transforms.transforms.ToTensor object at 0x7f5ca474c128>, <maskrcnn_benchmark.data.transforms.transforms.Normalize object at 0x7f5ca474c0b8>]

    Compose.__init__(self, transforms } // END
  self.transforms = build_transforms(self.cfg, self.is_recognition) // RETURNED
  // self.cfg.TEST.SCORE_THRESHOLD:0.3
  self.score_thresh = self.cfg.TEST.SCORE_THRESHOLD
} // END DetectionDemo.__init__



# =======================================
# II. Model Forward with test image
# =======================================


compute_prediction(self, image) { // BEGIN
  // defined in detection_model_debug.py


  // Params:
    > image.width: 512
    > image.height: 438

  # ==================================
  # 2-1 Transformer to input image
  # ==================================
  image_tensor = self.transforms(image) // CALL
  {

    Compose.__call__(self, image) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/data/transforms/transforms.py

      // Params:
        type(image): <class 'PIL.Image.Image'>
        image.height: 438
        image.width: 512

      for t in self.transforms:
        image = <maskrcnn_benchmark.data.transforms.transforms.Resize object at 0x7f5ca474c080>(image)
        type(image): <class 'PIL.Image.Image'>
        image.height: 480
        image.width: 561
        image = <maskrcnn_benchmark.data.transforms.transforms.ToTensor object at 0x7f5ca474c128>(image)
        type(image): <class 'torch.Tensor'>
        image.size(): torch.Size([3, 480, 561])
        image = <maskrcnn_benchmark.data.transforms.transforms.Normalize object at 0x7f5ca474c0b8>(image)
        type(image): <class 'torch.Tensor'>
        image.size(): torch.Size([3, 480, 561])

      return image

    } // END Compose.__call__()

  } image_tensor = self.transforms(image) // RETURNED
  // image_tensor.shape: torch.Size([3, 480, 561])

  # ==================================
  # 2-2 Zero padding and Batched Input
  # ==================================

  // padding images for 32 divisible size on width and height
  // self.cfg.DATALOADER.SIZE_DIVISIBILITY: 32
  // self.device: cuda
  image_list = to_image_list(image_tensor, self.cfg.DATALOADER.SIZE_DIVISIBILITY).to(self.device) // CALL
  {

  to_image_list(tensors, size_divisible=0) { // BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/image_list.py

    // Params:
      > type(tensors): <class 'torch.Tensor'>
      > size_divisible: 32

    if isinstance(tensors, torch.Tensor) and size_divisible > 0:
      tensors = [tensors]
      // len(tensors]: 1
      // tensors[0].shape: torch.Size([3, 480, 561])

    elif isinstance(tensors, (tuple, list)):
      max_size = tuple(max(s) for s in zip(*[img.shape for img in tensors]))
      // max_size: (3, 480, 561)

      if size_divisible > 0:
        import math

        stride = size_divisible
        // stride: 32

        max_size = list(max_size)
        // max_size: [3, 480, 561]

        max_size[1] = int(math.ceil(max_size[1] / stride) * stride)
        // max_size[1]: 480

        max_size[2] = int(math.ceil(max_size[2] / stride) * stride)
        // max_size[2]: 576

        max_size = tuple(max_size)
        max_size: (3, 480, 576)

      batch_shape = (len(tensors),) + max_size
      batch_shape: (1, 3, 480, 576)

      # make batch_imgs by adding axis with all pixel values is zero
      batched_imgs = tensors[0].new(*batch_shape).zero_()
      batched_imgs.shape: torch.Size([1, 3, 480, 576])

      # overlay tensors on batch_imgs (pad)img
      for img, pad_img in zip(tensors, batched_imgs):
        pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)

      image_sizes = [im.shape[-2:] for im in tensors]
      // image_sizes: [torch.Size([480, 561])]

      // type(batched_imgs): <class 'torch.Tensor'>

      // batched_imgs.shape: torch.Size([1, 3, 480, 576])
      // image_sizes: [torch.Size([480, 561])]

      return ImageList(batched_imgs, image_sizes) // CALL

  ImageList.__init__(self, tensors, image_sizes) { // BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/image_list.py

    // Params:
      > tensors.shape: torch.Size([1, 3, 480, 576])
      > image_sizes: [torch.Size([480, 561])]

    self.tensors = tensors
    self.image_sizes = image_sizes
  } // END ImageList.__init__(self, tensors, image_sizes)


  ImageList.to(self, *args, **kwargs) { // BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/image_list.py

    // Params:
      args: (device(type='cuda'),)
      kwargs: {}

    cast_tensor = self.tensors.to(*args, **kwargs)
    // cast_tensor: cast_tensor
  } // END ImageList.to(self, *args, **kwargs)


  ImageList.__init__(self, tensors, image_sizes) { // BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/image_list.py

    // Params:
      > tensors.shape: torch.Size([1, 3, 480, 576])
      > image_sizes: [torch.Size([480, 561])]

    self.tensors = tensors
    self.image_sizes = image_sizes
  } // END ImageList.__init__(self, tensors, image_sizes)


  } // END to_image_list(tensors, size_divisible=0)

  image_list = to_image_list(image_tensor, self.cfg.DATALOADER.SIZE_DIVISIBILITY).to(self.device) // RETURNED
  // image_list.image_sizes: [torch.Size([480, 561])]
  // image_list.tensors.shape: torch.Size([1, 3, 480, 576])



  # ==============================
  # 2-3 Inference with input image
  # ==============================
  with torch.no_grad():
    pred = self.model(image_list) // CALL


  GeneralizedRCNN.forward(self, images, targets=None) { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py

    // Params:
      > images:
      > type(images): <class 'maskrcnn_benchmark.structures.image_list.ImageList'>
      > targets: None

  if self.training: False
  images = to_image_list(images) // CALL
  {

  to_image_list(tensors, size_divisible=0) { // BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/image_list.py

    // Params:
      > type(tensors): <class 'maskrcnn_benchmark.structures.image_list.ImageList'>
      > size_divisible: 0

    if isinstance(tensors, ImageList):
      return tensors


  } // END to_image_list(tensors, size_divisible=0)

  }
  images = to_image_list(images) // RETURNED

  images.image_sizes: [torch.Size([480, 561])]
  images.tensors.shape: torch.Size([1, 3, 480, 576])
  # ===========================================
  # 2-3-1 Backbone Forward
  # ===========================================
  model.backbone.forward(images.tensors) // CALL
  {



    # =================================
    # 2-3-1-1 ResNet Forward
    # =================================

  Resnet.forward(self, x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Param
      x.shape=torch.Size([1, 3, 480, 576])

    # =================================
    # 2-3-1-1-1 stem (layer0) forward
    # =================================


    x = self.stem(x) { // CALL


  BaseStem.forward(self, x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    // Params:
      > x.shape: torch.Size([1, 3, 480, 576])

    x = self.conv1(x)
    // self.conv1: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    // x.shape: torch.Size([1, 64, 240, 288])

    x = self.bn1(x)
    // self.bn1: FrozenBatchNorm2d()
    // x.shape: torch.Size([1, 64, 240, 288])

    x = F.relu_(x)
    // x.shape: torch.Size([1, 64, 240, 288])

    x = F.max_pool2d(x, kernel_size=3, stride=2, padding=1)
    // x.shape: torch.Size([1, 64, 120, 144])

  return x


  } // END BaseStem.forward()

    }

    x = self.stem(x) // RETURNED
    // x.shape: torch.Size([1, 64, 120, 144])

    stem output of shape (1, 64, 120, 144) saved into ./npy_save/stem_output.npy


    for stage_name in self.stages:
    {
      # =================================
      # 2-3-1-1-1 forward
      # =================================
      { // BEGIN of iteration for 1 for layer1

      x = getattr(self, stage_name)(x) { // CALL


  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 64, 120, 144])

      self : BottleneckWithFixedBatchNorm(
  (downsample): Sequential(
    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): FrozenBatchNorm2d()
  )
  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 64, 120, 144])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 64, 120, 144])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 256, 120, 144])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 256, 120, 144])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: Sequential(
  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (1): FrozenBatchNorm2d()
)
    if self.downsample is not None:
      identity = self.downsample(x)
      // identity.shape: torch.Size([1, 256, 120, 144])

    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 256, 120, 144])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 120, 144])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 256, 120, 144])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 64, 120, 144])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 64, 120, 144])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 256, 120, 144])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 256, 120, 144])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 256, 120, 144])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 120, 144])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 256, 120, 144])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 64, 120, 144])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 64, 120, 144])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 64, 120, 144])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 256, 120, 144])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 256, 120, 144])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 256, 120, 144])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 120, 144])

    return out


  } // END Bottelneck.__forward__()

      } // x = getattr(self, stage_name)(x) RETURNED

        // output shape of layer1: torch.Size([1, 256, 120, 144])

      # Save all the calculation results of stage 1 ~ 4 (that is, the feature map) in the form of a list
      if self.return_features[stage_name]:
        outputs.append(x)
        // stage_name: layer1
        // x.shape: torch.Size([1, 256, 120, 144])

        #layer1 output of shape (1, 256, 120, 144) saved into ./npy_save/C1.npy


      } // END of iteration for layer1

      # =================================
      # 2-3-1-1-2 forward
      # =================================
      { // BEGIN of iteration for 2 for layer2

      x = getattr(self, stage_name)(x) { // CALL


  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 256, 120, 144])

      self : BottleneckWithFixedBatchNorm(
  (downsample): Sequential(
    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): FrozenBatchNorm2d()
  )
  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 128, 60, 72])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 128, 60, 72])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 512, 60, 72])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 512, 60, 72])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: Sequential(
  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): FrozenBatchNorm2d()
)
    if self.downsample is not None:
      identity = self.downsample(x)
      // identity.shape: torch.Size([1, 512, 60, 72])

    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 512, 60, 72])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 60, 72])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 512, 60, 72])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 128, 60, 72])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 128, 60, 72])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 512, 60, 72])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 512, 60, 72])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 512, 60, 72])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 60, 72])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 512, 60, 72])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 128, 60, 72])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 128, 60, 72])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 512, 60, 72])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 512, 60, 72])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 512, 60, 72])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 60, 72])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 512, 60, 72])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 128, 60, 72])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 128, 60, 72])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 128, 60, 72])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 512, 60, 72])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 512, 60, 72])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 512, 60, 72])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 60, 72])

    return out


  } // END Bottelneck.__forward__()

      } // x = getattr(self, stage_name)(x) RETURNED

        // output shape of layer2: torch.Size([1, 512, 60, 72])

      # Save all the calculation results of stage 1 ~ 4 (that is, the feature map) in the form of a list
      if self.return_features[stage_name]:
        outputs.append(x)
        // stage_name: layer2
        // x.shape: torch.Size([1, 512, 60, 72])

        #layer2 output of shape (1, 512, 60, 72) saved into ./npy_save/C2.npy


      } // END of iteration for layer2

      # =================================
      # 2-3-1-1-3 forward
      # =================================
      { // BEGIN of iteration for 3 for layer3

      x = getattr(self, stage_name)(x) { // CALL


  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 512, 60, 72])

      self : BottleneckWithFixedBatchNorm(
  (downsample): Sequential(
    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): FrozenBatchNorm2d()
  )
  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 1024, 30, 36])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: Sequential(
  (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): FrozenBatchNorm2d()
)
    if self.downsample is not None:
      identity = self.downsample(x)
      // identity.shape: torch.Size([1, 1024, 30, 36])

    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 1024, 30, 36])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 1024, 30, 36])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 1024, 30, 36])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 1024, 30, 36])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 1024, 30, 36])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 1024, 30, 36])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 1024, 30, 36])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 1024, 30, 36])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 1024, 30, 36])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 1024, 30, 36])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 1024, 30, 36])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 1024, 30, 36])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 1024, 30, 36])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 1024, 30, 36])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 256, 30, 36])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 256, 30, 36])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 1024, 30, 36])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 1024, 30, 36])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 1024, 30, 36])

    return out


  } // END Bottelneck.__forward__()

      } // x = getattr(self, stage_name)(x) RETURNED

        // output shape of layer3: torch.Size([1, 1024, 30, 36])

      # Save all the calculation results of stage 1 ~ 4 (that is, the feature map) in the form of a list
      if self.return_features[stage_name]:
        outputs.append(x)
        // stage_name: layer3
        // x.shape: torch.Size([1, 1024, 30, 36])

        #layer3 output of shape (1, 1024, 30, 36) saved into ./npy_save/C3.npy


      } // END of iteration for layer3

      # =================================
      # 2-3-1-1-4 forward
      # =================================
      { // BEGIN of iteration for 4 for layer4

      x = getattr(self, stage_name)(x) { // CALL


  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 1024, 30, 36])

      self : BottleneckWithFixedBatchNorm(
  (downsample): Sequential(
    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
    (1): FrozenBatchNorm2d()
  )
  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 15, 18])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 15, 18])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 2048, 15, 18])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 2048, 15, 18])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: Sequential(
  (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
  (1): FrozenBatchNorm2d()
)
    if self.downsample is not None:
      identity = self.downsample(x)
      // identity.shape: torch.Size([1, 2048, 15, 18])

    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 2048, 15, 18])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 2048, 15, 18])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 2048, 15, 18])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 15, 18])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 15, 18])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 2048, 15, 18])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 2048, 15, 18])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 2048, 15, 18])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 2048, 15, 18])

    return out


  } // END Bottelneck.__forward__()

  Bottleneck.__forward__(self.x) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/resnet.py

    Params:
      x.shape : torch.Size([1, 2048, 15, 18])

      self : BottleneckWithFixedBatchNorm(
  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn1): FrozenBatchNorm2d()
  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): FrozenBatchNorm2d()
  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn3): FrozenBatchNorm2d()
)

    // # Identity connection, directly make the residual equal to x
    identity = x

    // # conv1, bn1, relu (in place relu)
    // self.conv1: Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn1: FrozenBatchNorm2d()

    out = self.conv1(x)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = self.bn1(out)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 15, 18])


    // # conv2, bn2, relu (in place relu)
    // self.conv2: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    // self.bn2: FrozenBatchNorm2d()

    out = self.conv2(x)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = self.bn2(out)
    // out.shape: torch.Size([1, 512, 15, 18])

    out = F.relu_(out)
    // out.shape: torch.Size([1, 512, 15, 18])


    // # conv3, bn3, not relu here
    // self.conv3: Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
    // self.bn3: FrozenBatchNorm2d()

    out = self.conv3(x)
    // out.shape: torch.Size([1, 2048, 15, 18])

    out = self.bn3(out)
    // out.shape: torch.Size([1, 2048, 15, 18])


    // # If the number of input and output channels are different,
    // # they need to be mapped to make them the same.
    // self.downsample: None
    out += identity   # H = F + x in paper
    // out.shape: torch.Size([1, 2048, 15, 18])


    // # the third (final) relu ( in place relu)
    out = F.relu_(out)
    // out.shape: torch.Size([1, 2048, 15, 18])

    return out


  } // END Bottelneck.__forward__()

      } // x = getattr(self, stage_name)(x) RETURNED

        // output shape of layer4: torch.Size([1, 2048, 15, 18])

      # Save all the calculation results of stage 1 ~ 4 (that is, the feature map) in the form of a list
      if self.return_features[stage_name]:
        outputs.append(x)
        // stage_name: layer4
        // x.shape: torch.Size([1, 2048, 15, 18])

        #layer4 output of shape (1, 2048, 15, 18) saved into ./npy_save/C4.npy


      } // END of iteration for layer4


    } // END for stage_name in self.stages

      # -------------------------------
      # return value (outputs) info
      # fed into FPN.forward()
      # -------------------------------
      output of layer1 is C1 of shape: torch.Size([1, 256, 120, 144])
      output of layer2 is C2 of shape: torch.Size([1, 512, 60, 72])
      output of layer3 is C3 of shape: torch.Size([1, 1024, 30, 36])
      output of layer4 is C4 of shape: torch.Size([1, 2048, 15, 18])

    return outputs

  } // END Resnet.forward(self, x)



    # =================================
    # 2-3-1-2 FPN Forward
    # =================================

  FPN.forward(self,x) { // BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/fpn.py

    // Param: x  = [C1, C2, C3, C4], return of Resnet.forward()
      // C[1] of shape : torch.Size([1, 256, 120, 144])
      // C[2] of shape : torch.Size([1, 512, 60, 72])
      // C[3] of shape : torch.Size([1, 1024, 30, 36])
      // C[4] of shape : torch.Size([1, 2048, 15, 18])


      # ===========================================================================
      # FPN block info
      # self.inner_blocks: ['fpn_inner2', 'fpn_inner3', 'fpn_inner4'])
      # self.layer_blocks: ['fpn_layer2', 'fpn_layer3', 'fpn_layer4'])
      # ===========================================================================

      # last_inner = fpn_inner4(C4)

      # self.innerblocks[-1]:fpn_inner4
      # getattr(self, self.innerblocks[-1]):Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))
      // x[-1].shape = torch.Size([1, 2048, 15, 18])

      last_inner = getattr(self, self.inner_blocks[-1])(x[-1])
      // last_inner.shape:torch.Size([1, 1024, 15, 18])

      # fpn_inner4 output of shape (1, 1024, 15, 18) saved into ./npy_save/fpn_inner4_output.npy


      results = []
      # self.layer_blocks[-1]: fpn_layer4
      # getattr(self, self.layer_blocks[-1]): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      // last_inner.shape: torch.Size([1, 1024, 15, 18])]


    results.append(self.layer_blocks[-1](last_inner))
      # results.append() : P4
    // results[-1].shape: torch.Size([1, 1024, 15, 18]) <=== P4

      # fpn_layer4 output (P4) of shape (1, 1024, 15, 18) saved into ./npy_save/fpn_layer4_output.npy


      for feature, inner_block, layer_block in zip(
        [(x[:-1][::-1], self.inner_blocks[:-1][::-1], self.layer_blocks[:-1][::-1]): {

        { // BEGIN iteratrion for calc P3

        # ====================================
        # for calc P3
        # feature.shape: torch.Size([1, 1024, 30, 36])
        # inner_block: fpn_inner3 ==> Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        # layer_block: fpn_layer3 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        # last_inner.shape: torch.Size([1, 1024, 15, 18])
        # ====================================

        eltwise_suffix = inner_block[-1]
        // eltwise_suffix: 3
        # --------------------------------------------------
        # for calc P3
        # 1. Upsample : replace with Decovolution in caffe
        # layer name in caffe: fpn_inner3_upsample = Deconvolution(last_inner)
        # --------------------------------------------------
        // last_inner.shape: torch.Size([1, 1024, 15, 18])
        inner_top_down = F.interpolate(last_inner, scale_factor=2, mode='nearest')
        // inner_top_down.shape : torch.Size([1, 1024, 30, 36])

        # inner_top_down of shape (1, 1024, 30, 36) saved into ./npy_save/inner_top_down_for_fpn_inner3.npy


        --------------------------------------------------
        # for calc P3
        # 2. inner_lateral = Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
        # layer name in caffe: fpn_inner3_lateral=Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
        # --------------------------------------------------
        // inner_block: fpn_inner3 ==> Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))
        // feature.shape: torch.Size([1, 1024, 30, 36])
        inner_lateral = getattr(self, inner_block)(feature)
        // inner_lateral.shape: torch.Size([1, 1024, 30, 36])

        # fpn_inner3 output of shape (1, 1024, 30, 36) saved into ./npy_save/fpn_inner3_output.npy


        # --------------------------------------------------
        # for calc P3
        # 3. Elementwise Addition: replaced with eltwise in caffe
        # layer in caffe: eltwise_3 = eltwise(fpn_inner3_lateral, fpn_inner3_upsample )
        # --------------------------------------------------
        // inner_lateral.shape: torch.Size([1, 1024, 30, 36])
        // inner_top_down.shape: torch.Size([1, 1024, 30, 36])
        last_inner = inner_lateral + inner_top_down
        // last_inner.shape : torch.Size([1, 1024, 30, 36])
        # superimposing result of fpn_inner3 output plus inner topdown of shape (1, 1024, 30, 36) saved into ./npy_save/fpn_inner3_ouptut_plus_inner_topdown.npy


        # --------------------------------------------------
        # for calc P3
        # 4. results.insert(0, Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(last_inner)
        # layer in caffe: fpn_layer3 = Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(eltwise_3)
        # --------------------------------------------------
        // layer_block: fpn_layer3 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // input: last_inner.shape = torch.Size([1, 1024, 30, 36])
        results.insert(0, getattr(self, layer_block)(last_inner))
        // results[0].shape: torch.Size([1, 1024, 30, 36])

        # fpn_layer3 output (P3) of shape (1, 1024, 30, 36) saved into ./npy_save/fpn_layer3_ouptut.npy


        # --------------------------------------------------
        # results after iteration 0
        # --------------------------------------------------
        # results[0] of shape: torch.Size([1, 1024, 30, 36])
        # results[1] of shape: torch.Size([1, 1024, 15, 18])
        #--------------------------------------------------

        } // END iteratrion for calc P3

        { // BEGIN iteratrion for calc P2

        # ====================================
        # for calc P2
        # feature.shape: torch.Size([1, 512, 60, 72])
        # inner_block: fpn_inner2 ==> Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        # layer_block: fpn_layer2 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        # last_inner.shape: torch.Size([1, 1024, 30, 36])
        # ====================================

        eltwise_suffix = inner_block[-1]
        // eltwise_suffix: 2
        # --------------------------------------------------
        # for calc P2
        # 1. Upsample : replace with Decovolution in caffe
        # layer name in caffe: fpn_inner2_upsample = Deconvolution(last_inner)
        # --------------------------------------------------
        // last_inner.shape: torch.Size([1, 1024, 30, 36])
        inner_top_down = F.interpolate(last_inner, scale_factor=2, mode='nearest')
        // inner_top_down.shape : torch.Size([1, 1024, 60, 72])

        # inner_top_down of shape (1, 1024, 60, 72) saved into ./npy_save/inner_top_down_for_fpn_inner2.npy


        --------------------------------------------------
        # for calc P2
        # 2. inner_lateral = Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
        # layer name in caffe: fpn_inner2_lateral=Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))(feature)
        # --------------------------------------------------
        // inner_block: fpn_inner2 ==> Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1))
        // feature.shape: torch.Size([1, 512, 60, 72])
        inner_lateral = getattr(self, inner_block)(feature)
        // inner_lateral.shape: torch.Size([1, 1024, 60, 72])

        # fpn_inner2 output of shape (1, 1024, 60, 72) saved into ./npy_save/fpn_inner2_output.npy


        # --------------------------------------------------
        # for calc P2
        # 3. Elementwise Addition: replaced with eltwise in caffe
        # layer in caffe: eltwise_2 = eltwise(fpn_inner2_lateral, fpn_inner2_upsample )
        # --------------------------------------------------
        // inner_lateral.shape: torch.Size([1, 1024, 60, 72])
        // inner_top_down.shape: torch.Size([1, 1024, 60, 72])
        last_inner = inner_lateral + inner_top_down
        // last_inner.shape : torch.Size([1, 1024, 60, 72])
        # superimposing result of fpn_inner2 output plus inner topdown of shape (1, 1024, 60, 72) saved into ./npy_save/fpn_inner2_ouptut_plus_inner_topdown.npy


        # --------------------------------------------------
        # for calc P2
        # 4. results.insert(0, Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(last_inner)
        # layer in caffe: fpn_layer2 = Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(eltwise_2)
        # --------------------------------------------------
        // layer_block: fpn_layer2 ==> Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // input: last_inner.shape = torch.Size([1, 1024, 60, 72])
        results.insert(0, getattr(self, layer_block)(last_inner))
        // results[0].shape: torch.Size([1, 1024, 60, 72])

        # fpn_layer2 output (P2) of shape (1, 1024, 60, 72) saved into ./npy_save/fpn_layer2_ouptut.npy


        # --------------------------------------------------
        # results after iteration 1
        # --------------------------------------------------
        # results[0] of shape: torch.Size([1, 1024, 60, 72])
        # results[1] of shape: torch.Size([1, 1024, 30, 36])
        # results[2] of shape: torch.Size([1, 1024, 15, 18])
        #--------------------------------------------------

        } // END iteratrion for calc P2

      } // for loop END

      # --------------------------------------------------
      # results after for loop
      # --------------------------------------------------
        # results[0] AKA P2 of shape: torch.Size([1, 1024, 60, 72])
        # results[1] AKA P3 of shape: torch.Size([1, 1024, 30, 36])
        # results[2] AKA P4 of shape: torch.Size([1, 1024, 15, 18])



      if isinstance(self.top_blocks, LastLevelP6P7):
        // self.top_blocks: LastLevelP6P7(
  (p6): Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
  (p7): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
)
        // len(x): 4
          // x[0].shape : torch.Size([1, 256, 120, 144]) ==> C1
          // x[1].shape : torch.Size([1, 512, 60, 72]) ==> C2
          // x[2].shape : torch.Size([1, 1024, 30, 36]) ==> C3
          // x[3].shape : torch.Size([1, 2048, 15, 18]) ==> C4
          //x[-1] AKA C4 of shape : torch.Size([1, 2048, 15, 18])


        // len(results): 3
          // results[0].shape : torch.Size([1, 1024, 60, 72]) ==> P2
          // results[1].shape : torch.Size([1, 1024, 30, 36]) ==> P3
          // results[2].shape : torch.Size([1, 1024, 15, 18]) ==> P4

        // results[-1] AKA P4 of shape: torch.Size([1, 1024, 15, 18])


        last_result = self.top_blocks(x[-1]==>C4, results[-1]==>P4) { // CALL
    # =================================
    # 2-3-1-3 FPN.LastLevelP6P7 Forward
    # =================================


      LastLevelP6P7.forward(self, c5, p5) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/backbone/fpn.py

        //Param:
          //c5.shape: torch.Size([1, 2048, 15, 18])
          //p5.shape: torch.Size([1, 1024, 15, 18])

        // self.use_P5: False
        x = p5 if self.use_P5 else c5
        x=c5

        // x.shape = torch.Size([1, 2048, 15, 18])

        // self.p6: Conv2d(2048, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        // x.shape: torch.Size([1, 2048, 15, 18])
        p6 = self.p6(x)
        // p6.shape: torch.Size([1, 1024, 8, 9])

        # LastLevelP6P7::forward(), P6 of shape (1, 1024, 8, 9) saved into ./npy_save/P6.npy


        // self.p7: Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        p7 = self.p7(F.relu(p6))
        // p7.shape: torch.Size([1, 1024, 4, 5])

      # LastLevelP6P7::forward(), P7 of shape (1, 1024, 4, 5) saved into ./npy_save/P7.npy


        # ------------------
        # return value (P6, P7) info
        # which is appended into FPN results
        # ------------------
        P6 of shape torch.Size([1, 1024, 8, 9])
        P7 of shape torch.Size([1, 1024, 4, 5])

        returns [p6, p7]

      } // END LastLevelP6P7.forward(self, c5, p5)


        }
        last_result = self.top_blocks(x[-1]==>C4, results[-1]==>P4) // RETURNED

        // len(last_results):2
        //last_results[0] AKA P6shape : torch.Size([1, 1024, 8, 9])
        //last_results[1] AKA P7shape : torch.Size([1, 1024, 4, 5])

        results.extend(last_results)
        // len(results): 5
          results[0].shape : torch.Size([1, 1024, 60, 72])
          results[1].shape : torch.Size([1, 1024, 30, 36])
          results[2].shape : torch.Size([1, 1024, 15, 18])
          results[3].shape : torch.Size([1, 1024, 8, 9])
          results[4].shape : torch.Size([1, 1024, 4, 5])




      #-----------------------------------
      # return value tuple(results: P2, P3, P4, P6, P7) info
      # which fed into RPN.forward()
      #-----------------------------------
      results[0] = P2 of shape: torch.Size([1, 1024, 60, 72])
      results[1] = P3 of shape: torch.Size([1, 1024, 30, 36])
      results[2] = P4 of shape: torch.Size([1, 1024, 15, 18])
      results[3] = P6 of shape: torch.Size([1, 1024, 8, 9])
      results[4] = P7 of shape: torch.Size([1, 1024, 4, 5])

  return tuple(results)



  } // END FPN.forward(self,x)

  } model.backbone.forward(images.tensors) // RETURNED


  # ===========================================
  # 2-3-2 RPN Forward
  # ===========================================

  proposals, proposal_losses = self.rpn(images, features, targets) // CALL
  {
    RetinaNetModule.forward(self, images, features, targets=None) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py

      // Params:
        // type(images): <class 'maskrcnn_benchmark.structures.image_list.ImageList'>
        // len(images.image_sizes): 1
        // len(images.tensors): 1
        // len(features)): 5
        // target: None

        # images info
        // images.image_sizes[0]: torch.Size([480, 561]) # transformed image size (H, W)
        // images.tensors[0].shape: torch.Size([3, 480, 576]) # zero-padded batch image size (C, H, W)

        # features info
        // feature[0].shape: torch.Size([1, 1024, 60, 72]) <== P2 after FPN
        // feature[1].shape: torch.Size([1, 1024, 30, 36]) <== P3 after FPN
        // feature[2].shape: torch.Size([1, 1024, 15, 18]) <== P4 after FPN
        // feature[3].shape: torch.Size([1, 1024, 8, 9]) <== P6 after FPN
        // feature[4].shape: torch.Size([1, 1024, 4, 5]) <== P7 after FPN


    # ===========================================
    # 2-3-2-1 RPN.Head forward
    # ===========================================

    // type(self.head): <class 'maskrcnn_benchmark.modeling.rpn.retinanet.retinanet.RetinaNetHead'>
    box_cls, box_regression = self.head(features) // CALL
{


  RetinaNetHead.forward(self, x) { // BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py

    // Param:
      // self: RetinaNetHead(
  (cls_tower): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
  )
  (bbox_tower): Sequential(
    (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU()
    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU()
    (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): ReLU()
    (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU()
  )
  (cls_logits): Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bbox_pred): Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
      // len(x)): 5 # x is features retruned fron FPN forward

        # features info
        // feature[0].shape: torch.Size([1, 1024, 60, 72]) <== P2 after FPN
        // feature[1].shape: torch.Size([1, 1024, 30, 36]) <== P3 after FPN
        // feature[2].shape: torch.Size([1, 1024, 15, 18]) <== P4 after FPN
        // feature[3].shape: torch.Size([1, 1024, 8, 9]) <== P6 after FPN
        // feature[4].shape: torch.Size([1, 1024, 4, 5]) <== P7 after FPN


    logits = []
    bbox_reg = []

    #=========================================
    # for every P (total 5) from FPN,
    # - apply cls_tower and cls_logits
    # - apply bbox_tower and bbox_pred
    # hence 5 set of identical cls_tower, cls_logits, bbox_tower and bbox_pred
    # should be prepared because caffe don't have sub-net iteration structure
    #=========================================
    for idx, feature in enumerate(x) {
      {
      # BEGIN iteration: 1/5

      // ===================================
      // feature P2 of shape: torch.Size([1, 1024, 60, 72])
      // ===================================
      # 2-3-2-1-1. append cls_logits(cls_tower(feature))
      logits.append(self.cls_logits(self.cls_tower(feature)))
      logits.append(self.cls_logits(self.cls_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 60, 72])
        // self.cls_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.cls_logits: Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // logits[-1].shape: torch.Size([1, 9, 60, 72])
        // len(logits): 1

      # 2-3-2-1-2. append bbox_pred(bbox_tower(feature))
      bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 60, 72])
        // self.bbox_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.bbox_pred: Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // bbox_reg[-1].shape: torch.Size([1, 36, 60, 72])
        // len(bbox_reg): 1

      } // END iteration: 1/5

      {
      # BEGIN iteration: 2/5

      // ===================================
      // feature P3 of shape: torch.Size([1, 1024, 30, 36])
      // ===================================
      # 2-3-2-1-1. append cls_logits(cls_tower(feature))
      logits.append(self.cls_logits(self.cls_tower(feature)))
      logits.append(self.cls_logits(self.cls_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 30, 36])
        // self.cls_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.cls_logits: Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // logits[-1].shape: torch.Size([1, 9, 30, 36])
        // len(logits): 2

      # 2-3-2-1-2. append bbox_pred(bbox_tower(feature))
      bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 30, 36])
        // self.bbox_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.bbox_pred: Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // bbox_reg[-1].shape: torch.Size([1, 36, 30, 36])
        // len(bbox_reg): 2

      } // END iteration: 2/5

      {
      # BEGIN iteration: 3/5

      // ===================================
      // feature P4 of shape: torch.Size([1, 1024, 15, 18])
      // ===================================
      # 2-3-2-1-1. append cls_logits(cls_tower(feature))
      logits.append(self.cls_logits(self.cls_tower(feature)))
      logits.append(self.cls_logits(self.cls_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 15, 18])
        // self.cls_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.cls_logits: Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // logits[-1].shape: torch.Size([1, 9, 15, 18])
        // len(logits): 3

      # 2-3-2-1-2. append bbox_pred(bbox_tower(feature))
      bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 15, 18])
        // self.bbox_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.bbox_pred: Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // bbox_reg[-1].shape: torch.Size([1, 36, 15, 18])
        // len(bbox_reg): 3

      } // END iteration: 3/5

      {
      # BEGIN iteration: 4/5

      // ===================================
      // feature p6 of shape: torch.Size([1, 1024, 8, 9])
      // ===================================
      # 2-3-2-1-1. append cls_logits(cls_tower(feature))
      logits.append(self.cls_logits(self.cls_tower(feature)))
      logits.append(self.cls_logits(self.cls_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 8, 9])
        // self.cls_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.cls_logits: Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // logits[-1].shape: torch.Size([1, 9, 8, 9])
        // len(logits): 4

      # 2-3-2-1-2. append bbox_pred(bbox_tower(feature))
      bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 8, 9])
        // self.bbox_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.bbox_pred: Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // bbox_reg[-1].shape: torch.Size([1, 36, 8, 9])
        // len(bbox_reg): 4

      } // END iteration: 4/5

      {
      # BEGIN iteration: 5/5

      // ===================================
      // feature p7 of shape: torch.Size([1, 1024, 4, 5])
      // ===================================
      # 2-3-2-1-1. append cls_logits(cls_tower(feature))
      logits.append(self.cls_logits(self.cls_tower(feature)))
      logits.append(self.cls_logits(self.cls_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 4, 5])
        // self.cls_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.cls_logits: Conv2d(1024, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // logits[-1].shape: torch.Size([1, 9, 4, 5])
        // len(logits): 5

      # 2-3-2-1-2. append bbox_pred(bbox_tower(feature))
      bbox_reg.append(self.bbox_pred(self.bbox_tower(feature)))
        // feature.shape: torch.Size([1, 1024, 4, 5])
        // self.bbox_tower: Sequential(
  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (1): ReLU()
  (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (3): ReLU()
  (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (5): ReLU()
  (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (7): ReLU()
)
        // self.bbox_pred: Conv2d(1024, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        // bbox_reg[-1].shape: torch.Size([1, 36, 4, 5])
        // len(bbox_reg): 5

      } // END iteration: 5/5



    }// END for idx, feature n enumerate(x)

    // ==== logits ====
    // logits[0].shape: torch.Size([1, 9, 60, 72])
    // logits[1].shape: torch.Size([1, 9, 30, 36])
    // logits[2].shape: torch.Size([1, 9, 15, 18])
    // logits[3].shape: torch.Size([1, 9, 8, 9])
    // logits[4].shape: torch.Size([1, 9, 4, 5])

    // ==== bbox_reg ====
    // bbox_reg[0].shape: torch.Size([1, 36, 60, 72])
    // bbox_reg[1].shape: torch.Size([1, 36, 30, 36])
    // bbox_reg[2].shape: torch.Size([1, 36, 15, 18])
    // bbox_reg[3].shape: torch.Size([1, 36, 8, 9])
    // bbox_reg[4].shape: torch.Size([1, 36, 4, 5])

return logits, bbox_reg
  } // END RetinaNetHead.forward(self, x)

    }
box_cls, box_regression = self.head(features) // RETURNED
    // len(box_cls): 5
      // box_cls[0].shape: torch.Size([1, 9, 60, 72])
      // box_cls[1].shape: torch.Size([1, 9, 30, 36])
      // box_cls[2].shape: torch.Size([1, 9, 15, 18])
      // box_cls[3].shape: torch.Size([1, 9, 8, 9])
      // box_cls[4].shape: torch.Size([1, 9, 4, 5])
    // len(box_regression): 5
      // box_regression[0].shape: torch.Size([1, 36, 60, 72])
      // box_regression[1].shape: torch.Size([1, 36, 30, 36])
      // box_regression[2].shape: torch.Size([1, 36, 15, 18])
      // box_regression[3].shape: torch.Size([1, 36, 8, 9])
      // box_regression[4].shape: torch.Size([1, 36, 4, 5])



    # ===========================================
    # 2-3-2-2 RPN.anchor_generator forward
    # ===========================================

    // self.anchor_generator: AnchorGenerator(
  (cell_anchors): BufferList()
)
    anchors = self.anchor_generator(images, features) // CALL {

  AnchorGenerator.forward(image_list, feature_maps) { //BEGIN
    // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

    // Params:
      > image_list:
        > len(image_list.image_sizes): 1
        > image_list.image_sizes[0]: torch.Size([480, 561])
        > len(image_list.tensors): 1
        > image_list.tensors[0].shape: torch.Size([3, 480, 576])
      >feature_maps:
        feature_maps[0].shape: torch.Size([1, 1024, 60, 72]) <== P2
        feature_maps[1].shape: torch.Size([1, 1024, 30, 36]) <== P3
        feature_maps[2].shape: torch.Size([1, 1024, 15, 18]) <== P4
        feature_maps[3].shape: torch.Size([1, 1024, 8, 9]) <== P6
        feature_maps[4].shape: torch.Size([1, 1024, 4, 5]) <== P7


    grid_sizes = [feature_map.shape[-2:] for feature_map in feature_maps]
    // grid_sizes: [torch.Size([60, 72]), torch.Size([30, 36]), torch.Size([15, 18]), torch.Size([8, 9]), torch.Size([4, 5])]


    # -------------------------------------------------------------
    # 2-3-2-2-1
    # anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)
    # -------------------------------------------------------------
    anchors_over_all_feature_maps = self.grid_anchors(grid_sizes) // CALL
{
    AnchorGenerator.grid_anchors(grid_sizes) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

        Param:
        grid_sizes: [torch.Size([60, 72]), torch.Size([30, 36]), torch.Size([15, 18]), torch.Size([8, 9]), torch.Size([4, 5])]

      anchors = []
      // self.strides:(8, 16, 32, 64, 128)

      // self.cell_anchors[0].shape:torch.Size([9, 4])
      // self.cell_anchors[1].shape:torch.Size([9, 4])
      // self.cell_anchors[2].shape:torch.Size([9, 4])
      // self.cell_anchors[3].shape:torch.Size([9, 4])
      // self.cell_anchors[4].shape:torch.Size([9, 4])

      for size, stride, base_anchors in zip( grid_sizes, self.strides, self.cell_anchors ):
      {
        {
        # BEGIN iteration: 1/5

        #------------------------------------
        # size: torch.Size([60, 72]) from grid_sizes
        # size: 8 from self.strides
        # base_anchors.shape: torch.Size([9, 4]) from self.cell_anchors
        #------------------------------------
        grid_height, grid_width = size
        // grid_height: 60
        // grid_width: 72

        device = base_anchors.device
        // device: cuda:0

        shifts_x = torch.arange(
            0, grid_width * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_x: tensor([  0.,   8.,  16.,  24.,  32.,  40.,  48.,  56.,  64.,  72.,  80.,  88.,
         96., 104., 112., 120., 128., 136., 144., 152., 160., 168., 176., 184.,
        192., 200., 208., 216., 224., 232., 240., 248., 256., 264., 272., 280.,
        288., 296., 304., 312., 320., 328., 336., 344., 352., 360., 368., 376.,
        384., 392., 400., 408., 416., 424., 432., 440., 448., 456., 464., 472.,
        480., 488., 496., 504., 512., 520., 528., 536., 544., 552., 560., 568.])

        shifts_y = torch.arange(
            0, grid_height * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_y: tensor([  0.,   8.,  16.,  24.,  32.,  40.,  48.,  56.,  64.,  72.,  80.,  88.,
         96., 104., 112., 120., 128., 136., 144., 152., 160., 168., 176., 184.,
        192., 200., 208., 216., 224., 232., 240., 248., 256., 264., 272., 280.,
        288., 296., 304., 312., 320., 328., 336., 344., 352., 360., 368., 376.,
        384., 392., 400., 408., 416., 424., 432., 440., 448., 456., 464., 472.])

        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
        \ shift_y.shape: torch.Size([60, 72]), shift_x.shape: torch.Size([60, 72])

        shift_x = shift_x.reshape(-1)
        // shifts_x.shape: torch.Size([72])

        shift_y = shift_y.reshape(-1)
        // shifts_y.shape: torch.Size([60])

        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)
        // shifts.shape: torch.Size([4320, 4])
        anchors.append(
            (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4) )
        // anchors[-1].shape: torch.Size([38880, 4])
        } END iteration: 1/5

        {
        # BEGIN iteration: 2/5

        #------------------------------------
        # size: torch.Size([30, 36]) from grid_sizes
        # size: 16 from self.strides
        # base_anchors.shape: torch.Size([9, 4]) from self.cell_anchors
        #------------------------------------
        grid_height, grid_width = size
        // grid_height: 30
        // grid_width: 36

        device = base_anchors.device
        // device: cuda:0

        shifts_x = torch.arange(
            0, grid_width * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_x: tensor([  0.,  16.,  32.,  48.,  64.,  80.,  96., 112., 128., 144., 160., 176.,
        192., 208., 224., 240., 256., 272., 288., 304., 320., 336., 352., 368.,
        384., 400., 416., 432., 448., 464., 480., 496., 512., 528., 544., 560.])

        shifts_y = torch.arange(
            0, grid_height * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_y: tensor([  0.,  16.,  32.,  48.,  64.,  80.,  96., 112., 128., 144., 160., 176.,
        192., 208., 224., 240., 256., 272., 288., 304., 320., 336., 352., 368.,
        384., 400., 416., 432., 448., 464.])

        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
        \ shift_y.shape: torch.Size([30, 36]), shift_x.shape: torch.Size([30, 36])

        shift_x = shift_x.reshape(-1)
        // shifts_x.shape: torch.Size([36])

        shift_y = shift_y.reshape(-1)
        // shifts_y.shape: torch.Size([30])

        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)
        // shifts.shape: torch.Size([1080, 4])
        anchors.append(
            (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4) )
        // anchors[-1].shape: torch.Size([9720, 4])
        } END iteration: 2/5

        {
        # BEGIN iteration: 3/5

        #------------------------------------
        # size: torch.Size([15, 18]) from grid_sizes
        # size: 32 from self.strides
        # base_anchors.shape: torch.Size([9, 4]) from self.cell_anchors
        #------------------------------------
        grid_height, grid_width = size
        // grid_height: 15
        // grid_width: 18

        device = base_anchors.device
        // device: cuda:0

        shifts_x = torch.arange(
            0, grid_width * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_x: tensor([  0.,  32.,  64.,  96., 128., 160., 192., 224., 256., 288., 320., 352.,
        384., 416., 448., 480., 512., 544.])

        shifts_y = torch.arange(
            0, grid_height * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_y: tensor([  0.,  32.,  64.,  96., 128., 160., 192., 224., 256., 288., 320., 352.,
        384., 416., 448.])

        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
        \ shift_y.shape: torch.Size([15, 18]), shift_x.shape: torch.Size([15, 18])

        shift_x = shift_x.reshape(-1)
        // shifts_x.shape: torch.Size([18])

        shift_y = shift_y.reshape(-1)
        // shifts_y.shape: torch.Size([15])

        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)
        // shifts.shape: torch.Size([270, 4])
        anchors.append(
            (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4) )
        // anchors[-1].shape: torch.Size([2430, 4])
        } END iteration: 3/5

        {
        # BEGIN iteration: 4/5

        #------------------------------------
        # size: torch.Size([8, 9]) from grid_sizes
        # size: 64 from self.strides
        # base_anchors.shape: torch.Size([9, 4]) from self.cell_anchors
        #------------------------------------
        grid_height, grid_width = size
        // grid_height: 8
        // grid_width: 9

        device = base_anchors.device
        // device: cuda:0

        shifts_x = torch.arange(
            0, grid_width * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_x: tensor([  0.,  64., 128., 192., 256., 320., 384., 448., 512.])

        shifts_y = torch.arange(
            0, grid_height * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_y: tensor([  0.,  64., 128., 192., 256., 320., 384., 448.])

        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
        \ shift_y.shape: torch.Size([8, 9]), shift_x.shape: torch.Size([8, 9])

        shift_x = shift_x.reshape(-1)
        // shifts_x.shape: torch.Size([9])

        shift_y = shift_y.reshape(-1)
        // shifts_y.shape: torch.Size([8])

        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)
        // shifts.shape: torch.Size([72, 4])
        anchors.append(
            (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4) )
        // anchors[-1].shape: torch.Size([648, 4])
        } END iteration: 4/5

        {
        # BEGIN iteration: 5/5

        #------------------------------------
        # size: torch.Size([4, 5]) from grid_sizes
        # size: 128 from self.strides
        # base_anchors.shape: torch.Size([9, 4]) from self.cell_anchors
        #------------------------------------
        grid_height, grid_width = size
        // grid_height: 4
        // grid_width: 5

        device = base_anchors.device
        // device: cuda:0

        shifts_x = torch.arange(
            0, grid_width * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_x: tensor([  0., 128., 256., 384., 512.])

        shifts_y = torch.arange(
            0, grid_height * stride, step=stride, dtype=torch.float32, device=device)
        // shifts_y: tensor([  0., 128., 256., 384.])

        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
        \ shift_y.shape: torch.Size([4, 5]), shift_x.shape: torch.Size([4, 5])

        shift_x = shift_x.reshape(-1)
        // shifts_x.shape: torch.Size([5])

        shift_y = shift_y.reshape(-1)
        // shifts_y.shape: torch.Size([4])

        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)
        // shifts.shape: torch.Size([20, 4])
        anchors.append(
            (shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4) )
        // anchors[-1].shape: torch.Size([180, 4])
        } END iteration: 5/5

      } // END for size, stride, base_anchors in zip( grid_sizes, self.strides, self.cell_anchors )

      // len(anchors): 5
      // anchors[i].shape: torch.Size([38880, 4])
      // anchors[i].shape: torch.Size([9720, 4])
      // anchors[i].shape: torch.Size([2430, 4])
      // anchors[i].shape: torch.Size([648, 4])
      // anchors[i].shape: torch.Size([180, 4])

    return anchors
    } // END AnchorGenerator.grid_anchors(grid_sizes)



    }
    anchors_over_all_feature_maps = self.grid_anchors(grid_sizes) // RETURNED
    // len(anchors_over_all_feature_maps): 5

anchors = []


    # -------------------------------------------------------------
    # 2-3-2-2-2
    # loop over anchors_per_feature_map 
    # -------------------------------------------------------------
    // len(imge_list.image_sizes): 1
    for i, (image_height, image_width) in enumerate(image_list.image_sizes) {

      {
      # BEGIN iteration i: 1

      # image_height:480
      # image_width:561
      anchors_in_image = []

      # len(anchors_over_all_feature_maps): 5
      for j, anchors_per_feature_map in enumerate(anchors_over_all_feature_maps): {

        {
        # BEGIN iteration: j: 1/5

        # anchors_per_feature_map.shape: torch.Size([38880, 4])
        # image_width: 561
        # image_height: 480

        # -----------------------------------------
        # 2-3-2-2-2-1 boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy")
        # i: 1, j: 1/5
        # -----------------------------------------
        boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // CALL
    {
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([38880, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([38880, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')


        } boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // RETURNED
        // boxlist:
      BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-2 self.add_visibility_to(boxlist)
        # i: 1, j: 1/5
        # -----------------------------------------
        self.add_visibility_to(boxlist) // CALL
    {
        AnchorGenerator.add_visibitity_to(boxlist) { // BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params

          // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
          // boxlist.size: (561, 480)
          // boxlist.bbox.shape: torch.Size([38880, 4])

          image_width, image_height = boxlist.size
          // image_width: 561
          // image_height: 480

          anchors = boxlist.bbox
          // anchors.shape: torch.Size([38880, 4])

          // self.straddle_thresh: -1
          else: ie. self.straddle_thresh < 0
            device = anchors.device
            inds_inside = torch.ones(anchors.shape[0], dtype=torch.bool, device=device)

          // inds_inside.shape torch.Size([38880])
          boxlist.add_field("visibility", inds_inside)

        } // END AnchorGenerator.add_visibitity_to(boxlist)



        } self.add_visibility_to(boxlist) // RETURNED
        // boxlist:BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-3 anchors_in_image.append(boxlist)
        # i: 1, j: 1/5
        # -----------------------------------------
        anchors_in_image.append(boxlist)

        } // END iteration: j = 1/5

        {
        # BEGIN iteration: j: 2/5

        # anchors_per_feature_map.shape: torch.Size([9720, 4])
        # image_width: 561
        # image_height: 480

        # -----------------------------------------
        # 2-3-2-2-2-1 boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy")
        # i: 1, j: 2/5
        # -----------------------------------------
        boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // CALL
    {
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([9720, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([9720, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')


        } boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // RETURNED
        // boxlist:
      BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-2 self.add_visibility_to(boxlist)
        # i: 1, j: 2/5
        # -----------------------------------------
        self.add_visibility_to(boxlist) // CALL
    {
        AnchorGenerator.add_visibitity_to(boxlist) { // BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params

          // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
          // boxlist.size: (561, 480)
          // boxlist.bbox.shape: torch.Size([9720, 4])

          image_width, image_height = boxlist.size
          // image_width: 561
          // image_height: 480

          anchors = boxlist.bbox
          // anchors.shape: torch.Size([9720, 4])

          // self.straddle_thresh: -1
          else: ie. self.straddle_thresh < 0
            device = anchors.device
            inds_inside = torch.ones(anchors.shape[0], dtype=torch.bool, device=device)

          // inds_inside.shape torch.Size([9720])
          boxlist.add_field("visibility", inds_inside)

        } // END AnchorGenerator.add_visibitity_to(boxlist)



        } self.add_visibility_to(boxlist) // RETURNED
        // boxlist:BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-3 anchors_in_image.append(boxlist)
        # i: 1, j: 2/5
        # -----------------------------------------
        anchors_in_image.append(boxlist)

        } // END iteration: j = 2/5

        {
        # BEGIN iteration: j: 3/5

        # anchors_per_feature_map.shape: torch.Size([2430, 4])
        # image_width: 561
        # image_height: 480

        # -----------------------------------------
        # 2-3-2-2-2-1 boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy")
        # i: 1, j: 3/5
        # -----------------------------------------
        boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // CALL
    {
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([2430, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([2430, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')


        } boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // RETURNED
        // boxlist:
      BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-2 self.add_visibility_to(boxlist)
        # i: 1, j: 3/5
        # -----------------------------------------
        self.add_visibility_to(boxlist) // CALL
    {
        AnchorGenerator.add_visibitity_to(boxlist) { // BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params

          // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
          // boxlist.size: (561, 480)
          // boxlist.bbox.shape: torch.Size([2430, 4])

          image_width, image_height = boxlist.size
          // image_width: 561
          // image_height: 480

          anchors = boxlist.bbox
          // anchors.shape: torch.Size([2430, 4])

          // self.straddle_thresh: -1
          else: ie. self.straddle_thresh < 0
            device = anchors.device
            inds_inside = torch.ones(anchors.shape[0], dtype=torch.bool, device=device)

          // inds_inside.shape torch.Size([2430])
          boxlist.add_field("visibility", inds_inside)

        } // END AnchorGenerator.add_visibitity_to(boxlist)



        } self.add_visibility_to(boxlist) // RETURNED
        // boxlist:BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-3 anchors_in_image.append(boxlist)
        # i: 1, j: 3/5
        # -----------------------------------------
        anchors_in_image.append(boxlist)

        } // END iteration: j = 3/5

        {
        # BEGIN iteration: j: 4/5

        # anchors_per_feature_map.shape: torch.Size([648, 4])
        # image_width: 561
        # image_height: 480

        # -----------------------------------------
        # 2-3-2-2-2-1 boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy")
        # i: 1, j: 4/5
        # -----------------------------------------
        boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // CALL
    {
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([648, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([648, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')


        } boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // RETURNED
        // boxlist:
      BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-2 self.add_visibility_to(boxlist)
        # i: 1, j: 4/5
        # -----------------------------------------
        self.add_visibility_to(boxlist) // CALL
    {
        AnchorGenerator.add_visibitity_to(boxlist) { // BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params

          // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
          // boxlist.size: (561, 480)
          // boxlist.bbox.shape: torch.Size([648, 4])

          image_width, image_height = boxlist.size
          // image_width: 561
          // image_height: 480

          anchors = boxlist.bbox
          // anchors.shape: torch.Size([648, 4])

          // self.straddle_thresh: -1
          else: ie. self.straddle_thresh < 0
            device = anchors.device
            inds_inside = torch.ones(anchors.shape[0], dtype=torch.bool, device=device)

          // inds_inside.shape torch.Size([648])
          boxlist.add_field("visibility", inds_inside)

        } // END AnchorGenerator.add_visibitity_to(boxlist)



        } self.add_visibility_to(boxlist) // RETURNED
        // boxlist:BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-3 anchors_in_image.append(boxlist)
        # i: 1, j: 4/5
        # -----------------------------------------
        anchors_in_image.append(boxlist)

        } // END iteration: j = 4/5

        {
        # BEGIN iteration: j: 5/5

        # anchors_per_feature_map.shape: torch.Size([180, 4])
        # image_width: 561
        # image_height: 480

        # -----------------------------------------
        # 2-3-2-2-2-1 boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy")
        # i: 1, j: 5/5
        # -----------------------------------------
        boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // CALL
    {
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([180, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([180, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')


        } boxlist = BoxList( anchors_per_feature_map, (image_width, image_height), mode="xyxy" ) // RETURNED
        // boxlist:
      BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-2 self.add_visibility_to(boxlist)
        # i: 1, j: 5/5
        # -----------------------------------------
        self.add_visibility_to(boxlist) // CALL
    {
        AnchorGenerator.add_visibitity_to(boxlist) { // BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/anchor_generator.py

          // Params

          // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
          // boxlist.size: (561, 480)
          // boxlist.bbox.shape: torch.Size([180, 4])

          image_width, image_height = boxlist.size
          // image_width: 561
          // image_height: 480

          anchors = boxlist.bbox
          // anchors.shape: torch.Size([180, 4])

          // self.straddle_thresh: -1
          else: ie. self.straddle_thresh < 0
            device = anchors.device
            inds_inside = torch.ones(anchors.shape[0], dtype=torch.bool, device=device)

          // inds_inside.shape torch.Size([180])
          boxlist.add_field("visibility", inds_inside)

        } // END AnchorGenerator.add_visibitity_to(boxlist)



        } self.add_visibility_to(boxlist) // RETURNED
        // boxlist:BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)

        # -----------------------------------------
        # 2-3-2-2-2-3 anchors_in_image.append(boxlist)
        # i: 1, j: 5/5
        # -----------------------------------------
        anchors_in_image.append(boxlist)

        } // END iteration: j = 5/5

    } // END for j, anchors_per_feature_map in enumerate(anchors_over_all_feature_maps)

    // anchors_in_image:[BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)]
    // len(anchors_in_image):5

    # -----------------------------------------
    # 2-3-2-2-3 anchors.append(anchors_in_image)
    # -----------------------------------------
    anchors.append(anchors_in_image)

} # END iteration i: 1

} // END for i, (image_height, image_width) in enumerate(image_list.image_sizes)

    # return value (anchors) info
    // len(anchors):1

    // len(anchors[0]):5

    // anchors[0][0]: BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy)
    // anchors[0][1]: BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy)
    // anchors[0][2]: BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy)
    // anchors[0][3]: BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy)
    // anchors[0][4]: BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)

return anchors

  } // END AnchorGenerator.forward(image_list, feature_maps)
    }
anchors = self.anchor_generator(images, features) // RETURNED
    // anchors: [[BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)]]


    # ===========================================
    # 2-3-2-3 RPN._forward_test
    # ===========================================

    if self.training: False
      # call paramers info
      # anchors:
      #    from self.anchor_generator(self, x)  # 2-3-2-2
      # box_cls, box_regression:
      #     return value RetinaNetHead.forward(self, x)
      #     called by box_cls, box_regression = self.head(features)  # 2-3-2-1
      return self._forward_test(anchors, box_cls, box_regression) // CALL


RetinaNetModule._forward_test(self, anchors, box_cls, box_regression) { // BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/retinanet.py

  // Params:
    // len(anchors): 1
    // anchors: [[BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)]]
    // len(box_cls): 5 from RetinaNetHead (RPN.Head cls_towers and cls_logit)
    // len(box_regression): 5 from RetinaNetHead (RPN.Head bbox_towers and bbox_pred
  // self.box_selector_test: RetinaNetPostProcessor()
  boxes = self.box_selector_test(anchors=>anchors, box_cls=>objectness, box_regression=>box_regression) // CALL
{
    RPNPostProcessor.forward(self. anchors, objectness, box_regression, targets=None) { // BEGIN
        // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/inference.py

        // Params:
          // len(anchors): 1
          // anchors: [[BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)]]

          // len(objectness): 5 aka box_cls
          //objectness[0].shape): torch.Size([1, 9, 60, 72])
          //objectness[1].shape): torch.Size([1, 9, 30, 36])
          //objectness[2].shape): torch.Size([1, 9, 15, 18])
          //objectness[3].shape): torch.Size([1, 9, 8, 9])
          //objectness[4].shape): torch.Size([1, 9, 4, 5])

          // len(box_regression): : 5
          //box_regression[0].shape): torch.Size([1, 36, 60, 72])
          //box_regression[1].shape): torch.Size([1, 36, 30, 36])
          //box_regression[2].shape): torch.Size([1, 36, 15, 18])
          //box_regression[3].shape): torch.Size([1, 36, 8, 9])
          //box_regression[4].shape): torch.Size([1, 36, 4, 5])
          // target: None

        sampled_boxes = []
        num_levels = len(objectness)
        // num_levels: 5

        anchors = list(zip(*anchors))

        for a, o, b in zip(anchors, objectness, box_regression) {
          # 2-3-2-3-1 loop over single_feature_map
          {
          # BEGIN for a, o, b in zip()  iteration: 1/5
          # a: (BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy),)
          # o.shape: torch.Size([1, 9, 60, 72])
          # b.shape: torch.Size([1, 36, 60, 72])

          # 2-3-2-3-2 self.forward_for_single_feature_map
          // self.forward_for_single_feature_map: <bound method RetinaNetPostProcessor.forward_for_single_feature_map of RetinaNetPostProcessor()>
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) { // CALL
RetinaNetPostProcessor.forward_for_single_feature_map() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/inference.py

  // Params:
    > anchors: list[BoxList]
    > box_cls: tensor of size N, A*C, H, W)
    > box_regression: tensor of size N, A*4, H, W)

  device = box_cls.device
  // device: cuda:0

  N, _, H, W = box_cls.shape
  // N:1, H:60, W:72

  A = box_regression.size(1) // 4
  // A : 9

  C = box_cls.size(1) // A
  // C: 1

  # put 'box_cls' in the same format as anchors
  box_cls = permute_and_flatten(box_cls, N, A, C, H, W)
  // box_cls.shape: torch.Size([1, 38880, 1])

  box_cls = box_cls.sigmoid()
  // box_cls.shape: torch.Size([1, 38880, 1])

  # put 'box_cls' in the same format as anchors
  box_regression = permute_and_flatten(box_regression, N, A, 4, H, W)
  // box_regression.shape: torch.Size([1, 38880, 4])

  box_regression = box_regression.reshape(N, -1, 4)
  // box_regression.shape: torch.Size([1, 38880, 4])

  num_anchors = A * H * W
  // num_anchors: 38880

  candidate_inds = box_cls > self.pre_nms_thresh
  // candidate_inds.shape: torch.Size([1, 38880, 1])

  pre_nms_top_n = candidate_inds.view(N, -1).sum(1)
  // pre_nms_top_n: tensor([824], device='cuda:0')

  pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)
  // pre_nms_top_n: tensor([824], device='cuda:0')

  results = []

  // box_cls.shape: torch.Size([1, 38880, 1])
  // box_regression.shape: torch.Size([1, 38880, 4])
  // pre_nmns_top_n: tensor([824], device='cuda:0')
  // candidate.inds.shape: torch.Size([1, 38880, 1])
  // anchors: (BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy),)
  for per_box_cls, per_box_regression, ... in zip():
  {
    # ====================================
    # per_box_cls.shape: torch.Size([38880, 1])
    # type(per_box_cls): <class 'torch.Tensor'>
    # per_box_regression.shape: torch.Size([38880, 4])
    # per_pre_nms_top_n: 824
    # per_candidate_inds.shape: torch.Size([38880, 1])
    # per_anchors: BoxList(num_boxes=38880, image_width=561, image_height=480, mode=xyxy)
    # ====================================

    per_box_cls = per_box_cls[per_candidate_inds]
    per_box_cls, top_k_indices =per_box_cls.topk(per_pre_nms_top_n, sorted=False)
    // per_box_cls: tensor([0.0545, 0.0505, 0.0648, 0.0724, 0.0626, 0.0504, 0.0510, 0.0538, 0.0627,
        0.0821, 0.0829, 0.0701, 0.0647, 0.0618, 0.0578, 0.0525, 0.1022, 0.0557,
        0.0775, 0.1503, 0.0947, 0.0545, 0.0761, 0.0569, 0.7679, 0.8071, 0.0563,
        0.9739, 0.9796, 0.9685, 0.9913, 0.9851, 0.9970, 0.9873, 0.9951, 0.9798,
        0.9599, 0.2523, 0.1126, 0.7417, 0.7727, 0.9519, 0.9796, 0.0760, 0.9769,
        0.9890, 0.1430, 0.9761, 0.9766, 0.1035, 0.9572, 0.9230, 0.0703, 0.7751,
        0.6128, 0.0589, 0.2658, 0.5974, 0.7757, 0.9671, 0.7705, 0.9889, 0.7880,
        0.9958, 0.8507, 0.9952, 0.6687, 0.8897, 0.2057, 0.3776, 0.6328, 0.9302,
        0.7955, 0.9803, 0.7912, 0.9726, 0.7273, 0.8915, 0.3314, 0.3621, 0.0613,
        0.0568, 0.0645, 0.0552, 0.4269, 0.1427, 0.9832, 0.2970, 0.9912, 0.2450,
        0.8570, 0.0858, 0.6460, 0.4278, 0.4642, 0.9917, 0.9399, 0.6859, 0.9967,
        0.9935, 0.6712, 0.9936, 0.9913, 0.6326, 0.9925, 0.9467, 0.3024, 0.8266,
        0.2804, 0.4216, 0.0515, 0.9660, 0.9744, 0.5289, 0.0560, 0.0676, 0.0554,
        0.0545, 0.2606, 0.8336, 0.9346, 0.9546, 0.9298, 0.3850, 0.0838, 0.0802,
        0.1481, 0.0967, 0.1999, 0.2023, 0.1041, 0.0512, 0.0826, 0.2406, 0.1312,
        0.0652, 0.0601, 0.0538, 0.1123, 0.1057, 0.2720, 0.0973, 0.5162, 0.1514,
        0.4001, 0.1195, 0.1080, 0.0857, 0.0588, 0.1129, 0.2547, 0.0707, 0.1654,
        0.1141, 0.2053, 0.0546, 0.1029, 0.1793, 0.0815, 0.3471, 0.9028, 0.4967,
        0.2189, 0.9935, 0.9864, 0.9938, 0.9984, 0.9945, 0.9990, 0.9945, 0.9989,
        0.9946, 0.9982, 0.9608, 0.9804, 0.1602, 0.0869, 0.1118, 0.5033, 0.2528,
        0.2721, 0.5947, 0.4944, 0.2941, 0.1496, 0.5086, 0.6390, 0.1028, 0.0914,
        0.3283, 0.3811, 0.2243, 0.1633, 0.1493, 0.1201, 0.1979, 0.1082, 0.0582,
        0.1223, 0.0617, 0.1152, 0.0824, 0.2102, 0.0969, 0.0616, 0.2021, 0.1036,
        0.1511, 0.3139, 0.2755, 0.1030, 0.1009, 0.3179, 0.3660, 0.2245, 0.2003,
        0.2719, 0.4079, 0.1314, 0.2014, 0.2840, 0.1691, 0.2618, 0.0967, 0.4253,
        0.1710, 0.2743, 0.1794, 0.0961, 0.1299, 0.1385, 0.7103, 0.1749, 0.4337,
        0.6922, 0.4099, 0.1957, 0.8846, 0.5571, 0.1918, 0.9290, 0.1688, 0.7825,
        0.5599, 0.2200, 0.3936, 0.8421, 0.9132, 0.8819, 0.5864, 0.0621, 0.0504,
        0.0687, 0.0587, 0.1633, 0.0703, 0.0998, 0.3276, 0.0542, 0.3042, 0.0788,
        0.0767, 0.0550, 0.0908, 0.0634, 0.0644, 0.1185, 0.1366, 0.7618, 0.9387,
        0.9779, 0.9674, 0.7246, 0.0722, 0.0605, 0.0646, 0.2000, 0.4133, 0.0802,
        0.0855, 0.2838, 0.4975, 0.4837, 0.1694, 0.1231, 0.2607, 0.3590, 0.3049,
        0.1724, 0.3226, 0.0971, 0.1659, 0.2749, 0.1677, 0.2282, 0.0563, 0.1812,
        0.3336, 0.1443, 0.1828, 0.2576, 0.4413, 0.1839, 0.0984, 0.0768, 0.0587,
        0.0543, 0.0687, 0.3637, 0.0944, 0.0912, 0.9398, 0.8341, 0.9575, 0.9766,
        0.9646, 0.9929, 0.9782, 0.9966, 0.9818, 0.9962, 0.9396, 0.9858, 0.0911,
        0.1649, 0.1393, 0.0874, 0.2587, 0.4373, 0.0516, 0.0526, 0.3515, 0.4708,
        0.3577, 0.0913, 0.2027, 0.2087, 0.2515, 0.1564, 0.1120, 0.2046, 0.0503,
        0.1411, 0.1732, 0.0805, 0.1626, 0.1341, 0.3717, 0.2639, 0.0753, 0.1582,
        0.3672, 0.3477, 0.1412, 0.1502, 0.0730, 0.0581, 0.2402, 0.4959, 0.5533,
        0.3312, 0.0511, 0.0572, 0.0549, 0.0816, 0.0587, 0.0549, 0.0793, 0.0665,
        0.0508, 0.0807, 0.1192, 0.0789, 0.0502, 0.0722, 0.0872, 0.4946, 0.6951,
        0.5751, 0.6494, 0.6798, 0.0522, 0.1281, 0.2811, 0.1409, 0.2105, 0.1369,
        0.0616, 0.0518, 0.0630, 0.0660, 0.0944, 0.0670, 0.1051, 0.0801, 0.7698,
        0.8336, 0.6916, 0.9541, 0.1085, 0.7989, 0.8978, 0.1199, 0.7018, 0.9595,
        0.1290, 0.0661, 0.0509, 0.0836, 0.2364, 0.1878, 0.9165, 0.1960, 0.9672,
        0.2706, 0.8132, 0.0595, 0.1523, 0.1810, 0.1796, 0.1555, 0.4479, 0.1965,
        0.0775, 0.0608, 0.1591, 0.8917, 0.9053, 0.0601, 0.0590, 0.8498, 0.9045,
        0.5905, 0.6499, 0.6893, 0.6255, 0.3261, 0.9478, 0.7239, 0.1012, 0.2592,
        0.8900, 0.6326, 0.1830, 0.0508, 0.1599, 0.3648, 0.0512, 0.0755, 0.0553,
        0.8603, 0.8847, 0.0895, 0.5981, 0.8602, 0.9042, 0.0734, 0.9302, 0.8190,
        0.0554, 0.9254, 0.9551, 0.1745, 0.9480, 0.8683, 0.0562, 0.8328, 0.7992,
        0.1086, 0.4353, 0.1224, 0.2767, 0.0810, 0.7835, 0.9798, 0.7289, 0.8500,
        0.9906, 0.9666, 0.8622, 0.9877, 0.9793, 0.8968, 0.9920, 0.8804, 0.6951,
        0.9550, 0.4199, 0.0505, 0.0563, 0.0847, 0.0692, 0.0534, 0.5886, 0.1316,
        0.6456, 0.1393, 0.0943, 0.9112, 0.9696, 0.0694, 0.7265, 0.0795, 0.0804,
        0.0998, 0.1180, 0.1248, 0.1146, 0.1275, 0.1374, 0.1307, 0.1244, 0.1074,
        0.0780, 0.0517, 0.0546, 0.0638, 0.0710, 0.0825, 0.1162, 0.1527, 0.1805,
        0.1887, 0.1686, 0.1584, 0.1729, 0.1820, 0.1485, 0.1209, 0.1078, 0.0874,
        0.0571, 0.0637, 0.0608, 0.0550, 0.0633, 0.0648, 0.0630, 0.0666, 0.0670,
        0.0570, 0.0608, 0.0595, 0.0567, 0.0634, 0.0696, 0.0743, 0.0900, 0.1051,
        0.1129, 0.1155, 0.1192, 0.1189, 0.1233, 0.1235, 0.1178, 0.1029, 0.0928,
        0.0728, 0.0620, 0.0730, 0.2589, 0.0710, 0.0902, 0.0739, 0.0781, 0.2166,
        0.0573, 0.2008, 0.0599, 0.1267, 0.0743, 0.1300, 0.0674, 0.3356, 0.6155,
        0.0728, 0.3799, 0.5818, 0.0522, 0.4907, 0.3339, 0.8775, 0.9179, 0.9322,
        0.9923, 0.9675, 0.9974, 0.9847, 0.9986, 0.9902, 0.9991, 0.9331, 0.9814,
        0.0804, 0.1555, 0.3222, 0.0599, 0.0599, 0.8823, 0.5462, 0.4506, 0.9124,
        0.8704, 0.0569, 0.3374, 0.0977, 0.8377, 0.7096, 0.0765, 0.1592, 0.4593,
        0.1133, 0.0930, 0.1864, 0.7496, 0.3008, 0.4304, 0.9101, 0.8391, 0.4610,
        0.2634, 0.7837, 0.6869, 0.3811, 0.1453, 0.6172, 0.2871, 0.2160, 0.1551,
        0.0725, 0.0502, 0.4727, 0.6879, 0.1076, 0.4593, 0.6061, 0.0658, 0.1326,
        0.1686, 0.7960, 0.1583, 0.9749, 0.2215, 0.9914, 0.3438, 0.9967, 0.6237,
        0.9979, 0.2953, 0.9093, 0.0721, 0.0524, 0.0611, 0.4950, 0.0683, 0.1352,
        0.2102, 0.0759, 0.0686, 0.0570, 0.1448, 0.2133, 0.0909, 0.1651, 0.0745,
        0.0929, 0.0740, 0.5871, 0.7192, 0.3888, 0.1329, 0.2092, 0.7976, 0.9400,
        0.9055, 0.9906, 0.9263, 0.9882, 0.9440, 0.9924, 0.9703, 0.9942, 0.8400,
        0.9592, 0.1394, 0.1806, 0.0500, 0.7550, 0.5700, 0.0776, 0.9945, 0.9924,
        0.0607, 0.9980, 0.9993, 0.0785, 0.9989, 0.9996, 0.2283, 0.9993, 0.9981,
        0.1689, 0.9927, 0.9442, 0.1370, 0.0598, 0.0641, 0.0706, 0.7493, 0.9604,
        0.0568, 0.9716, 0.0525, 0.9791, 0.1255, 0.9741, 0.1266, 0.6598, 0.0515,
        0.0523, 0.0643, 0.0754, 0.0569, 0.0683, 0.2669, 0.4762, 0.9882, 0.6769,
        0.9993, 0.7387, 0.9997, 0.8385, 0.9988, 0.2810, 0.8360, 0.0678, 0.1137,
        0.1225, 0.1207, 0.4339, 0.8146, 0.8294, 0.3573, 0.0730, 0.0734, 0.0851,
        0.0644, 0.1395, 0.0569, 0.1660, 0.0551, 0.6439, 0.6573, 0.7485, 0.8226,
        0.7195, 0.7623, 0.7148, 0.7276, 0.0815, 0.8037, 0.7579, 0.5275, 0.5121,
        0.1253, 0.2048, 0.8914, 0.4629, 0.2687, 0.9724, 0.8970, 0.1997, 0.9717,
        0.9799, 0.2221, 0.9818, 0.9834, 0.2238, 0.9864, 0.9584, 0.1106, 0.8446,
        0.6469, 0.1084, 0.6116, 0.0645, 0.0837, 0.9617, 0.7296, 0.7211, 0.0890,
        0.9617, 0.7340, 0.7753, 0.1262, 0.6316, 0.1416, 0.1082, 0.2503, 0.6117,
        0.7349, 0.7030, 0.5063, 0.1274, 0.1003, 0.0958, 0.7919, 0.0999, 0.9593,
        0.1084, 0.9720, 0.1124, 0.8592, 0.1971, 0.6617, 0.5543, 0.3492, 0.1972,
        0.6614, 0.5007, 0.5371, 0.3017, 0.0500])
    // top_k_indices: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,
         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,
         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,
         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,
         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,
         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,
         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,
         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,
        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,
        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,
        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,
        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,
        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,
        196, 197, 198, 199, 200, 201, 203, 204, 205, 206, 207, 208, 209, 210,
        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,
        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,
        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,
        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,
        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,
        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,
        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,
        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,
        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,
        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,
        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,
        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,
        379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,
        393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406,
        407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,
        421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,
        435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448,
        449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462,
        463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476,
        477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490,
        491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504,
        505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518,
        519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,
        533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546,
        547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560,
        561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574,
        575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588,
        589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602,
        603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,
        617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,
        631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644,
        645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658,
        659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,
        673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686,
        687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700,
        701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,
        715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728,
        729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742,
        743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756,
        757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770,
        771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784,
        785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,
        799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812,
        813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 202])

    per_candidate_nonzeros = \
       per_candidate_inds.nonzero()[top_k_indices, :]
    // per_candidate_inds.shape: torch.Size([38880, 1])

    per_box_loc = per_candidate_nonzeros[:, 0]
    // per_box_loc.shape: torch.Size([824])

    per_class = per_candidate_nonzeros[:, 1]
    // per_class.shape: torch.Size([824])

    per_class += 1

    detections = self.box_coder.decode( ) { // CALL
    BoxCoder.decode(self, rel_codes, boxes) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/box_coder.py
      // Params:
        rel_codes.shape: torch.Size([824, 4])
        boxes.shape: torch.Size([824, 4])

      boxes = boxes.to(rel_codes.dtype)
      // boxes.shape: torch.Size([824, 4])

      TO_REMOVE = 1  # TODO remove

      widths = boxes[:, 2] - boxes[:, 0] + TO_REMOVE
      // widths.shape: torch.Size([824])

      heights = boxes[:, 3] - boxes[:, 1] + TO_REMOVE
      // heights.shape: torch.Size([824])

      ctr_x = boxes[:, 0] + 0.5 * widths
      // ctr_x.shape: torch.Size([824])

      ctr_y = boxes[:, 1] + 0.5 * heights
      // ctr_y.shape: torch.Size([824])


      wx, wy, ww, wh = self.weights
      // wx: 10.0, wy: 10.0, ww: 5.0, wh: 5.0

      dx = rel_codes[:, 0::4] / wx
      // dx.shape: torch.Size([824, 1])

      dy = rel_codes[:, 1::4] / wy
      // dy.shape: torch.Size([824, 1])

      dw = rel_codes[:, 2::4] / ww
      // dw.shape: torch.Size([824, 1])

      dh = rel_codes[:, 3::4] / wh
      // dh.shape: torch.Size([824, 1])


      # Prevent sending too large values into torch.exp()
      dw = torch.clamp(dw, max=self.bbox_xform_clip)
      // dw.shape: torch.Size([824, 1])

      dh = torch.clamp(dh, max=self.bbox_xform_clip)
      // dh.shape: torch.Size([824, 1])

      pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
      // pred_ctr_x.shape: torch.Size([824, 1])

      pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
      // pred_ctr_y.shape: torch.Size([824, 1])

      pred_w = torch.exp(dw) * widths[:, None]
      // pred_w.shape: torch.Size([824, 1])

      pred_h = torch.exp(dh) * heights[:, None]
      // pred_h.shape: torch.Size([824, 1])


      pred_boxes = torch.zeros_like(rel_codes)
      // pred_boxes.shape: torch.Size([824, 4])


      # x1
      pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w

      # y1
      pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h

      # x2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1

      # y2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1


      pred_boxes.shape: torch.Size([824, 4])

      return pred_boxes

    } // END BoxCoder.decode(self, rel_codes, boxes)

    } detections = self.box_coder.decode( ) // RETURNED

    // type(detections): <class 'torch.Tensor'>
    // detections: tensor([[ 84.3667,  26.0055, 137.4505,  61.0160],
        [ 81.6323,  21.6916, 144.6450,  64.9447],
        [ 85.3342,  25.3851, 137.6819,  63.8709],
        [ 82.5207,  21.0348, 143.6139,  66.8546],
        [ 82.4601,  22.2807, 141.0100,  66.6658],
        [ 84.8567,  21.2071, 144.3667,  69.5575],
        [ 83.5618,  25.2106, 145.9124,  69.6452],
        [ 81.7194,  24.8670, 142.5818,  69.9192],
        [ 86.0792,  27.7400, 138.9902,  69.0329],
        [ 85.1424,  24.1224, 145.2530,  69.9293],
        [ 83.3664,  24.8877, 141.8482,  69.6323],
        [ 81.5598,  22.5834, 139.9326,  73.9334],
        [ 86.7267,  23.8156, 146.5647,  71.6098],
        [ 84.0919,  25.1052, 144.7825,  71.3426],
        [ 81.7754,  24.4667, 142.7063,  74.0643],
        [189.8495,  47.5290, 276.9255,  75.5372],
        [191.1963,  47.8503, 277.6338,  74.9034],
        [191.7859,  47.7573, 277.5412,  74.3465],
        [283.6277,  46.3342, 365.4694,  73.3390],
        [284.1583,  47.2475, 370.0930,  73.9406],
        [284.3569,  46.8385, 371.4356,  73.8201],
        [ 81.5055,  26.8466, 141.0288,  80.8369],
        [188.1788,  49.0368, 254.0852,  77.1832],
        [190.5516,  49.3452, 255.4318,  76.1301],
        [190.1833,  48.8391, 273.7321,  76.5337],
        [189.2685,  47.7271, 279.0483,  77.3737],
        [190.7873,  49.6181, 269.3083,  75.1623],
        [189.8712,  49.6322, 278.9484,  75.9440],
        [189.4833,  49.7178, 279.6477,  76.4452],
        [190.4059,  49.9150, 277.5968,  75.3687],
        [189.9719,  49.9034, 278.5552,  75.6677],
        [190.4708,  49.8276, 278.0909,  75.1828],
        [190.0893,  49.9301, 278.5348,  75.4135],
        [190.8251,  49.6378, 277.7680,  74.9851],
        [190.3705,  49.6691, 277.9599,  75.3342],
        [190.4892,  49.5645, 277.6003,  74.9959],
        [189.4738,  49.0137, 278.2446,  75.8319],
        [198.6089,  48.5100, 278.5241,  75.0934],
        [188.4596,  46.7365, 279.9145,  77.0831],
        [285.3959,  49.4109, 366.3403,  75.0933],
        [285.1287,  48.8809, 371.0335,  75.6697],
        [284.6689,  49.7370, 374.1402,  75.5037],
        [284.6151,  50.1222, 375.5279,  75.8295],
        [287.7508,  49.9139, 364.8832,  73.8887],
        [284.4057,  50.2465, 373.6537,  75.1794],
        [284.6501,  50.5545, 373.5644,  75.3763],
        [289.8368,  49.5247, 368.8397,  73.6209],
        [284.0468,  49.5628, 372.9261,  75.2243],
        [283.8894,  49.6209, 372.8475,  75.3838],
        [293.9538,  49.2324, 368.9598,  73.7479],
        [285.2876,  49.2435, 372.1641,  75.2607],
        [284.5014,  49.3594, 372.4071,  75.3319],
        [298.5197,  49.1326, 369.3886,  74.0693],
        [287.3830,  49.0057, 372.3595,  75.6727],
        [286.8722,  48.7796, 372.2668,  76.2148],
        [299.6143,  48.2472, 371.2213,  74.9308],
        [189.4962,  50.1970, 267.6792,  77.5250],
        [188.1603,  48.9389, 279.3611,  78.7619],
        [189.0618,  49.9792, 279.4271,  76.3343],
        [189.0124,  49.6007, 280.1022,  76.4823],
        [189.6323,  50.2384, 278.0634,  75.5641],
        [189.2673,  49.8686, 278.6461,  75.4595],
        [189.8959,  50.4295, 277.5914,  75.7454],
        [189.3003,  50.1906, 278.5018,  75.5450],
        [190.6594,  50.2274, 277.0323,  75.3966],
        [190.1403,  49.6295, 277.4900,  74.9943],
        [190.2222,  50.3332, 277.7574,  75.8734],
        [188.7934,  49.3633, 277.7984,  75.5836],
        [284.9245,  50.6336, 358.3344,  75.7025],
        [284.8996,  49.5978, 369.6307,  76.2320],
        [284.4221,  50.2827, 370.9164,  75.4529],
        [284.7253,  50.0843, 374.6643,  75.2775],
        [284.8036,  50.5805, 372.0775,  75.2716],
        [284.4157,  50.2916, 373.0325,  74.8922],
        [284.3869,  50.3826, 371.2841,  75.6394],
        [284.0992,  49.9024, 372.2044,  75.4457],
        [286.4869,  49.8923, 371.8328,  75.9250],
        [284.6575,  49.5183, 372.3630,  75.5887],
        [290.0332,  49.8589, 372.4819,  76.5001],
        [287.8281,  48.9908, 372.5586,  76.4634],
        [383.7546,  49.5181, 460.7633,  90.3532],
        [384.7573,  48.8599, 466.8135,  89.0081],
        [381.6604,  53.5942, 459.7448, 100.0716],
        [383.9131,  55.1019, 466.2549,  99.4586],
        [238.4122,  84.9613, 318.5222, 113.1132],
        [241.3050,  85.2621, 317.9218, 111.5503],
        [239.6175,  85.8503, 319.2203, 112.8527],
        [242.4292,  85.3954, 317.8752, 111.1026],
        [240.4922,  86.0718, 319.6345, 112.5249],
        [244.0032,  85.5653, 318.6644, 110.9319],
        [240.8734,  85.4673, 320.1519, 112.2903],
        [239.9544,  85.4599, 303.1147, 112.3042],
        [237.0853,  84.5245, 321.8450, 113.9552],
        [236.7069,  84.2587, 320.9389, 114.8031],
        [240.2554,  85.9088, 317.9520, 112.4352],
        [238.9633,  85.4919, 320.5674, 113.0183],
        [238.4246,  85.3136, 319.4730, 113.2248],
        [240.8385,  86.2176, 318.9206, 112.3688],
        [239.2397,  85.8700, 319.3668, 112.5920],
        [239.0491,  85.8640, 319.6366, 112.5499],
        [241.7009,  86.4177, 318.7724, 111.7471],
        [240.2428,  85.8981, 319.8569, 112.3916],
        [240.0954,  85.9654, 320.0150, 112.2914],
        [241.3369,  86.2712, 318.2841, 111.6793],
        [239.6893,  86.1205, 319.7219, 112.2652],
        [239.6442,  85.9381, 319.9695, 112.4216],
        [246.8428,  85.8008, 318.5392, 112.0654],
        [240.8509,  86.0777, 320.1894, 112.8395],
        [241.1006,  85.5724, 320.9250, 113.5559],
        [236.8602,  85.0819, 319.2736, 113.8143],
        [239.1993,  86.1806, 318.5273, 113.0504],
        [238.3354,  85.3555, 319.9039, 112.1881],
        [238.9124,  85.3098, 320.1028, 112.1169],
        [238.5804,  85.6610, 320.3701, 112.5850],
        [160.1840,  27.1372, 254.7311,  52.4772],
        [141.6438,  28.5806, 229.6658,  54.6327],
        [119.7949,  32.8597, 208.5206,  61.9928],
        [151.5045,  21.2250, 226.3292,  46.5286],
        [ 86.4807, 166.0465, 179.1964, 191.0767],
        [ 86.5233, 166.5631, 183.4807, 191.3772],
        [ 86.6161, 166.4895, 183.4474, 191.3174],
        [ 86.3728, 166.1384, 184.5667, 191.6783],
        [ 87.7853, 166.5156, 183.9725, 191.5572],
        [ 90.5461, 165.2031, 184.5834, 191.5187],
        [262.7173, 168.4517, 290.5772, 186.7375],
        [263.3297, 167.7620, 309.2607, 187.2453],
        [262.6386, 167.8145, 311.7198, 188.2211],
        [265.1910, 168.7962, 302.7722, 188.3348],
        [263.3929, 167.3896, 308.3913, 189.6693],
        [264.3418, 168.9993, 316.7410, 188.4087],
        [264.2531, 167.5240, 330.0654, 189.3398],
        [264.8373, 168.1254, 312.0886, 189.7386],
        [265.1667, 168.1816, 329.6533, 188.1714],
        [264.7882, 168.3198, 341.3795, 189.2636],
        [266.8629, 168.3102, 350.8253, 189.1152],
        [269.7712, 167.7047, 359.1746, 188.6405],
        [277.1826, 168.1331, 363.3842, 188.8204],
        [337.9041, 168.6228, 360.7967, 188.0749],
        [304.1264, 167.0116, 408.6430, 189.8706],
        [338.7826, 167.7864, 405.6568, 188.6584],
        [336.2987, 167.8795, 408.4687, 189.6487],
        [340.5846, 167.5184, 408.4134, 188.8685],
        [340.6177, 167.6747, 409.4513, 189.3988],
        [347.2201, 167.1461, 409.3073, 189.0291],
        [343.3271, 167.3522, 409.9089, 189.6219],
        [356.7016, 167.5980, 411.0726, 189.1527],
        [349.8129, 166.5238, 412.2441, 189.7936],
        [361.6164, 167.5695, 410.2159, 189.7805],
        [418.1718, 169.5261, 438.9090, 186.9725],
        [419.9349, 169.0921, 451.5002, 187.7706],
        [419.4501, 168.5807, 473.2019, 189.0437],
        [419.7139, 167.7697, 472.3462, 189.8224],
        [420.1118, 168.5201, 473.4005, 188.9676],
        [421.6666, 168.1755, 472.6396, 189.7003],
        [448.7673, 169.2168, 473.7910, 188.3952],
        [ 84.9108, 167.0341, 115.6545, 192.3629],
        [ 88.6738, 166.9031, 134.2433, 191.3712],
        [ 86.7691, 165.7581, 160.3551, 193.5271],
        [ 87.8115, 167.1930, 116.9703, 190.9194],
        [ 85.8493, 166.6143, 162.5639, 191.6983],
        [ 86.0858, 166.1444, 179.9378, 193.1385],
        [ 85.3195, 164.7366, 182.7987, 194.7586],
        [ 86.9239, 167.1123, 169.7412, 191.2399],
        [ 86.9005, 166.8154, 185.1661, 192.2665],
        [ 86.9113, 166.3486, 181.9599, 192.6921],
        [ 86.6852, 166.9681, 183.7578, 192.0217],
        [ 86.6323, 166.5857, 184.2450, 192.2815],
        [ 86.4307, 166.6757, 184.1377, 191.8983],
        [ 86.5360, 166.4693, 184.1372, 192.0059],
        [ 86.2062, 166.4251, 184.8529, 192.3263],
        [ 86.3828, 166.3524, 184.8439, 192.4374],
        [ 87.9055, 166.6272, 184.0691, 191.7903],
        [ 87.8159, 166.5867, 184.1252, 191.8681],
        [ 85.6391, 166.1307, 184.0376, 192.1512],
        [ 86.2415, 165.5695, 184.4081, 192.5424],
        [ 93.4726, 166.4858, 185.0723, 192.7867],
        [ 89.3614, 165.3303, 185.8295, 193.7601],
        [258.6169, 167.7851, 289.1847, 188.6724],
        [263.9857, 169.3526, 302.6563, 189.1865],
        [262.2264, 168.1130, 309.1320, 190.5010],
        [263.6999, 169.4824, 292.9491, 189.1076],
        [264.6188, 169.3148, 309.9610, 188.6769],
        [263.7929, 169.0276, 311.9009, 188.9826],
        [264.9117, 168.9355, 305.5656, 189.1461],
        [263.3862, 167.9597, 308.6210, 190.4547],
        [264.9560, 169.3295, 313.0210, 188.5455],
        [264.8738, 169.4712, 317.8957, 188.4003],
        [264.3494, 168.1417, 334.0334, 189.9050],
        [265.8916, 168.5858, 309.4896, 189.3276],
        [265.5546, 168.7516, 321.2274, 188.7786],
        [265.7479, 169.3345, 328.9677, 188.4430],
        [264.5922, 168.7762, 339.9061, 189.6571],
        [274.5782, 168.8113, 331.9250, 188.4484],
        [268.1146, 169.1673, 336.0731, 188.4949],
        [266.1603, 168.5325, 347.9766, 189.6745],
        [288.1655, 168.6447, 334.4942, 188.1468],
        [274.7938, 168.8626, 338.2504, 188.4585],
        [269.1167, 168.0424, 358.3644, 189.5081],
        [300.1331, 168.6498, 338.0110, 188.0038],
        [283.3191, 168.7860, 346.3962, 188.4816],
        [304.6886, 168.4482, 349.6947, 188.5187],
        [291.9662, 168.5495, 358.1196, 188.2702],
        [319.4800, 168.1796, 358.6035, 188.3684],
        [297.8243, 168.2039, 364.1556, 188.2823],
        [329.2999, 167.1448, 356.3864, 188.4874],
        [336.2022, 168.5175, 362.8732, 188.7909],
        [321.7201, 167.5253, 384.8431, 189.3909],
        [338.0876, 167.5847, 361.0117, 188.8349],
        [340.8165, 168.0761, 390.1904, 189.5500],
        [338.4178, 167.7909, 407.2128, 189.6727],
        [309.5218, 166.8835, 410.9693, 190.4461],
        [340.7619, 168.4236, 362.1142, 187.9267],
        [341.1334, 167.8386, 406.3761, 189.2383],
        [340.1854, 167.8449, 409.2256, 189.2587],
        [337.9795, 167.5193, 409.1070, 189.6631],
        [344.8590, 167.8534, 409.6698, 189.3604],
        [342.5002, 167.8782, 409.0249, 189.2851],
        [340.7220, 167.8200, 409.3045, 189.3582],
        [352.4498, 168.2261, 408.7179, 189.4298],
        [347.3658, 168.1074, 408.9985, 189.3592],
        [342.9194, 167.7215, 410.0361, 189.7272],
        [356.2625, 168.1098, 409.8875, 189.8266],
        [352.6268, 168.0492, 410.1704, 189.8535],
        [347.3728, 167.4189, 411.4239, 190.7505],
        [361.8760, 167.7826, 410.4435, 190.0359],
        [359.1304, 167.6698, 411.1489, 190.3348],
        [373.8126, 167.6676, 409.9417, 190.1261],
        [381.7013, 167.4787, 412.0681, 190.3214],
        [390.1020, 167.2379, 411.4895, 190.1600],
        [419.4382, 168.8486, 457.1228, 189.2762],
        [419.2539, 169.6673, 439.4394, 188.3402],
        [419.9612, 168.8825, 472.7606, 189.0620],
        [418.9035, 168.3884, 472.4711, 189.3466],
        [421.7532, 169.3831, 457.8564, 189.1070],
        [420.4478, 169.0378, 472.2016, 188.8997],
        [420.0702, 169.0415, 472.6554, 188.9434],
        [424.0123, 168.8805, 468.3499, 189.3364],
        [421.5844, 169.4838, 472.6123, 188.8281],
        [421.1687, 169.4416, 472.8473, 188.8149],
        [425.3893, 169.4781, 469.9029, 189.2510],
        [426.9045, 169.1225, 472.6360, 188.9157],
        [421.7922, 168.5516, 473.6422, 189.3221],
        [445.1537, 169.5959, 471.7038, 188.5364],
        [448.8875, 168.6361, 475.5298, 188.8803],
        [454.4830, 169.2838, 473.9942, 188.8987],
        [ 85.8408, 166.9792, 180.4978, 194.0135],
        [ 86.1601, 167.1202, 183.3605, 192.6026],
        [ 86.4247, 166.9897, 183.0733, 192.3362],
        [ 87.2601, 166.5284, 183.7415, 192.3324],
        [ 88.8787, 166.3365, 183.6306, 191.9683],
        [ 90.4243, 166.1976, 184.3168, 192.5452],
        [263.6964, 171.7386, 289.8964, 190.6655],
        [262.7852, 168.5493, 305.8412, 190.9908],
        [264.3307, 169.9395, 317.0175, 190.4359],
        [264.6479, 169.1057, 335.8821, 191.4104],
        [266.5181, 169.3827, 343.0572, 192.2176],
        [334.5686, 168.0407, 408.6198, 191.1435],
        [340.3021, 167.9066, 409.3480, 189.8693],
        [348.1866, 168.6697, 409.6609, 190.7528],
        [342.2384, 167.8196, 410.5226, 190.0969],
        [358.5732, 169.0353, 410.8103, 190.7482],
        [347.0853, 167.7897, 412.0640, 191.3577],
        [361.6732, 168.4876, 409.2210, 190.0298],
        [419.2306, 168.8338, 472.9011, 189.4822],
        [420.1685, 169.1828, 473.6411, 189.6598],
        [421.1005, 168.7114, 472.6408, 189.7112],
        [446.9391, 170.8629, 471.5777, 188.0081],
        [ 88.2993, 212.4557, 181.5531, 238.6429],
        [ 88.1048, 212.7195, 184.2381, 239.0958],
        [ 87.5120, 212.4502, 184.4520, 239.1782],
        [ 87.8039, 212.1766, 184.2796, 239.3218],
        [ 87.4528, 212.4299, 184.9152, 239.6743],
        [ 86.1944, 210.7790, 185.7376, 240.3016],
        [369.9443, 214.6039, 422.0940, 234.1102],
        [368.8543, 214.2290, 426.4999, 234.4606],
        [375.0612, 215.8157, 401.7202, 233.7840],
        [371.0064, 215.3929, 425.8668, 234.1766],
        [370.3332, 215.3262, 427.1061, 234.2773],
        [369.1037, 214.1511, 430.8066, 235.3234],
        [381.2670, 215.7021, 411.7526, 233.6704],
        [377.4276, 215.4886, 425.3850, 233.9483],
        [371.0132, 215.6398, 426.3771, 234.2332],
        [370.9949, 214.1617, 447.2137, 235.3185],
        [386.2551, 215.4361, 420.6707, 233.8454],
        [385.4487, 214.7374, 424.4011, 233.3411],
        [376.3962, 214.5242, 440.5806, 233.6230],
        [370.5358, 213.4044, 474.0706, 235.4579],
        [398.6454, 215.4348, 423.1554, 233.3761],
        [380.6764, 214.7501, 472.4746, 233.6523],
        [368.9940, 214.2437, 481.3349, 234.7232],
        [409.5374, 215.7868, 425.0090, 233.2323],
        [383.6313, 215.0231, 475.2368, 233.4823],
        [371.0280, 214.8044, 481.0363, 234.2019],
        [388.7836, 214.7776, 474.9496, 233.5186],
        [371.0832, 214.9196, 478.0605, 234.6959],
        [426.9503, 216.8208, 446.2838, 233.7522],
        [427.1217, 216.5989, 473.3499, 233.6802],
        [406.6902, 215.7591, 474.7355, 234.0483],
        [375.9704, 214.1128, 476.8959, 235.1288],
        [430.7351, 217.1066, 462.2012, 234.1270],
        [429.5855, 216.9434, 476.1293, 233.7141],
        [426.1304, 216.6839, 476.7361, 233.7916],
        [433.9200, 217.0580, 470.1667, 234.0029],
        [431.8882, 216.4058, 477.2046, 233.5195],
        [430.4213, 216.5260, 477.7715, 234.0565],
        [439.1028, 216.7661, 474.3296, 234.0174],
        [ 83.0957, 213.6915, 110.5127, 239.6143],
        [ 89.1761, 213.0834, 146.4393, 239.2297],
        [ 86.7066, 213.0768, 182.1014, 240.6517],
        [ 86.5249, 212.6041, 182.6845, 242.1470],
        [ 88.1855, 213.7254, 169.1861, 239.0936],
        [ 87.9187, 213.5482, 185.5499, 239.7403],
        [ 87.6429, 213.1632, 182.3097, 240.0525],
        [ 88.1366, 213.5763, 183.8915, 239.6056],
        [ 87.5967, 213.1063, 184.4645, 239.6129],
        [ 87.4142, 213.0797, 184.7288, 239.7035],
        [ 87.2137, 212.6658, 184.8242, 239.7200],
        [ 87.1244, 212.7422, 184.2865, 239.9040],
        [ 86.5897, 212.1758, 184.6164, 240.0336],
        [ 88.0162, 213.1633, 183.8575, 239.8587],
        [ 87.6678, 212.7245, 184.7078, 239.8383],
        [ 82.3752, 212.6685, 183.6799, 240.5503],
        [ 82.6046, 211.6688, 184.9469, 241.2943],
        [105.1737, 212.8792, 182.6639, 241.3497],
        [ 90.8478, 211.4639, 185.4301, 242.7876],
        [371.3224, 214.7206, 422.5980, 233.8479],
        [369.2389, 214.0680, 429.4314, 234.6020],
        [371.8924, 215.0956, 427.1136, 233.5626],
        [370.8863, 214.7741, 427.9959, 233.7211],
        [369.1853, 213.7650, 430.8760, 235.0245],
        [380.7544, 215.0338, 412.3713, 233.0638],
        [377.6071, 215.3717, 425.5652, 233.3633],
        [371.7820, 214.9301, 427.1497, 233.6000],
        [371.0959, 214.0045, 449.8553, 235.4614],
        [386.1010, 215.2607, 419.7377, 233.3779],
        [383.0343, 215.0291, 424.2531, 233.0987],
        [375.5623, 214.8829, 443.3173, 234.2352],
        [369.6420, 213.8183, 471.6472, 235.5043],
        [397.9195, 215.0316, 422.8610, 232.4503],
        [377.3078, 214.8300, 470.9938, 234.2983],
        [369.1644, 214.3898, 477.8159, 234.9505],
        [408.7876, 215.1372, 424.5245, 231.9823],
        [379.6002, 214.4747, 476.1956, 233.5626],
        [368.7227, 214.0923, 481.8051, 234.2849],
        [419.2658, 215.6598, 468.8927, 233.1015],
        [391.2058, 215.1283, 476.9712, 234.0101],
        [365.4753, 213.6152, 480.0696, 234.6762],
        [427.9742, 216.6761, 475.2256, 233.0365],
        [418.3366, 216.1560, 475.7079, 233.5560],
        [377.2358, 213.2156, 477.0316, 234.8982],
        [430.7788, 216.2748, 464.0018, 233.4227],
        [429.4432, 216.4026, 475.9920, 232.7876],
        [428.2410, 216.3161, 476.5078, 233.2142],
        [433.5630, 216.3629, 470.4153, 233.4251],
        [429.9487, 216.1364, 476.3855, 232.9899],
        [429.1692, 215.9622, 477.0669, 233.4602],
        [457.6685, 216.4444, 477.1715, 233.0924],
        [ 86.6652, 213.8525, 182.6852, 239.9506],
        [ 86.2805, 213.6962, 183.2912, 239.6093],
        [ 86.4925, 213.3530, 183.9239, 239.5478],
        [ 87.7832, 213.4218, 184.2358, 239.7390],
        [229.8425, 206.0351, 264.0146, 266.0923],
        [202.1852, 209.7880, 235.0489, 262.9377],
        [234.6037, 219.4003, 266.1331, 265.5230],
        [236.5524, 217.7327, 271.0794, 271.4353],
        [256.1626, 220.2080, 293.8290, 273.1092],
        [204.6209, 222.1308, 234.9541, 262.8286],
        [206.7763, 221.3875, 239.1078, 266.6196],
        [230.3000, 234.6892, 267.2329, 279.5276],
        [213.9327, 229.2657, 244.3768, 265.9382],
        [209.9572, 224.9115, 244.2021, 265.7338],
        [213.6001, 224.0166, 249.5701, 269.6523],
        [208.3784, 223.1862, 252.9446, 270.1814],
        [ 86.5901, 261.6510, 139.3716, 286.2280],
        [ 85.0667, 262.2927, 131.4126, 287.3525],
        [ 88.5754, 261.4500, 157.2504, 285.7423],
        [ 87.1644, 260.3001, 179.4767, 287.1983],
        [ 86.4445, 261.1275, 182.8976, 287.2454],
        [ 87.0296, 261.4134, 181.7034, 286.7826],
        [ 86.6788, 261.0338, 182.5289, 287.1575],
        [ 87.1021, 261.7832, 181.5485, 286.4646],
        [136.6010, 261.7800, 179.7348, 284.6882],
        [123.8110, 261.7195, 179.4943, 284.5895],
        [ 87.5450, 260.2269, 182.3812, 287.0238],
        [137.4543, 262.3218, 178.2155, 284.7041],
        [139.8743, 263.1362, 180.9698, 285.2491],
        [138.3463, 263.2523, 182.2100, 286.1490],
        [179.4997, 222.7923, 214.3791, 260.7984],
        [186.7064, 235.9968, 226.5901, 268.9597],
        [176.7349, 226.6281, 214.0621, 264.0306],
        [159.0645, 214.1282, 197.9513, 249.3843],
        [162.3829, 214.7112, 202.1357, 252.4546],
        [141.7036, 204.6502, 193.2755, 243.9683],
        [343.7545, 259.8091, 389.0076, 285.5703],
        [343.8968, 260.5422, 382.5041, 284.7157],
        [345.7183, 261.8193, 388.6859, 285.8210],
        [345.6065, 261.9201, 388.9511, 285.7778],
        [345.8854, 261.7182, 388.8839, 285.5664],
        [345.7411, 261.8664, 388.8116, 285.9558],
        [344.7576, 260.6206, 389.3434, 287.0429],
        [345.6553, 261.5767, 389.1196, 285.5386],
        [345.4021, 261.8126, 389.2196, 285.5286],
        [344.0260, 260.4209, 390.9736, 286.2235],
        [346.4036, 261.8402, 388.4408, 285.3022],
        [345.6078, 261.9865, 388.7732, 285.6177],
        [344.7865, 260.3423, 390.2543, 286.7790],
        [346.7761, 260.4796, 390.8409, 285.3382],
        [344.9601, 260.1008, 393.2995, 286.9009],
        [395.0889, 262.3214, 468.8819, 285.9131],
        [394.8073, 262.1695, 472.3721, 286.4110],
        [395.7414, 262.7681, 472.0747, 286.0166],
        [395.5584, 263.2561, 473.2578, 286.6925],
        [396.7199, 262.4330, 471.0774, 285.7683],
        [396.4470, 263.1149, 471.7142, 286.4226],
        [397.5620, 262.5273, 470.7482, 285.7568],
        [396.6105, 262.8198, 471.5081, 286.3641],
        [399.8521, 261.1652, 473.4439, 286.5433],
        [ 85.6566, 261.7060, 132.6101, 287.1834],
        [ 86.1805, 261.4951, 139.0507, 287.8280],
        [ 86.3777, 261.5794, 124.6254, 287.2240],
        [ 86.1752, 261.7063, 135.0201, 287.1593],
        [ 88.6288, 261.3250, 156.5790, 288.4926],
        [ 87.9958, 260.2598, 179.2706, 290.9064],
        [ 86.4594, 261.6038, 132.3044, 286.8711],
        [ 85.4218, 261.6043, 133.1578, 287.2735],
        [ 88.5516, 261.6497, 141.5440, 287.2548],
        [ 87.6237, 260.9172, 176.7113, 287.9902],
        [ 86.7971, 260.7841, 181.5577, 288.2926],
        [ 90.0008, 262.0085, 130.8440, 286.1098],
        [ 90.9894, 261.8672, 159.3498, 287.0627],
        [ 86.6546, 261.4034, 180.8608, 287.6679],
        [ 85.5678, 261.2018, 182.7774, 287.6706],
        [ 86.3684, 261.3737, 182.0034, 287.4339],
        [ 85.9119, 261.3433, 182.2569, 287.2831],
        [ 84.7614, 260.8905, 182.6436, 287.6959],
        [ 85.6774, 260.9151, 182.3885, 287.5406],
        [107.4421, 262.4360, 177.7067, 286.1265],
        [ 86.8067, 261.6602, 182.3108, 286.4398],
        [ 84.3825, 261.5442, 182.1324, 286.5184],
        [134.5865, 262.4234, 170.0006, 285.0043],
        [133.8529, 262.6405, 179.8100, 285.1407],
        [100.6976, 261.7437, 179.6052, 286.7055],
        [ 85.2414, 260.8985, 180.9450, 287.1184],
        [137.7403, 262.5445, 179.4775, 285.1861],
        [135.4067, 261.9396, 179.8092, 284.8146],
        [136.2690, 262.4460, 181.0897, 284.9840],
        [133.5758, 262.4978, 181.0323, 285.0782],
        [104.4259, 260.9689, 180.0508, 287.0803],
        [138.2883, 262.4868, 181.1866, 285.0145],
        [109.3376, 192.0102, 151.6265, 224.9982],
        [345.3745, 262.3252, 389.5100, 286.1235],
        [345.9424, 261.8899, 389.7755, 286.4711],
        [345.4772, 260.8509, 389.7357, 287.3049],
        [345.0694, 261.5171, 387.7205, 286.7193],
        [345.1318, 262.1373, 388.7241, 285.9576],
        [344.9384, 262.1105, 388.7260, 286.0072],
        [343.9941, 261.2834, 389.8608, 286.9984],
        [345.2805, 262.0101, 388.5977, 285.8584],
        [345.0283, 261.9563, 388.7007, 286.2583],
        [343.9064, 260.7346, 389.3882, 287.4873],
        [345.5716, 262.1889, 388.9398, 285.9611],
        [345.3286, 262.1560, 389.1058, 286.1053],
        [344.3368, 261.2700, 390.5203, 287.3007],
        [346.0165, 262.2098, 388.6924, 285.9799],
        [345.5827, 262.0889, 388.9943, 286.2837],
        [344.6957, 260.7767, 390.4995, 287.7625],
        [346.3167, 262.3388, 388.9241, 285.9067],
        [345.9141, 262.0896, 389.0313, 286.2479],
        [345.0938, 260.9887, 392.0776, 287.6253],
        [351.7405, 262.1590, 388.4503, 286.2796],
        [396.6360, 262.4331, 450.5504, 285.9942],
        [395.5190, 262.0160, 464.0126, 286.8440],
        [393.7474, 261.2657, 465.6024, 287.7073],
        [395.3800, 262.2401, 471.0843, 286.7153],
        [395.0706, 262.1987, 475.2807, 286.7859],
        [394.9654, 262.1498, 474.5691, 286.7757],
        [395.3711, 262.4557, 472.7603, 286.8666],
        [395.3145, 262.6890, 473.5201, 286.7359],
        [395.4112, 262.9552, 472.9773, 286.5859],
        [396.2565, 262.8934, 472.3856, 286.5550],
        [396.2570, 262.9042, 472.2652, 286.5037],
        [396.2170, 262.9672, 471.9123, 286.3831],
        [397.3072, 263.1210, 471.3775, 286.4894],
        [396.6263, 263.2300, 471.5999, 286.2566],
        [396.5434, 263.2119, 471.6957, 286.3959],
        [400.6249, 263.3581, 470.3612, 286.7260],
        [398.4891, 262.8824, 470.5813, 286.3182],
        [398.2754, 262.1808, 471.6765, 286.9306],
        [408.6939, 262.6478, 471.5797, 287.6767],
        [ 85.2286, 262.7238, 169.9450, 289.9132],
        [ 84.9390, 262.6750, 178.5504, 289.2409],
        [ 85.4831, 262.6182, 179.9593, 288.5237],
        [ 85.5576, 261.5596, 180.9747, 287.9647],
        [344.9926, 262.4039, 388.1841, 286.4964],
        [343.9085, 261.1796, 389.0244, 287.1562],
        [345.6110, 262.4129, 389.1105, 286.6797],
        [344.5246, 261.5006, 390.2055, 287.2717],
        [393.7469, 261.6125, 472.4429, 288.2007],
        [395.9835, 261.5436, 473.1262, 286.1618],
        [395.8929, 262.2656, 472.1258, 286.1611],
        [399.2107, 263.6363, 470.8983, 287.2401],
        [396.7115, 262.5640, 472.8940, 286.5918],
        [145.3309, 110.5462, 239.0388, 148.6005],
        [134.9470, 112.2836, 208.9248, 144.5197],
        [122.0130, 115.9792, 191.3904, 145.7875],
        [115.1487, 122.4835, 186.6705, 151.4319],
        [105.5314, 130.4254, 185.0890, 160.0074],
        [ 97.5187, 135.8796, 187.7799, 167.7156],
        [ 91.3185, 139.3350, 193.4931, 173.9368],
        [ 88.7436, 144.0244, 204.5142, 182.2771],
        [ 84.5763, 157.4334, 207.8504, 198.8536],
        [ 80.2139, 173.6590, 204.3496, 218.1595],
        [ 74.5236, 190.7773, 195.5833, 236.7741],
        [ 66.8721, 200.8840, 188.5037, 248.9331],
        [ 56.8488, 207.9629, 188.6394, 260.1623],
        [ 18.9124, 142.8951, 204.1982, 181.9877],
        [ 37.7446, 143.8682, 204.2102, 186.0721],
        [ 56.2031, 144.9518, 204.8893, 188.7766],
        [ 63.8664, 148.8127, 201.9497, 193.3071],
        [ 78.8250, 159.8743, 212.4623, 205.4558],
        [104.0803, 175.3138, 237.4090, 222.5059],
        [141.7062, 190.9822, 276.8549, 240.1922],
        [176.4965, 200.6850, 312.3248, 252.3068],
        [198.9307, 204.1224, 333.7989, 255.1768],
        [214.6116, 202.9767, 347.6804, 250.7497],
        [228.0249, 201.5846, 354.8440, 244.3212],
        [240.3258, 200.1193, 360.7406, 238.2327],
        [247.9971, 196.8658, 354.4699, 232.9959],
        [253.2380, 197.0732, 347.9808, 233.8904],
        [243.1062, 196.4661, 331.5101, 234.4300],
        [228.1636, 189.2696, 317.1639, 231.5276],
        [122.1997, 113.2184, 192.5895, 145.8531],
        [117.0682, 116.7395, 192.2698, 149.0319],
        [108.7586, 119.1783, 194.3853, 153.2450],
        [ 82.3378, 119.4702, 211.4911, 163.3298],
        [ 70.4916, 135.3281, 216.6931, 183.8205],
        [ 58.1296, 153.2119, 216.6561, 207.5781],
        [ 47.2687, 174.2719, 208.5955, 231.9099],
        [ 48.4160, 193.6062, 193.0829, 250.2760],
        [ 51.3491, 210.9668, 184.9421, 266.6249],
        [ 57.5084, 217.2217, 188.6176, 276.8447],
        [ 53.3046, 120.7421, 212.0515, 158.8892],
        [ 41.8554, 129.6871, 227.5135, 168.0592],
        [ 35.6549, 139.8237, 235.6647, 179.3158],
        [ 43.2786, 146.8640, 236.0430, 185.8963],
        [ 66.2497, 154.4533, 225.2268, 190.7127],
        [ 86.8405, 158.7301, 219.1615, 192.1604],
        [ 98.7832, 166.8565, 216.9776, 199.1577],
        [108.6745, 177.9300, 228.8878, 212.5742],
        [119.5695, 191.5421, 252.4895, 230.9971],
        [136.7987, 200.4012, 286.4422, 246.5838],
        [164.8634, 205.5809, 318.9569, 257.3550],
        [193.6935, 205.8239, 348.1803, 259.3765],
        [216.1179, 203.1180, 366.5342, 253.4372],
        [230.6768, 200.4553, 376.7155, 245.3559],
        [240.7633, 196.8252, 379.5504, 236.2896],
        [249.5432, 193.6135, 369.3044, 229.5002],
        [258.8553, 195.9932, 359.8202, 230.1492],
        [253.2822, 199.0017, 344.4948, 233.1399],
        [ 87.3686, 344.6478, 183.2510, 369.5937],
        [ 89.6375, 344.3542, 183.6721, 369.3692],
        [275.1650, 345.9764, 328.2038, 365.5595],
        [275.7509, 345.4047, 326.4936, 366.3917],
        [274.6543, 345.3718, 330.1155, 365.2005],
        [333.7131, 342.6186, 360.2012, 359.3286],
        [333.6749, 341.1842, 381.7576, 360.3196],
        [333.6532, 345.2519, 382.0286, 364.5508],
        [338.4509, 345.4499, 373.2888, 363.3702],
        [334.1253, 343.9319, 380.3567, 363.9236],
        [336.5767, 338.8035, 381.4471, 357.7325],
        [335.3425, 345.0690, 383.7530, 364.7695],
        [345.5241, 344.9169, 380.0977, 362.8927],
        [337.2245, 343.8677, 382.1333, 363.6747],
        [362.6997, 344.5855, 383.3629, 360.4461],
        [269.9788, 214.7295, 319.7971, 237.0447],
        [255.0185, 198.5095, 305.7846, 221.7773],
        [259.8114, 195.3030, 310.6258, 218.1607],
        [298.5313, 214.1061, 348.7216, 236.3221],
        [272.1169, 204.8304, 316.7454, 227.6811],
        [266.6782, 211.4525, 312.6080, 235.7068],
        [ 86.8581, 344.8368, 177.0243, 370.9390],
        [ 85.9674, 343.8032, 181.7721, 372.7835],
        [ 86.2958, 345.3680, 189.2784, 370.2368],
        [ 86.6127, 345.0297, 186.4661, 371.1508],
        [ 86.4430, 345.6700, 184.9238, 370.1095],
        [ 85.9548, 345.4597, 186.3862, 371.1675],
        [ 86.0681, 345.7934, 185.0699, 370.2379],
        [ 85.5839, 345.6317, 186.0031, 371.0764],
        [ 86.3103, 345.6411, 183.6538, 370.2009],
        [ 85.3076, 345.4743, 184.6705, 371.1358],
        [ 86.4841, 346.0334, 183.1090, 370.0822],
        [ 86.0990, 345.7655, 184.2989, 370.9620],
        [ 88.3216, 345.1856, 183.8595, 370.2786],
        [ 84.2847, 344.3251, 185.4608, 371.5126],
        [ 98.5895, 344.7508, 184.6046, 370.2225],
        [ 86.1267, 342.9312, 187.1669, 371.9132],
        [274.8869, 345.4985, 313.9215, 366.0052],
        [272.9022, 344.0318, 321.3861, 367.4315],
        [274.1433, 345.9153, 300.9494, 365.5120],
        [276.1260, 345.6399, 326.0026, 365.5465],
        [275.7028, 345.7037, 325.9156, 365.9185],
        [275.9097, 345.2512, 318.6876, 365.8542],
        [276.0724, 346.2498, 325.9481, 365.1347],
        [275.8991, 346.1931, 327.2764, 365.0887],
        [274.6979, 345.1355, 329.4232, 366.3057],
        [277.8865, 345.5400, 322.3049, 365.7274],
        [275.5667, 345.1111, 326.4861, 366.5554],
        [275.8946, 346.3783, 328.5977, 365.0347],
        [275.7788, 346.2052, 329.0645, 365.1206],
        [274.5619, 345.0617, 331.2206, 366.4878],
        [279.6641, 345.9365, 325.2673, 365.4614],
        [277.9096, 346.1851, 329.9205, 365.0891],
        [276.1143, 345.5609, 331.7606, 365.8771],
        [300.9462, 346.4388, 328.2908, 364.5000],
        [330.0018, 346.6060, 363.3960, 366.1976],
        [334.3513, 346.9029, 373.1167, 365.6797],
        [333.1187, 346.4228, 377.6971, 366.3021],
        [335.2150, 347.1511, 363.9145, 365.7003],
        [335.8822, 346.6082, 378.4187, 365.2836],
        [335.2239, 346.3239, 380.0534, 365.3279],
        [337.9002, 346.6714, 374.3036, 365.5245],
        [334.8206, 345.6858, 380.2446, 366.2395],
        [338.5731, 346.3109, 381.4831, 364.6951],
        [336.7579, 346.3075, 382.4302, 364.9517],
        [340.4204, 345.9380, 380.5773, 364.9862],
        [338.2105, 345.5615, 382.3323, 365.8598],
        [342.0407, 345.9740, 381.6820, 364.2197],
        [340.0674, 345.2679, 391.9269, 364.8875],
        [357.5462, 345.7156, 382.2811, 362.7184],
        [357.1270, 341.7289, 395.5538, 357.8609],
        [343.9306, 344.6012, 425.9437, 363.6339],
        [337.9890, 310.1262, 396.5450, 327.7263],
        [290.7906, 234.1894, 336.1934, 253.0536],
        [276.2438, 217.6833, 325.0055, 238.0502],
        [278.7304, 213.3409, 327.5203, 234.5887],
        [297.3465, 221.0079, 345.2650, 242.3477],
        [272.8511, 214.0041, 315.9554, 235.7369],
        [272.6780, 220.3649, 317.2357, 244.5809],
        [ 84.9860, 344.8528, 179.5571, 375.1299],
        [ 85.7027, 345.7553, 185.2903, 371.8179],
        [ 86.2129, 345.0805, 184.7434, 371.9344],
        [ 86.9801, 346.2766, 182.9205, 371.4256],
        [ 85.6567, 345.1774, 186.9255, 371.4224],
        [ 87.4627, 346.6316, 184.2237, 371.4012],
        [ 86.2830, 345.6859, 186.3343, 371.2346],
        [ 88.0537, 346.6350, 182.3380, 371.3144],
        [ 85.9454, 345.6778, 184.3890, 371.4399],
        [ 88.9996, 346.8560, 182.0690, 371.0627],
        [ 86.8585, 345.9160, 183.7029, 371.2542],
        [ 91.5543, 346.4661, 183.5366, 371.3416],
        [ 87.8248, 345.2335, 185.2370, 371.8886],
        [275.6865, 347.5210, 309.4301, 366.6720],
        [273.9688, 345.3097, 320.7375, 368.0921],
        [275.7232, 347.0369, 325.0679, 366.6409],
        [275.3129, 346.0475, 327.1249, 365.9278],
        [273.6395, 345.0149, 329.9748, 367.1946],
        [275.7490, 345.4009, 325.0734, 366.7851],
        [275.8828, 346.2349, 329.7156, 366.0494],
        [274.6061, 345.0425, 332.6235, 366.8600],
        [277.7406, 345.7437, 328.7975, 366.9852],
        [334.0639, 348.5885, 361.1711, 366.1660],
        [333.5627, 346.6817, 380.2747, 367.0246],
        [334.5582, 346.1211, 378.5829, 366.9690],
        [335.8669, 346.6144, 383.8037, 366.8524],
        [337.7897, 345.8168, 382.3252, 366.4950],
        [ 85.0210, 391.8192, 181.4117, 417.6493],
        [ 86.2496, 391.2658, 181.7924, 417.2401],
        [ 87.3782, 390.7791, 182.9739, 416.6224],
        [389.0892, 392.9492, 474.4005, 417.4519],
        [389.8250, 392.7077, 475.2260, 417.7350],
        [390.5832, 392.2434, 475.9428, 417.5661],
        [ 88.0475, 391.4075, 172.3638, 419.2178],
        [ 86.8912, 390.7097, 183.1397, 421.6864],
        [ 87.0441, 391.9272, 186.1408, 418.7027],
        [ 86.3911, 391.7241, 186.8623, 420.0098],
        [ 86.3925, 392.2550, 182.8986, 418.1960],
        [ 85.5213, 391.8602, 184.9746, 419.1771],
        [ 86.7271, 392.2199, 181.9249, 418.1114],
        [ 85.6657, 391.8454, 183.1880, 418.6868],
        [ 86.4825, 392.4221, 182.3950, 418.2082],
        [ 85.4309, 392.1337, 183.5708, 418.9064],
        [ 86.0763, 392.9645, 182.9100, 417.7545],
        [ 85.8946, 392.8203, 183.5695, 418.5077],
        [ 86.6560, 392.3003, 184.2076, 418.1906],
        [ 83.1062, 391.5676, 185.0454, 419.1540],
        [127.4008, 392.9526, 182.3468, 417.2206],
        [ 91.1692, 390.2938, 184.0117, 419.6870],
        [388.7258, 393.2469, 460.8777, 417.9217],
        [388.7762, 392.6034, 476.7139, 418.6180],
        [387.6853, 391.9773, 475.1331, 419.5417],
        [389.8033, 393.6147, 470.0187, 417.7334],
        [388.9718, 393.5947, 473.6794, 418.7685],
        [388.4413, 393.6043, 474.8713, 419.1562],
        [391.3658, 393.4642, 470.9606, 417.3313],
        [389.0453, 393.6333, 476.1096, 418.5872],
        [388.6763, 393.6739, 476.6282, 418.7945],
        [391.8987, 393.4109, 471.4182, 417.0272],
        [389.0139, 393.4263, 475.6458, 418.7496],
        [388.9767, 393.3918, 476.1665, 419.0525],
        [394.7085, 393.2032, 472.1891, 416.6124],
        [388.7330, 393.4027, 475.9316, 418.3595],
        [388.9381, 393.4384, 476.3464, 418.7303],
        [395.8958, 393.0854, 473.3517, 416.9285],
        [389.7856, 393.2010, 475.9849, 418.1694],
        [388.7818, 392.6185, 476.6735, 419.2582],
        [397.9584, 392.8499, 475.2685, 417.6750],
        [389.7050, 391.1722, 477.2878, 419.4236],
        [ 85.4497, 392.0512, 182.3111, 423.3148],
        [ 86.6354, 393.1423, 180.8802, 420.1842],
        [ 86.5139, 392.2619, 185.0084, 420.4898],
        [ 85.9153, 392.3116, 185.5889, 419.2481],
        [ 88.0377, 393.8241, 180.4293, 418.9864],
        [ 86.3810, 392.5144, 183.2507, 418.8316],
        [ 88.0988, 393.6560, 180.4993, 418.6522],
        [ 86.7166, 392.7544, 183.5883, 418.8722],
        [ 90.0100, 393.9556, 180.8297, 418.2927],
        [ 87.5024, 393.0273, 183.2094, 418.7434],
        [ 97.2253, 394.3347, 183.2630, 418.4927],
        [ 88.6814, 392.9847, 184.6531, 418.9863],
        [ 99.4600, 392.8532, 183.6901, 420.6660],
        [278.9503, 342.1385, 337.0021, 379.2654],
        [291.0977, 340.8291, 353.2288, 376.2788],
        [288.7437, 340.6597, 352.5865, 375.7222],
        [298.0482, 344.1582, 354.1968, 377.4969],
        [304.0060, 354.0763, 365.8972, 388.3362],
        [386.9320, 392.7228, 474.7348, 420.5791],
        [388.6790, 393.6614, 474.5508, 419.3343],
        [388.4295, 393.0506, 476.1790, 419.2848],
        [389.2299, 394.0544, 475.4077, 419.1812],
        [388.4067, 393.7065, 476.6136, 418.8978],
        [389.5987, 394.2091, 474.9135, 419.3255],
        [388.7237, 393.7319, 476.2682, 419.0888],
        [390.3885, 394.4005, 475.3370, 419.1075],
        [388.7279, 393.5770, 476.7498, 418.6067],
        [391.5850, 394.3824, 476.2707, 419.4084],
        [389.6796, 393.4921, 476.9870, 419.6020],
        [ 88.0010, 438.6632, 179.9657, 465.4870],
        [ 87.9846, 438.9665, 182.0035, 465.4287],
        [ 88.2349, 438.4856, 182.9052, 465.1753],
        [ 89.5316, 438.4918, 182.8666, 464.4456],
        [340.0736, 439.8738, 424.4028, 463.8168],
        [340.8059, 440.0324, 425.1205, 463.8088],
        [340.5474, 439.6244, 425.3036, 464.1585],
        [340.8380, 439.1013, 426.5979, 464.2423],
        [431.1369, 440.7939, 477.2778, 462.2872],
        [433.0493, 440.1533, 478.3543, 461.8962],
        [ 86.8064, 441.1029, 127.4668, 465.7195],
        [ 87.0528, 440.6368, 133.7205, 466.3009],
        [ 87.1611, 440.7556, 120.4142, 465.5869],
        [ 87.2949, 441.0490, 133.2160, 465.5595],
        [ 89.4130, 440.3928, 145.5999, 466.0355],
        [ 89.0972, 440.8965, 138.2374, 465.0927],
        [ 89.1196, 440.0576, 171.2962, 466.3735],
        [ 87.2045, 439.7118, 182.3847, 467.3622],
        [ 87.6529, 440.6491, 180.3385, 466.3246],
        [ 86.6065, 440.4113, 183.4387, 466.9301],
        [ 88.5798, 440.4410, 181.6359, 466.3793],
        [ 87.5921, 440.1534, 182.6570, 466.6306],
        [ 89.9139, 440.3265, 182.0499, 466.6299],
        [ 89.6499, 440.1324, 183.2320, 466.8347],
        [117.3307, 441.1529, 179.0093, 464.6017],
        [ 88.4261, 440.6917, 183.8014, 466.1744],
        [ 88.3791, 440.4731, 183.3068, 466.1878],
        [111.3457, 440.7230, 181.4122, 465.7902],
        [ 89.0993, 439.6835, 182.8543, 466.9341],
        [133.4583, 440.9734, 183.4704, 465.2993],
        [340.5718, 440.4939, 416.1429, 464.5966],
        [340.3099, 439.9526, 422.3788, 464.8998],
        [339.8345, 439.5638, 423.1839, 465.5763],
        [340.4550, 440.4976, 422.4846, 464.3311],
        [340.0055, 440.5227, 425.0131, 464.4655],
        [339.8391, 440.5188, 425.5156, 464.4816],
        [341.8341, 440.4279, 424.5643, 464.0674],
        [339.7129, 440.4283, 426.4063, 464.4489],
        [340.0193, 440.5714, 426.2517, 464.3807],
        [342.8931, 440.5885, 424.1372, 463.8832],
        [339.7506, 440.2892, 425.9966, 464.5985],
        [339.6225, 440.2737, 425.9724, 464.5565],
        [343.3533, 440.3951, 425.1064, 463.8829],
        [339.1133, 440.1943, 426.8608, 464.6560],
        [339.5489, 440.0114, 427.0240, 464.8058],
        [351.8027, 440.2894, 424.9022, 464.3586],
        [341.7174, 439.7153, 426.3391, 465.5842],
        [339.8010, 438.7753, 426.2903, 466.3391],
        [353.2915, 439.6631, 426.3195, 466.2902],
        [431.0114, 443.3427, 476.8143, 461.8677],
        [429.3916, 442.4989, 477.2522, 463.0101],
        [432.1374, 442.8197, 466.5618, 462.4632],
        [432.7419, 443.5265, 477.1430, 461.8002],
        [431.9246, 443.1066, 477.8690, 462.4235],
        [433.3173, 442.8300, 476.6271, 462.3333],
        [431.8452, 441.7662, 477.5468, 463.5710],
        [433.5903, 443.4458, 477.5225, 461.7463],
        [432.6114, 442.8297, 478.2775, 462.5157],
        [433.6480, 442.5906, 477.5390, 462.5290],
        [433.2336, 441.5422, 478.5706, 463.9091],
        [434.2589, 442.5391, 477.4745, 462.3928],
        [433.4678, 441.5959, 478.8174, 463.4277],
        [ 86.2105, 440.7759, 131.1497, 464.9586],
        [ 87.2419, 440.3545, 177.1179, 467.2180],
        [ 86.7805, 440.4798, 180.9692, 466.2327],
        [ 87.3468, 440.4521, 181.3109, 466.0486],
        [ 89.2555, 440.7233, 182.2982, 466.2395],
        [ 89.7915, 440.6115, 182.6758, 465.6407],
        [ 98.5885, 440.6519, 182.3343, 466.2996],
        [338.4379, 440.0815, 421.6997, 466.4538],
        [339.2486, 440.8063, 422.8925, 464.6961],
        [339.1683, 440.1114, 425.5771, 464.3082],
        [338.9901, 440.4600, 424.2954, 463.5732],
        [339.3001, 440.3602, 426.0987, 463.5009],
        [339.1489, 440.1252, 424.1992, 462.9080],
        [339.1858, 440.5942, 425.8784, 463.8494],
        [340.5420, 440.0757, 425.3074, 463.0075],
        [339.2529, 440.3732, 427.3669, 464.5078],
        [342.2406, 439.9585, 427.8939, 465.7519],
        [432.3468, 444.5504, 477.0427, 463.4055],
        [431.7207, 443.7023, 478.2050, 463.2355],
        [433.6292, 444.1052, 475.4257, 463.5220],
        [432.0806, 442.6471, 477.6072, 463.9404],
        [433.6457, 444.5548, 477.5194, 463.4919],
        [432.8525, 443.8432, 478.4249, 463.7361],
        [435.4164, 444.0365, 476.6516, 463.6368],
        [433.3797, 442.6252, 478.3538, 464.1943],
        [276.3603, 167.5948, 370.0816, 189.3530]])

    boxlist = BoxList(detections, per_anchors.size, mode="xyxy") { // CALL
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([824, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([824, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    } boxlist = BoxList(detections, per_anchors.size, mode="xyxy") // RETURNED

    // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
    // boxlist: BoxList(num_boxes=824, image_width=561, image_height=480, mode=xyxy)

    boxlist.add_field("labels", per_class)
    boxlist.add_field("scores", per_box_cls)
    boxlist = boxlist.clip_to_image(remove_empty=False)
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([824, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([824, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([824, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([824, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    boxlist = remove_small_boxes(boxlist, self.min_size)
    results.append(boxlist)
  } // END for per_box_cls, per_box_regression, ... in zip():

  return results

} // END RetinaNetPostProcessor.forward_for_single_feature_map()
          }
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) // RETURNED

          } // END of for a, o, b in zip() iteration: 1/5

          {
          # BEGIN for a, o, b in zip()  iteration: 2/5
          # a: (BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy),)
          # o.shape: torch.Size([1, 9, 30, 36])
          # b.shape: torch.Size([1, 36, 30, 36])

          # 2-3-2-3-2 self.forward_for_single_feature_map
          // self.forward_for_single_feature_map: <bound method RetinaNetPostProcessor.forward_for_single_feature_map of RetinaNetPostProcessor()>
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) { // CALL
RetinaNetPostProcessor.forward_for_single_feature_map() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/inference.py

  // Params:
    > anchors: list[BoxList]
    > box_cls: tensor of size N, A*C, H, W)
    > box_regression: tensor of size N, A*4, H, W)

  device = box_cls.device
  // device: cuda:0

  N, _, H, W = box_cls.shape
  // N:1, H:30, W:36

  A = box_regression.size(1) // 4
  // A : 9

  C = box_cls.size(1) // A
  // C: 1

  # put 'box_cls' in the same format as anchors
  box_cls = permute_and_flatten(box_cls, N, A, C, H, W)
  // box_cls.shape: torch.Size([1, 9720, 1])

  box_cls = box_cls.sigmoid()
  // box_cls.shape: torch.Size([1, 9720, 1])

  # put 'box_cls' in the same format as anchors
  box_regression = permute_and_flatten(box_regression, N, A, 4, H, W)
  // box_regression.shape: torch.Size([1, 9720, 4])

  box_regression = box_regression.reshape(N, -1, 4)
  // box_regression.shape: torch.Size([1, 9720, 4])

  num_anchors = A * H * W
  // num_anchors: 9720

  candidate_inds = box_cls > self.pre_nms_thresh
  // candidate_inds.shape: torch.Size([1, 9720, 1])

  pre_nms_top_n = candidate_inds.view(N, -1).sum(1)
  // pre_nms_top_n: tensor([94], device='cuda:0')

  pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)
  // pre_nms_top_n: tensor([94], device='cuda:0')

  results = []

  // box_cls.shape: torch.Size([1, 9720, 1])
  // box_regression.shape: torch.Size([1, 9720, 4])
  // pre_nmns_top_n: tensor([94], device='cuda:0')
  // candidate.inds.shape: torch.Size([1, 9720, 1])
  // anchors: (BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy),)
  for per_box_cls, per_box_regression, ... in zip():
  {
    # ====================================
    # per_box_cls.shape: torch.Size([9720, 1])
    # type(per_box_cls): <class 'torch.Tensor'>
    # per_box_regression.shape: torch.Size([9720, 4])
    # per_pre_nms_top_n: 94
    # per_candidate_inds.shape: torch.Size([9720, 1])
    # per_anchors: BoxList(num_boxes=9720, image_width=561, image_height=480, mode=xyxy)
    # ====================================

    per_box_cls = per_box_cls[per_candidate_inds]
    per_box_cls, top_k_indices =per_box_cls.topk(per_pre_nms_top_n, sorted=False)
    // per_box_cls: tensor([0.1345, 0.9036, 0.4656, 0.1300, 0.8047, 0.1503, 0.0956, 0.8720, 0.3230,
        0.1303, 0.8717, 0.1617, 0.5510, 0.0715, 0.0541, 0.0897, 0.7447, 0.3094,
        0.0537, 0.0566, 0.1238, 0.7576, 0.2213, 0.0879, 0.2428, 0.3263, 0.0613,
        0.2942, 0.0689, 0.2982, 0.0671, 0.0550, 0.5420, 0.9158, 0.2148, 0.5423,
        0.3299, 0.5127, 0.5528, 0.0572, 0.5661, 0.1425, 0.0668, 0.0529, 0.0876,
        0.6581, 0.0832, 0.0866, 0.0637, 0.1982, 0.1229, 0.3017, 0.8022, 0.0926,
        0.3010, 0.0561, 0.1815, 0.0854, 0.0511, 0.4907, 0.0733, 0.0662, 0.0610,
        0.1889, 0.1799, 0.6530, 0.0768, 0.1748, 0.2070, 0.3694, 0.1945, 0.2376,
        0.8411, 0.0549, 0.3183, 0.2164, 0.1173, 0.1433, 0.5896, 0.0552, 0.1872,
        0.6098, 0.7594, 0.1827, 0.0510, 0.2178, 0.1112, 0.2363, 0.5608, 0.1068,
        0.2481, 0.5955, 0.4249, 0.0503])
    // top_k_indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,
        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
        54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,
        72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,
        90, 92, 93, 91])

    per_candidate_nonzeros = \
       per_candidate_inds.nonzero()[top_k_indices, :]
    // per_candidate_inds.shape: torch.Size([9720, 1])

    per_box_loc = per_candidate_nonzeros[:, 0]
    // per_box_loc.shape: torch.Size([94])

    per_class = per_candidate_nonzeros[:, 1]
    // per_class.shape: torch.Size([94])

    per_class += 1

    detections = self.box_coder.decode( ) { // CALL
    BoxCoder.decode(self, rel_codes, boxes) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/box_coder.py
      // Params:
        rel_codes.shape: torch.Size([94, 4])
        boxes.shape: torch.Size([94, 4])

      boxes = boxes.to(rel_codes.dtype)
      // boxes.shape: torch.Size([94, 4])

      TO_REMOVE = 1  # TODO remove

      widths = boxes[:, 2] - boxes[:, 0] + TO_REMOVE
      // widths.shape: torch.Size([94])

      heights = boxes[:, 3] - boxes[:, 1] + TO_REMOVE
      // heights.shape: torch.Size([94])

      ctr_x = boxes[:, 0] + 0.5 * widths
      // ctr_x.shape: torch.Size([94])

      ctr_y = boxes[:, 1] + 0.5 * heights
      // ctr_y.shape: torch.Size([94])


      wx, wy, ww, wh = self.weights
      // wx: 10.0, wy: 10.0, ww: 5.0, wh: 5.0

      dx = rel_codes[:, 0::4] / wx
      // dx.shape: torch.Size([94, 1])

      dy = rel_codes[:, 1::4] / wy
      // dy.shape: torch.Size([94, 1])

      dw = rel_codes[:, 2::4] / ww
      // dw.shape: torch.Size([94, 1])

      dh = rel_codes[:, 3::4] / wh
      // dh.shape: torch.Size([94, 1])


      # Prevent sending too large values into torch.exp()
      dw = torch.clamp(dw, max=self.bbox_xform_clip)
      // dw.shape: torch.Size([94, 1])

      dh = torch.clamp(dh, max=self.bbox_xform_clip)
      // dh.shape: torch.Size([94, 1])

      pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
      // pred_ctr_x.shape: torch.Size([94, 1])

      pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
      // pred_ctr_y.shape: torch.Size([94, 1])

      pred_w = torch.exp(dw) * widths[:, None]
      // pred_w.shape: torch.Size([94, 1])

      pred_h = torch.exp(dh) * heights[:, None]
      // pred_h.shape: torch.Size([94, 1])


      pred_boxes = torch.zeros_like(rel_codes)
      // pred_boxes.shape: torch.Size([94, 4])


      # x1
      pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w

      # y1
      pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h

      # x2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1

      # y2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1


      pred_boxes.shape: torch.Size([94, 4])

      return pred_boxes

    } // END BoxCoder.decode(self, rel_codes, boxes)

    } detections = self.box_coder.decode( ) // RETURNED

    // type(detections): <class 'torch.Tensor'>
    // detections: tensor([[187.7698,  46.9255, 277.8330,  77.7009],
        [189.3434,  49.4835, 278.1791,  76.3865],
        [189.1159,  48.4543, 278.5117,  77.2294],
        [283.2841,  48.4486, 372.1926,  77.7526],
        [284.6645,  49.5578, 372.4418,  76.4505],
        [285.5008,  47.9117, 373.3582,  76.9188],
        [187.5619,  48.6241, 278.5428,  79.4149],
        [190.1614,  49.4306, 278.4030,  75.9757],
        [189.3392,  49.1784, 280.6287,  78.0784],
        [280.7638,  48.5657, 371.0868,  78.4935],
        [285.1133,  49.0682, 372.6497,  75.9045],
        [286.0146,  48.5692, 373.2711,  77.8653],
        [239.7191,  82.5362, 321.6304, 113.3811],
        [239.2612,  81.2841, 320.0988, 115.1622],
        [240.9865,  81.2244, 324.9697, 112.7591],
        [240.4226,  83.0574, 322.7343, 114.4046],
        [240.2540,  83.3737, 321.8690, 113.0687],
        [240.5661,  81.9677, 324.3827, 113.6874],
        [411.5912, 170.3924, 455.8348, 221.3199],
        [437.3890, 182.9685, 480.8045, 231.0632],
        [ 84.1894, 163.4035, 185.7175, 193.3785],
        [ 85.7353, 165.8925, 185.9964, 193.9657],
        [ 84.7756, 164.5183, 186.9007, 196.0748],
        [ 85.9160, 163.4611, 186.7238, 193.5863],
        [263.1984, 165.2426, 409.3178, 189.7323],
        [265.2646, 166.5369, 410.6508, 190.1833],
        [261.5188, 163.8930, 412.3414, 193.4174],
        [265.0626, 166.5643, 411.4380, 190.0431],
        [261.6717, 164.3763, 412.8316, 193.3463],
        [265.2574, 165.9680, 412.2213, 189.7988],
        [292.2971, 163.9689, 410.8403, 189.6240],
        [458.5660, 183.3736, 502.2656, 233.8997],
        [ 85.2531, 164.3982, 186.1423, 194.4035],
        [ 85.7899, 165.0676, 185.9036, 193.3730],
        [ 85.0118, 163.6471, 186.4815, 195.2982],
        [ 85.3311, 164.1955, 185.6558, 194.6009],
        [261.4498, 166.3172, 413.8255, 191.6945],
        [263.8688, 166.6416, 412.1081, 191.4771],
        [263.6052, 166.4700, 411.9356, 191.2189],
        [262.2835, 164.0439, 412.3903, 193.3834],
        [262.5627, 165.8141, 411.9416, 190.8469],
        [280.4725, 164.5947, 410.7241, 191.7022],
        [470.4424, 182.7051, 515.4810, 239.7024],
        [474.0451, 187.9186, 519.8169, 247.9307],
        [ 84.3340, 211.0780, 182.9138, 240.1492],
        [ 86.2441, 213.6267, 184.4768, 240.2641],
        [ 85.5062, 212.1066, 185.6162, 242.0844],
        [ 85.8916, 210.8293, 185.1441, 239.5058],
        [367.0221, 208.3553, 479.0656, 235.9722],
        [368.3114, 209.1645, 479.4719, 235.5297],
        [368.8083, 208.0458, 480.1084, 236.2677],
        [ 85.1548, 212.3560, 183.6012, 241.5646],
        [ 86.1576, 213.1381, 183.5388, 240.0961],
        [ 85.1330, 211.7696, 184.4511, 242.0759],
        [ 85.6975, 211.8527, 184.2807, 241.3036],
        [367.6271, 209.0610, 477.8727, 235.8221],
        [369.5889, 209.7024, 479.0498, 234.6037],
        [370.6907, 208.9844, 480.6426, 236.4766],
        [ 84.8441, 257.7629, 182.1506, 287.8442],
        [ 86.1399, 260.6571, 183.5299, 287.8147],
        [ 84.8728, 258.6430, 184.8499, 289.4891],
        [ 87.3283, 258.0323, 183.8525, 287.0706],
        [390.3381, 259.2419, 474.3709, 289.4019],
        [394.8950, 260.3250, 474.3170, 289.4285],
        [ 85.2886, 259.1924, 182.3535, 288.8804],
        [ 86.4656, 260.5362, 182.2331, 287.5551],
        [ 85.1844, 258.7988, 183.4149, 289.7260],
        [ 86.5480, 259.2727, 182.9919, 288.6504],
        [392.5992, 260.4286, 473.3945, 289.5997],
        [394.9281, 261.1599, 473.2119, 289.1755],
        [ 87.6587, 343.1876, 184.5458, 369.3659],
        [ 85.2312, 343.8962, 184.5105, 372.4033],
        [ 86.4746, 345.6485, 184.7059, 371.2781],
        [ 85.2603, 343.2021, 185.6091, 373.5878],
        [ 86.3627, 344.5987, 184.3569, 372.6037],
        [ 87.0790, 391.0248, 184.6134, 417.2079],
        [390.1253, 390.3607, 479.1086, 418.1123],
        [ 85.6343, 391.2431, 183.7561, 419.7229],
        [ 86.9227, 392.7870, 183.6457, 418.5193],
        [ 85.8001, 390.8651, 184.6979, 420.6162],
        [ 86.6150, 391.4546, 183.0333, 419.5503],
        [387.9162, 391.9261, 477.2711, 419.2252],
        [389.3057, 392.5693, 477.9179, 418.7062],
        [ 84.8133, 438.0739, 184.7539, 464.2397],
        [ 82.3041, 436.3583, 186.5368, 466.6979],
        [337.1081, 434.9932, 426.1758, 464.4104],
        [338.9463, 434.1652, 429.8907, 463.0970],
        [ 83.7225, 438.5392, 183.5918, 467.6612],
        [ 84.7951, 439.8958, 183.2926, 466.6231],
        [ 83.2914, 438.3945, 184.6448, 469.0623],
        [ 83.2211, 438.7009, 182.3056, 467.4387],
        [339.2164, 438.6003, 425.0907, 465.9252],
        [339.8734, 438.3979, 428.1830, 465.4457],
        [335.7524, 430.1598, 422.0422, 461.3536]])

    boxlist = BoxList(detections, per_anchors.size, mode="xyxy") { // CALL
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([94, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([94, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    } boxlist = BoxList(detections, per_anchors.size, mode="xyxy") // RETURNED

    // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
    // boxlist: BoxList(num_boxes=94, image_width=561, image_height=480, mode=xyxy)

    boxlist.add_field("labels", per_class)
    boxlist.add_field("scores", per_box_cls)
    boxlist = boxlist.clip_to_image(remove_empty=False)
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([94, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([94, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([94, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([94, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    boxlist = remove_small_boxes(boxlist, self.min_size)
    results.append(boxlist)
  } // END for per_box_cls, per_box_regression, ... in zip():

  return results

} // END RetinaNetPostProcessor.forward_for_single_feature_map()
          }
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) // RETURNED

          } // END of for a, o, b in zip() iteration: 2/5

          {
          # BEGIN for a, o, b in zip()  iteration: 3/5
          # a: (BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy),)
          # o.shape: torch.Size([1, 9, 15, 18])
          # b.shape: torch.Size([1, 36, 15, 18])

          # 2-3-2-3-2 self.forward_for_single_feature_map
          // self.forward_for_single_feature_map: <bound method RetinaNetPostProcessor.forward_for_single_feature_map of RetinaNetPostProcessor()>
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) { // CALL
RetinaNetPostProcessor.forward_for_single_feature_map() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/inference.py

  // Params:
    > anchors: list[BoxList]
    > box_cls: tensor of size N, A*C, H, W)
    > box_regression: tensor of size N, A*4, H, W)

  device = box_cls.device
  // device: cuda:0

  N, _, H, W = box_cls.shape
  // N:1, H:15, W:18

  A = box_regression.size(1) // 4
  // A : 9

  C = box_cls.size(1) // A
  // C: 1

  # put 'box_cls' in the same format as anchors
  box_cls = permute_and_flatten(box_cls, N, A, C, H, W)
  // box_cls.shape: torch.Size([1, 2430, 1])

  box_cls = box_cls.sigmoid()
  // box_cls.shape: torch.Size([1, 2430, 1])

  # put 'box_cls' in the same format as anchors
  box_regression = permute_and_flatten(box_regression, N, A, 4, H, W)
  // box_regression.shape: torch.Size([1, 2430, 4])

  box_regression = box_regression.reshape(N, -1, 4)
  // box_regression.shape: torch.Size([1, 2430, 4])

  num_anchors = A * H * W
  // num_anchors: 2430

  candidate_inds = box_cls > self.pre_nms_thresh
  // candidate_inds.shape: torch.Size([1, 2430, 1])

  pre_nms_top_n = candidate_inds.view(N, -1).sum(1)
  // pre_nms_top_n: tensor([1], device='cuda:0')

  pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)
  // pre_nms_top_n: tensor([1], device='cuda:0')

  results = []

  // box_cls.shape: torch.Size([1, 2430, 1])
  // box_regression.shape: torch.Size([1, 2430, 4])
  // pre_nmns_top_n: tensor([1], device='cuda:0')
  // candidate.inds.shape: torch.Size([1, 2430, 1])
  // anchors: (BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy),)
  for per_box_cls, per_box_regression, ... in zip():
  {
    # ====================================
    # per_box_cls.shape: torch.Size([2430, 1])
    # type(per_box_cls): <class 'torch.Tensor'>
    # per_box_regression.shape: torch.Size([2430, 4])
    # per_pre_nms_top_n: 1
    # per_candidate_inds.shape: torch.Size([2430, 1])
    # per_anchors: BoxList(num_boxes=2430, image_width=561, image_height=480, mode=xyxy)
    # ====================================

    per_box_cls = per_box_cls[per_candidate_inds]
    per_box_cls, top_k_indices =per_box_cls.topk(per_pre_nms_top_n, sorted=False)
    // per_box_cls: tensor([0.0535])
    // top_k_indices: tensor([0])

    per_candidate_nonzeros = \
       per_candidate_inds.nonzero()[top_k_indices, :]
    // per_candidate_inds.shape: torch.Size([2430, 1])

    per_box_loc = per_candidate_nonzeros[:, 0]
    // per_box_loc.shape: torch.Size([1])

    per_class = per_candidate_nonzeros[:, 1]
    // per_class.shape: torch.Size([1])

    per_class += 1

    detections = self.box_coder.decode( ) { // CALL
    BoxCoder.decode(self, rel_codes, boxes) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/box_coder.py
      // Params:
        rel_codes.shape: torch.Size([1, 4])
        boxes.shape: torch.Size([1, 4])

      boxes = boxes.to(rel_codes.dtype)
      // boxes.shape: torch.Size([1, 4])

      TO_REMOVE = 1  # TODO remove

      widths = boxes[:, 2] - boxes[:, 0] + TO_REMOVE
      // widths.shape: torch.Size([1])

      heights = boxes[:, 3] - boxes[:, 1] + TO_REMOVE
      // heights.shape: torch.Size([1])

      ctr_x = boxes[:, 0] + 0.5 * widths
      // ctr_x.shape: torch.Size([1])

      ctr_y = boxes[:, 1] + 0.5 * heights
      // ctr_y.shape: torch.Size([1])


      wx, wy, ww, wh = self.weights
      // wx: 10.0, wy: 10.0, ww: 5.0, wh: 5.0

      dx = rel_codes[:, 0::4] / wx
      // dx.shape: torch.Size([1, 1])

      dy = rel_codes[:, 1::4] / wy
      // dy.shape: torch.Size([1, 1])

      dw = rel_codes[:, 2::4] / ww
      // dw.shape: torch.Size([1, 1])

      dh = rel_codes[:, 3::4] / wh
      // dh.shape: torch.Size([1, 1])


      # Prevent sending too large values into torch.exp()
      dw = torch.clamp(dw, max=self.bbox_xform_clip)
      // dw.shape: torch.Size([1, 1])

      dh = torch.clamp(dh, max=self.bbox_xform_clip)
      // dh.shape: torch.Size([1, 1])

      pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
      // pred_ctr_x.shape: torch.Size([1, 1])

      pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
      // pred_ctr_y.shape: torch.Size([1, 1])

      pred_w = torch.exp(dw) * widths[:, None]
      // pred_w.shape: torch.Size([1, 1])

      pred_h = torch.exp(dh) * heights[:, None]
      // pred_h.shape: torch.Size([1, 1])


      pred_boxes = torch.zeros_like(rel_codes)
      // pred_boxes.shape: torch.Size([1, 4])


      # x1
      pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w

      # y1
      pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h

      # x2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1

      # y2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1


      pred_boxes.shape: torch.Size([1, 4])

      return pred_boxes

    } // END BoxCoder.decode(self, rel_codes, boxes)

    } detections = self.box_coder.decode( ) // RETURNED

    // type(detections): <class 'torch.Tensor'>
    // detections: tensor([[327.2357, 352.8868, 454.5999, 405.2030]])

    boxlist = BoxList(detections, per_anchors.size, mode="xyxy") { // CALL
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([1, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([1, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    } boxlist = BoxList(detections, per_anchors.size, mode="xyxy") // RETURNED

    // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
    // boxlist: BoxList(num_boxes=1, image_width=561, image_height=480, mode=xyxy)

    boxlist.add_field("labels", per_class)
    boxlist.add_field("scores", per_box_cls)
    boxlist = boxlist.clip_to_image(remove_empty=False)
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([1, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([1, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([1, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([1, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    boxlist = remove_small_boxes(boxlist, self.min_size)
    results.append(boxlist)
  } // END for per_box_cls, per_box_regression, ... in zip():

  return results

} // END RetinaNetPostProcessor.forward_for_single_feature_map()
          }
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) // RETURNED

          } // END of for a, o, b in zip() iteration: 3/5

          {
          # BEGIN for a, o, b in zip()  iteration: 4/5
          # a: (BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy),)
          # o.shape: torch.Size([1, 9, 8, 9])
          # b.shape: torch.Size([1, 36, 8, 9])

          # 2-3-2-3-2 self.forward_for_single_feature_map
          // self.forward_for_single_feature_map: <bound method RetinaNetPostProcessor.forward_for_single_feature_map of RetinaNetPostProcessor()>
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) { // CALL
RetinaNetPostProcessor.forward_for_single_feature_map() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/inference.py

  // Params:
    > anchors: list[BoxList]
    > box_cls: tensor of size N, A*C, H, W)
    > box_regression: tensor of size N, A*4, H, W)

  device = box_cls.device
  // device: cuda:0

  N, _, H, W = box_cls.shape
  // N:1, H:8, W:9

  A = box_regression.size(1) // 4
  // A : 9

  C = box_cls.size(1) // A
  // C: 1

  # put 'box_cls' in the same format as anchors
  box_cls = permute_and_flatten(box_cls, N, A, C, H, W)
  // box_cls.shape: torch.Size([1, 648, 1])

  box_cls = box_cls.sigmoid()
  // box_cls.shape: torch.Size([1, 648, 1])

  # put 'box_cls' in the same format as anchors
  box_regression = permute_and_flatten(box_regression, N, A, 4, H, W)
  // box_regression.shape: torch.Size([1, 648, 4])

  box_regression = box_regression.reshape(N, -1, 4)
  // box_regression.shape: torch.Size([1, 648, 4])

  num_anchors = A * H * W
  // num_anchors: 648

  candidate_inds = box_cls > self.pre_nms_thresh
  // candidate_inds.shape: torch.Size([1, 648, 1])

  pre_nms_top_n = candidate_inds.view(N, -1).sum(1)
  // pre_nms_top_n: tensor([0], device='cuda:0')

  pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)
  // pre_nms_top_n: tensor([0], device='cuda:0')

  results = []

  // box_cls.shape: torch.Size([1, 648, 1])
  // box_regression.shape: torch.Size([1, 648, 4])
  // pre_nmns_top_n: tensor([0], device='cuda:0')
  // candidate.inds.shape: torch.Size([1, 648, 1])
  // anchors: (BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy),)
  for per_box_cls, per_box_regression, ... in zip():
  {
    # ====================================
    # per_box_cls.shape: torch.Size([648, 1])
    # type(per_box_cls): <class 'torch.Tensor'>
    # per_box_regression.shape: torch.Size([648, 4])
    # per_pre_nms_top_n: 0
    # per_candidate_inds.shape: torch.Size([648, 1])
    # per_anchors: BoxList(num_boxes=648, image_width=561, image_height=480, mode=xyxy)
    # ====================================

    per_box_cls = per_box_cls[per_candidate_inds]
    per_box_cls, top_k_indices =per_box_cls.topk(per_pre_nms_top_n, sorted=False)
    // per_box_cls: tensor([])
    // top_k_indices: tensor([], dtype=torch.int64)

    per_candidate_nonzeros = \
       per_candidate_inds.nonzero()[top_k_indices, :]
    // per_candidate_inds.shape: torch.Size([648, 1])

    per_box_loc = per_candidate_nonzeros[:, 0]
    // per_box_loc.shape: torch.Size([0])

    per_class = per_candidate_nonzeros[:, 1]
    // per_class.shape: torch.Size([0])

    per_class += 1

    detections = self.box_coder.decode( ) { // CALL
    BoxCoder.decode(self, rel_codes, boxes) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/box_coder.py
      // Params:
        rel_codes.shape: torch.Size([0, 4])
        boxes.shape: torch.Size([0, 4])

      boxes = boxes.to(rel_codes.dtype)
      // boxes.shape: torch.Size([0, 4])

      TO_REMOVE = 1  # TODO remove

      widths = boxes[:, 2] - boxes[:, 0] + TO_REMOVE
      // widths.shape: torch.Size([0])

      heights = boxes[:, 3] - boxes[:, 1] + TO_REMOVE
      // heights.shape: torch.Size([0])

      ctr_x = boxes[:, 0] + 0.5 * widths
      // ctr_x.shape: torch.Size([0])

      ctr_y = boxes[:, 1] + 0.5 * heights
      // ctr_y.shape: torch.Size([0])


      wx, wy, ww, wh = self.weights
      // wx: 10.0, wy: 10.0, ww: 5.0, wh: 5.0

      dx = rel_codes[:, 0::4] / wx
      // dx.shape: torch.Size([0, 1])

      dy = rel_codes[:, 1::4] / wy
      // dy.shape: torch.Size([0, 1])

      dw = rel_codes[:, 2::4] / ww
      // dw.shape: torch.Size([0, 1])

      dh = rel_codes[:, 3::4] / wh
      // dh.shape: torch.Size([0, 1])


      # Prevent sending too large values into torch.exp()
      dw = torch.clamp(dw, max=self.bbox_xform_clip)
      // dw.shape: torch.Size([0, 1])

      dh = torch.clamp(dh, max=self.bbox_xform_clip)
      // dh.shape: torch.Size([0, 1])

      pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
      // pred_ctr_x.shape: torch.Size([0, 1])

      pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
      // pred_ctr_y.shape: torch.Size([0, 1])

      pred_w = torch.exp(dw) * widths[:, None]
      // pred_w.shape: torch.Size([0, 1])

      pred_h = torch.exp(dh) * heights[:, None]
      // pred_h.shape: torch.Size([0, 1])


      pred_boxes = torch.zeros_like(rel_codes)
      // pred_boxes.shape: torch.Size([0, 4])


      # x1
      pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w

      # y1
      pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h

      # x2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1

      # y2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1


      pred_boxes.shape: torch.Size([0, 4])

      return pred_boxes

    } // END BoxCoder.decode(self, rel_codes, boxes)

    } detections = self.box_coder.decode( ) // RETURNED

    // type(detections): <class 'torch.Tensor'>
    // detections: tensor([], size=(0, 4))

    boxlist = BoxList(detections, per_anchors.size, mode="xyxy") { // CALL
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([0, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([0, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    } boxlist = BoxList(detections, per_anchors.size, mode="xyxy") // RETURNED

    // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
    // boxlist: BoxList(num_boxes=0, image_width=561, image_height=480, mode=xyxy)

    boxlist.add_field("labels", per_class)
    boxlist.add_field("scores", per_box_cls)
    boxlist = boxlist.clip_to_image(remove_empty=False)
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([0, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([0, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([0, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([0, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    boxlist = remove_small_boxes(boxlist, self.min_size)
    results.append(boxlist)
  } // END for per_box_cls, per_box_regression, ... in zip():

  return results

} // END RetinaNetPostProcessor.forward_for_single_feature_map()
          }
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) // RETURNED

          } // END of for a, o, b in zip() iteration: 4/5

          {
          # BEGIN for a, o, b in zip()  iteration: 5/5
          # a: (BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy),)
          # o.shape: torch.Size([1, 9, 4, 5])
          # b.shape: torch.Size([1, 36, 4, 5])

          # 2-3-2-3-2 self.forward_for_single_feature_map
          // self.forward_for_single_feature_map: <bound method RetinaNetPostProcessor.forward_for_single_feature_map of RetinaNetPostProcessor()>
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) { // CALL
RetinaNetPostProcessor.forward_for_single_feature_map() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/inference.py

  // Params:
    > anchors: list[BoxList]
    > box_cls: tensor of size N, A*C, H, W)
    > box_regression: tensor of size N, A*4, H, W)

  device = box_cls.device
  // device: cuda:0

  N, _, H, W = box_cls.shape
  // N:1, H:4, W:5

  A = box_regression.size(1) // 4
  // A : 9

  C = box_cls.size(1) // A
  // C: 1

  # put 'box_cls' in the same format as anchors
  box_cls = permute_and_flatten(box_cls, N, A, C, H, W)
  // box_cls.shape: torch.Size([1, 180, 1])

  box_cls = box_cls.sigmoid()
  // box_cls.shape: torch.Size([1, 180, 1])

  # put 'box_cls' in the same format as anchors
  box_regression = permute_and_flatten(box_regression, N, A, 4, H, W)
  // box_regression.shape: torch.Size([1, 180, 4])

  box_regression = box_regression.reshape(N, -1, 4)
  // box_regression.shape: torch.Size([1, 180, 4])

  num_anchors = A * H * W
  // num_anchors: 180

  candidate_inds = box_cls > self.pre_nms_thresh
  // candidate_inds.shape: torch.Size([1, 180, 1])

  pre_nms_top_n = candidate_inds.view(N, -1).sum(1)
  // pre_nms_top_n: tensor([0], device='cuda:0')

  pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_top_n)
  // pre_nms_top_n: tensor([0], device='cuda:0')

  results = []

  // box_cls.shape: torch.Size([1, 180, 1])
  // box_regression.shape: torch.Size([1, 180, 4])
  // pre_nmns_top_n: tensor([0], device='cuda:0')
  // candidate.inds.shape: torch.Size([1, 180, 1])
  // anchors: (BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy),)
  for per_box_cls, per_box_regression, ... in zip():
  {
    # ====================================
    # per_box_cls.shape: torch.Size([180, 1])
    # type(per_box_cls): <class 'torch.Tensor'>
    # per_box_regression.shape: torch.Size([180, 4])
    # per_pre_nms_top_n: 0
    # per_candidate_inds.shape: torch.Size([180, 1])
    # per_anchors: BoxList(num_boxes=180, image_width=561, image_height=480, mode=xyxy)
    # ====================================

    per_box_cls = per_box_cls[per_candidate_inds]
    per_box_cls, top_k_indices =per_box_cls.topk(per_pre_nms_top_n, sorted=False)
    // per_box_cls: tensor([])
    // top_k_indices: tensor([], dtype=torch.int64)

    per_candidate_nonzeros = \
       per_candidate_inds.nonzero()[top_k_indices, :]
    // per_candidate_inds.shape: torch.Size([180, 1])

    per_box_loc = per_candidate_nonzeros[:, 0]
    // per_box_loc.shape: torch.Size([0])

    per_class = per_candidate_nonzeros[:, 1]
    // per_class.shape: torch.Size([0])

    per_class += 1

    detections = self.box_coder.decode( ) { // CALL
    BoxCoder.decode(self, rel_codes, boxes) { // BEGIN
      // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/box_coder.py
      // Params:
        rel_codes.shape: torch.Size([0, 4])
        boxes.shape: torch.Size([0, 4])

      boxes = boxes.to(rel_codes.dtype)
      // boxes.shape: torch.Size([0, 4])

      TO_REMOVE = 1  # TODO remove

      widths = boxes[:, 2] - boxes[:, 0] + TO_REMOVE
      // widths.shape: torch.Size([0])

      heights = boxes[:, 3] - boxes[:, 1] + TO_REMOVE
      // heights.shape: torch.Size([0])

      ctr_x = boxes[:, 0] + 0.5 * widths
      // ctr_x.shape: torch.Size([0])

      ctr_y = boxes[:, 1] + 0.5 * heights
      // ctr_y.shape: torch.Size([0])


      wx, wy, ww, wh = self.weights
      // wx: 10.0, wy: 10.0, ww: 5.0, wh: 5.0

      dx = rel_codes[:, 0::4] / wx
      // dx.shape: torch.Size([0, 1])

      dy = rel_codes[:, 1::4] / wy
      // dy.shape: torch.Size([0, 1])

      dw = rel_codes[:, 2::4] / ww
      // dw.shape: torch.Size([0, 1])

      dh = rel_codes[:, 3::4] / wh
      // dh.shape: torch.Size([0, 1])


      # Prevent sending too large values into torch.exp()
      dw = torch.clamp(dw, max=self.bbox_xform_clip)
      // dw.shape: torch.Size([0, 1])

      dh = torch.clamp(dh, max=self.bbox_xform_clip)
      // dh.shape: torch.Size([0, 1])

      pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
      // pred_ctr_x.shape: torch.Size([0, 1])

      pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
      // pred_ctr_y.shape: torch.Size([0, 1])

      pred_w = torch.exp(dw) * widths[:, None]
      // pred_w.shape: torch.Size([0, 1])

      pred_h = torch.exp(dh) * heights[:, None]
      // pred_h.shape: torch.Size([0, 1])


      pred_boxes = torch.zeros_like(rel_codes)
      // pred_boxes.shape: torch.Size([0, 4])


      # x1
      pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w

      # y1
      pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h

      # x2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1

      # y2 (note: '- 1' is correct; don't be fooled by the asymmetry)
      pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1


      pred_boxes.shape: torch.Size([0, 4])

      return pred_boxes

    } // END BoxCoder.decode(self, rel_codes, boxes)

    } detections = self.box_coder.decode( ) // RETURNED

    // type(detections): <class 'torch.Tensor'>
    // detections: tensor([], size=(0, 4))

    boxlist = BoxList(detections, per_anchors.size, mode="xyxy") { // CALL
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([0, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([0, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    } boxlist = BoxList(detections, per_anchors.size, mode="xyxy") // RETURNED

    // type(boxlist): <class 'maskrcnn_benchmark.structures.bounding_box.BoxList'>
    // boxlist: BoxList(num_boxes=0, image_width=561, image_height=480, mode=xyxy)

    boxlist.add_field("labels", per_class)
    boxlist.add_field("scores", per_box_cls)
    boxlist = boxlist.clip_to_image(remove_empty=False)
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([0, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([0, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([0, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([0, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
    boxlist = remove_small_boxes(boxlist, self.min_size)
    results.append(boxlist)
  } // END for per_box_cls, per_box_regression, ... in zip():

  return results

} // END RetinaNetPostProcessor.forward_for_single_feature_map()
          }
          sampled_boxes.append(self.forward_for_single_feature_map(a, o, b)) // RETURNED

          } // END of for a, o, b in zip() iteration: 5/5

        }// END for a, o, b in zip(anchors, objectness, box_regression)

        # 2-3-2-3-2 boxlists = list(zip(*sampled_boxes))
        boxlists = list(zip(*sampled_boxes))
        boxlists: [(BoxList(num_boxes=824, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=94, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=1, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=0, image_width=561, image_height=480, mode=xyxy), BoxList(num_boxes=0, image_width=561, image_height=480, mode=xyxy))]

        # 2-3-2-3-3 boxlists = [cat_boxlist(boxlist) for boxlist in boxlists]

        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([919, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([919, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        boxlists = [cat_boxlist(boxlist) for boxlist in boxlists]

        boxlists: [BoxList(num_boxes=919, image_width=561, image_height=480, mode=xyxy)]

        if num_levels > 1:
          # 2-3-2-3-4 boxlists = self.select_over_all_levels(boxlists)
          boxlists = self.select_over_all_levels(boxlists) { // CALL
RetinaNetPostProcessor.select_over_all_levels() { //BEGIN
  // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/modeling/rpn/retinanet/inference.py

  // Params:
    > boxlists:
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([919, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([919, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([72, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([72, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([72, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cuda:0

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([72, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
} // END RetinaNetPostProcessor.select_over_all_levels()
          }
          boxlists = self.select_over_all_levels(boxlists) // RETURNED
        if self.training: False and targets: None is not None:
        // boxlists: [BoxList(num_boxes=72, image_width=561, image_height=480, mode=xyxy)]

        return boxlists

    } // END RPNProcessor.forward(self. anchors, objectness, box_regression, targets=None)


  }
  boxes = self.box_selector_test(anchors, box_cls, box_regression) // RETURNED
  // len(boxes): 1
(boxes): [BoxList(num_boxes=72, image_width=561, image_height=480, mode=xyxy)]
return boxes, {} # {} is just empty dictionayr


} // RetinaNetModule._forward_test(self, anchors, box_cls, box_regression): END
} // END RetinaNetModule.forward(self, images, features, targets=None)


  }
  proposals, proposal_losses = self.rpn(images, features, targets) // RETURNED

x = features
result = proposals
return result
} // END GeneralizedRCNN.forward(self, images, targets=None)
    }
    pred = self.model(image_list) // RETURNED
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([72, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cpu

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([72, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
return pred
  pred: BoxList(num_boxes=72, image_width=561, image_height=480, mode=xyxy)
} // END compute_prediction(self, image)




        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([27, 4])
            // image_size: (561, 480)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cpu

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([27, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
        BoxList.__init__(self, bbox, image_size, mode='xyxy') { //BEGIN
          // defined in /home/kimkk/work/lomin/maskrcnn_benchmark/structures/bounding_box.py

          // Params:
            // type(bbox):<class 'torch.Tensor'>
            // bbox.shape:torch.Size([27, 4])
            // image_size: (512, 438)

          // isinstance(bbox, torch.Tensor): True
          device = bbox.device if isinstance(bbox, torch.Tensor) else torch.device("cpu")
          // device: cpu

          bbox = torch.as_tensor(bbox, dtype=torch.float32, device=device)
          // bbox.shape: torch.Size([27, 4])

          // bbox.ndimension(): 2
          // bbox.size(-1): 4

          self.bbox = bbox
          self.size = image_size  # (image_width, image_height)
          self.mode = mode
          self.extra_fields = {}
        } // END BoxList.__init__(self, bbox, image_size, mode='xyxy')
